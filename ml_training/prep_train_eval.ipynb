{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3cd6b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from pathlib import Path\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import xarray as xr\n",
    "import hydroeval as he \n",
    "import seaborn as sns \n",
    "from tqdm import tqdm \n",
    "import warnings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1760f988",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO DO  \n",
    "## - instead of upsampling: downsampling  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fc73e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SETTINGS \n",
    "\n",
    "## which dataset to use \n",
    "## 1 --> most samples, less features \n",
    "## 2 --> less samples (about 20 less?), complete set of features \n",
    "use_set_1 = False\n",
    "\n",
    "## omit coordinates as feature \n",
    "omit_coords = True\n",
    "\n",
    "## set size of buffer search \n",
    "set_buffer_size = 0\n",
    "\n",
    "## what similarity to use: diff, abs, ratio-1, ratio-2, double \n",
    "similarity_method = 'diff'\n",
    "\n",
    "## grid training or cell training \n",
    "grid_format = True\n",
    "\n",
    "## apply PCA \n",
    "do_pca = True "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bccdba",
   "metadata": {},
   "source": [
    "Load dataset with features. On each row, a different simulation of observation sample can be found, in each column feature values or location metadata is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05fcc648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nm-all</th>\n",
       "      <th>Ns-all</th>\n",
       "      <th>N-gof-all</th>\n",
       "      <th>Lm-all</th>\n",
       "      <th>Ls-all</th>\n",
       "      <th>L-gof-all</th>\n",
       "      <th>Gu-all</th>\n",
       "      <th>Ga-all</th>\n",
       "      <th>Gev-gof-all</th>\n",
       "      <th>Gk-all</th>\n",
       "      <th>...</th>\n",
       "      <th>lf-t-seasonal_2</th>\n",
       "      <th>lf-t-seasonal_3</th>\n",
       "      <th>lf-t-seasonal_4</th>\n",
       "      <th>pks-all</th>\n",
       "      <th>pks-seasonal_1</th>\n",
       "      <th>pks-seasonal_2</th>\n",
       "      <th>pks-seasonal_3</th>\n",
       "      <th>pks-seasonal_4</th>\n",
       "      <th>tag</th>\n",
       "      <th>dem_flag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6119010_11</th>\n",
       "      <td>102.035362</td>\n",
       "      <td>106.090645</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.221500</td>\n",
       "      <td>0.863208</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.306767</td>\n",
       "      <td>0.012089</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.925012</td>\n",
       "      <td>...</td>\n",
       "      <td>1.045455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>666.803894</td>\n",
       "      <td>550.015259</td>\n",
       "      <td>479.626465</td>\n",
       "      <td>212.843933</td>\n",
       "      <td>673.393555</td>\n",
       "      <td>6119010</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6119010_12</th>\n",
       "      <td>100.917046</td>\n",
       "      <td>105.168777</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.209119</td>\n",
       "      <td>0.864435</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.603184</td>\n",
       "      <td>0.012195</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.920779</td>\n",
       "      <td>...</td>\n",
       "      <td>1.086957</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>654.762268</td>\n",
       "      <td>549.104004</td>\n",
       "      <td>470.134583</td>\n",
       "      <td>212.791443</td>\n",
       "      <td>666.401978</td>\n",
       "      <td>6119010</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6119010_13</th>\n",
       "      <td>0.544169</td>\n",
       "      <td>0.542255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.002645</td>\n",
       "      <td>0.867551</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.300217</td>\n",
       "      <td>2.365214</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.007072</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.636364</td>\n",
       "      <td>3.131104</td>\n",
       "      <td>2.947998</td>\n",
       "      <td>2.192993</td>\n",
       "      <td>0.960388</td>\n",
       "      <td>3.788452</td>\n",
       "      <td>6119010</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6119010_14</th>\n",
       "      <td>162.321777</td>\n",
       "      <td>138.910507</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.760090</td>\n",
       "      <td>0.835850</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.828016</td>\n",
       "      <td>0.009233</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.365474</td>\n",
       "      <td>...</td>\n",
       "      <td>2.880000</td>\n",
       "      <td>4.181818</td>\n",
       "      <td>7.960000</td>\n",
       "      <td>723.082581</td>\n",
       "      <td>657.824402</td>\n",
       "      <td>661.195068</td>\n",
       "      <td>604.262695</td>\n",
       "      <td>815.905151</td>\n",
       "      <td>6119010</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6119010_15</th>\n",
       "      <td>17.163115</td>\n",
       "      <td>16.641987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.442999</td>\n",
       "      <td>0.888787</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.676133</td>\n",
       "      <td>0.077067</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.063609</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.846154</td>\n",
       "      <td>98.092957</td>\n",
       "      <td>88.489990</td>\n",
       "      <td>74.034424</td>\n",
       "      <td>37.886658</td>\n",
       "      <td>124.954529</td>\n",
       "      <td>6119010</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6983350_96</th>\n",
       "      <td>522.018127</td>\n",
       "      <td>325.349060</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.080506</td>\n",
       "      <td>0.592225</td>\n",
       "      <td>0.0</td>\n",
       "      <td>375.648448</td>\n",
       "      <td>0.003942</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.574376</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1890.435181</td>\n",
       "      <td>574.974670</td>\n",
       "      <td>1660.376587</td>\n",
       "      <td>1322.813416</td>\n",
       "      <td>659.456177</td>\n",
       "      <td>6983350</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6983350_97</th>\n",
       "      <td>1.769354</td>\n",
       "      <td>1.644365</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.271318</td>\n",
       "      <td>0.743112</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.029579</td>\n",
       "      <td>0.779967</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.157798</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>10.525513</td>\n",
       "      <td>11.605530</td>\n",
       "      <td>8.256226</td>\n",
       "      <td>8.186340</td>\n",
       "      <td>9.359741</td>\n",
       "      <td>6983350</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6983350_98</th>\n",
       "      <td>6.406629</td>\n",
       "      <td>5.884712</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.530997</td>\n",
       "      <td>0.797030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.759185</td>\n",
       "      <td>0.217946</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.185247</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>33.151855</td>\n",
       "      <td>41.973267</td>\n",
       "      <td>34.476318</td>\n",
       "      <td>29.242249</td>\n",
       "      <td>29.821472</td>\n",
       "      <td>6983350</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6983350_99</th>\n",
       "      <td>0.280780</td>\n",
       "      <td>0.267437</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.570457</td>\n",
       "      <td>0.738658</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.160464</td>\n",
       "      <td>4.795705</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.102273</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.593628</td>\n",
       "      <td>1.537781</td>\n",
       "      <td>1.508484</td>\n",
       "      <td>1.701965</td>\n",
       "      <td>1.826477</td>\n",
       "      <td>6983350</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6983350_gauge</th>\n",
       "      <td>413.776970</td>\n",
       "      <td>228.897089</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.840650</td>\n",
       "      <td>0.653553</td>\n",
       "      <td>0.0</td>\n",
       "      <td>310.799590</td>\n",
       "      <td>0.005603</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.267775</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>992.500000</td>\n",
       "      <td>920.000000</td>\n",
       "      <td>830.000000</td>\n",
       "      <td>1180.000000</td>\n",
       "      <td>895.000000</td>\n",
       "      <td>6983350</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45182 rows × 215 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Nm-all      Ns-all  N-gof-all    Lm-all    Ls-all  \\\n",
       "ID                                                                     \n",
       "6119010_11     102.035362  106.090645        0.0  4.221500  0.863208   \n",
       "6119010_12     100.917046  105.168777        0.0  4.209119  0.864435   \n",
       "6119010_13       0.544169    0.542255        0.0 -1.002645  0.867551   \n",
       "6119010_14     162.321777  138.910507        0.0  4.760090  0.835850   \n",
       "6119010_15      17.163115   16.641987        0.0  2.442999  0.888787   \n",
       "...                   ...         ...        ...       ...       ...   \n",
       "6983350_96     522.018127  325.349060        0.0  6.080506  0.592225   \n",
       "6983350_97       1.769354    1.644365        0.0  0.271318  0.743112   \n",
       "6983350_98       6.406629    5.884712        0.0  1.530997  0.797030   \n",
       "6983350_99       0.280780    0.267437        0.0 -1.570457  0.738658   \n",
       "6983350_gauge  413.776970  228.897089        0.0  5.840650  0.653553   \n",
       "\n",
       "               L-gof-all      Gu-all    Ga-all  Gev-gof-all    Gk-all  ...  \\\n",
       "ID                                                                     ...   \n",
       "6119010_11           0.0   54.306767  0.012089          0.0  0.925012  ...   \n",
       "6119010_12           0.0   53.603184  0.012195          0.0  0.920779  ...   \n",
       "6119010_13           0.0    0.300217  2.365214          0.0  1.007072  ...   \n",
       "6119010_14           0.0   99.828016  0.009233          0.0  1.365474  ...   \n",
       "6119010_15           0.0    9.676133  0.077067          0.0  1.063609  ...   \n",
       "...                  ...         ...       ...          ...       ...  ...   \n",
       "6983350_96           0.0  375.648448  0.003942          0.0  2.574376  ...   \n",
       "6983350_97           0.0    1.029579  0.779967          0.0  1.157798  ...   \n",
       "6983350_98           0.0    3.759185  0.217946          0.0  1.185247  ...   \n",
       "6983350_99           0.0    0.160464  4.795705          0.0  1.102273  ...   \n",
       "6983350_gauge        0.0  310.799590  0.005603          0.0  3.267775  ...   \n",
       "\n",
       "               lf-t-seasonal_2  lf-t-seasonal_3  lf-t-seasonal_4      pks-all  \\\n",
       "ID                                                                              \n",
       "6119010_11            1.045455         0.000000         0.000000   666.803894   \n",
       "6119010_12            1.086957         0.000000         0.000000   654.762268   \n",
       "6119010_13            0.000000         0.000000         8.636364     3.131104   \n",
       "6119010_14            2.880000         4.181818         7.960000   723.082581   \n",
       "6119010_15            0.000000         0.000000         9.846154    98.092957   \n",
       "...                        ...              ...              ...          ...   \n",
       "6983350_96            0.000000         0.000000         0.000000  1890.435181   \n",
       "6983350_97            0.000000         0.000000         0.222222    10.525513   \n",
       "6983350_98            0.000000         0.888889         2.666667    33.151855   \n",
       "6983350_99            0.000000         0.000000         0.000000     1.593628   \n",
       "6983350_gauge         0.000000         0.888889         1.333333   992.500000   \n",
       "\n",
       "               pks-seasonal_1  pks-seasonal_2  pks-seasonal_3  pks-seasonal_4  \\\n",
       "ID                                                                              \n",
       "6119010_11         550.015259      479.626465      212.843933      673.393555   \n",
       "6119010_12         549.104004      470.134583      212.791443      666.401978   \n",
       "6119010_13           2.947998        2.192993        0.960388        3.788452   \n",
       "6119010_14         657.824402      661.195068      604.262695      815.905151   \n",
       "6119010_15          88.489990       74.034424       37.886658      124.954529   \n",
       "...                       ...             ...             ...             ...   \n",
       "6983350_96         574.974670     1660.376587     1322.813416      659.456177   \n",
       "6983350_97          11.605530        8.256226        8.186340        9.359741   \n",
       "6983350_98          41.973267       34.476318       29.242249       29.821472   \n",
       "6983350_99           1.537781        1.508484        1.701965        1.826477   \n",
       "6983350_gauge      920.000000      830.000000     1180.000000      895.000000   \n",
       "\n",
       "                   tag  dem_flag  \n",
       "ID                                \n",
       "6119010_11     6119010         0  \n",
       "6119010_12     6119010         0  \n",
       "6119010_13     6119010         0  \n",
       "6119010_14     6119010         0  \n",
       "6119010_15     6119010         0  \n",
       "...                ...       ...  \n",
       "6983350_96     6983350         0  \n",
       "6983350_97     6983350         0  \n",
       "6983350_98     6983350         0  \n",
       "6983350_99     6983350         0  \n",
       "6983350_gauge  6983350         0  \n",
       "\n",
       "[45182 rows x 215 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signature_dir = Path(r\"C:\\Users\\mvand\\Documents\\Master EE\\Year 4\\Thesis\\data\\training_data_S1\\signatures_nc_V1_output\") \n",
    " \n",
    "if use_set_1:\n",
    "    fn_signatures = signature_dir / \"S1_merged_signatures_v2-cleanup-1.csv\"\n",
    "else:\n",
    "    fn_signatures = signature_dir / \"S1_merged_signatures_v2-cleanup-2.csv\" \n",
    "\n",
    "df_signatures = pd.read_csv(fn_signatures, index_col = 0)\n",
    "\n",
    "df_signatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038fca7d",
   "metadata": {},
   "source": [
    "Slightly expand the searching area by labeling the up- and downstream neigbhours of the target pixel as 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ca33ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## expand searching area by max. 2 pixels \n",
    "\n",
    "def expand_labels(df, id_col, target_col, critical_variable, \n",
    "                  p_diff = 5., n_max = 3, pixel_size = 5000., option = 1,\n",
    "                  x_coord = 'x', y_coord = 'y'):\n",
    "    \n",
    "    out_col = 'range_{}'.format(target_col)\n",
    "    df[out_col] = 0.\n",
    "    \n",
    "    for bix in df[id_col].unique():\n",
    "        \n",
    "        ## select buffer and target value \n",
    "        _buffer = df[ df[id_col] == bix ] \n",
    "        \n",
    "        ## discard gauge values \n",
    "        buffer_ix = [ix for ix in _buffer.index if not 'gauge' in ix]\n",
    "        _sim = _buffer.loc[buffer_ix] \n",
    "        \n",
    "        ## select target row \n",
    "        target = _sim[ _sim[target_col] == 1.]\n",
    "        \n",
    "        ## if target value found\n",
    "        if len(target) > 0:\n",
    "            \n",
    "            ## search criterium \n",
    "            target_var = target[critical_variable].values[0] \n",
    "            \n",
    "            ## look to adjacent pixels only \n",
    "            target_X_coord = target[x_coord].values[0]\n",
    "            target_Y_coord = target[y_coord].values[0] \n",
    "            x_filter = (_sim[x_coord] >= target_X_coord - (pixel_size+1.) ) & (_sim[x_coord] <= target_X_coord + (pixel_size+1.))\n",
    "            y_filter = (_sim[y_coord] >= target_Y_coord - (pixel_size+1.) ) & (_sim[y_coord] <= target_Y_coord + (pixel_size+1.))\n",
    "            _sim = _sim[ x_filter & y_filter]\n",
    "\n",
    "            ## calculate absolute percentual difference \n",
    "            d_var = (( ((_sim[critical_variable] - target_var)**2)**0.5 / target_var)*100 ).sort_values()\n",
    "            \n",
    "            ## OPTION 1 - select top three (including original target pixel) based on difference \n",
    "            if option == 1:\n",
    "                expand_targets = d_var.head(n_max).index \n",
    "\n",
    "            ## OPTION 2 - also include a percentual limit (1% - 5%) \n",
    "            if option == 2:\n",
    "                expand_targets = d_var[ d_var <= p_diff].index\n",
    "                if len(expand_targets) > n_max:\n",
    "                    expand_targets = expand_targets[:int(n_max-1)]\n",
    "\n",
    "            ## relabel \n",
    "            if target.index[0] in expand_targets:\n",
    "                df.loc[expand_targets, out_col] = 1 \n",
    "            else:\n",
    "                print('check ', bix)            \n",
    "\n",
    "    return df, out_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be3b8c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_signatures, range_col = expand_labels(df_signatures, 'tag', 'target', 'Nm-all', option = 2, p_diff = 1.) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d799edab",
   "metadata": {},
   "source": [
    "Set the label categories - identify feature columns and target labels. Used for later processing of similarity and splitting X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9167ee0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## column descriptors \n",
    "columns = df_signatures.columns.values \n",
    "\n",
    "## target value to predict \n",
    "target_col = ['target'] \n",
    "\n",
    "## target for continued search \n",
    "target_buffer = ['in_buffer']\n",
    "\n",
    "## should be omitted from dataset - but could be useful for easy selection \n",
    "## dem_flag only 1 for gauge values\n",
    "non_feature_cols = ['n_buffer', 'tag', 'dem_flag', range_col, 'in_buffer'] \n",
    "\n",
    "## coord cols - could be ommitted from dataset, risk for overfitting\n",
    "## and also takes away focus on selection based on timeseries\n",
    "coord_cols = ['x', 'y', 'lat', 'lon']\n",
    "\n",
    "## features that do not have to be transformed by subtraction \n",
    "## and cross-correlation properties, that are already a similarity property \n",
    "## as they are cross-correlation values of simulations and observations\n",
    "non_similarity_cols = [col for col in columns if 'clag' in col]\n",
    "\n",
    "feature_cols = [col for col in columns if (col not in non_feature_cols) & (not 'target' in col)]\n",
    "\n",
    "## if coord_cols ommitted:\n",
    "if omit_coords:\n",
    "    feature_cols = [col for col in feature_cols if col not in coord_cols] \n",
    "\n",
    "## columns on which to perform similarity calculations:\n",
    "calc_cols = [col for col in feature_cols if col not in non_similarity_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011f1d68",
   "metadata": {},
   "source": [
    "Set the similarity metric used by the algorithm to find a best matching simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ebcecc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate similarity per buffer \n",
    "\n",
    "\n",
    "def calc_similarity(df, buffer_col, calc_cols,\n",
    "                   methods = ['diff']):\n",
    "    \n",
    "    df_out = df.copy() \n",
    "    \n",
    "    buffer_idx = df[buffer_col].unique() \n",
    "    \n",
    "    for ix in buffer_idx:\n",
    "        df_buffer = df[ df[buffer_col] == ix] \n",
    "        \n",
    "        cell_index = [row for row in df_buffer.index if not 'gauge' in row]\n",
    "        gauge_index = [row for row in df_buffer.index if 'gauge' in row] \n",
    "        \n",
    "        if len(gauge_index) > 0:\n",
    "                \n",
    "            if 'diff' in methods:            \n",
    "                for col in calc_cols:\n",
    "                    df_out.loc[cell_index, 'diff_{}'.format(col)] = df_out.loc[cell_index, col] - df_out.loc[gauge_index, col].values\n",
    "\n",
    "            if 'abs' in methods:\n",
    "                for col in calc_cols:\n",
    "                     df_out.loc[cell_index, 'abs_{}'.format(col)] = ((df_out.loc[cell_index, col] - df_out.loc[gauge_index, col].values)**2)**0.5 \n",
    "\n",
    "            if 'ratio-1' in methods:\n",
    "                for col in calc_cols:\n",
    "                    df_out.loc[cell_index, 'rat1_{}'.format(col)] = 1 - (( df_out.loc[cell_index, col] / (df_out.loc[gauge_index, col].values+1e-6)  )**2)**0.5\n",
    "\n",
    "            if 'ratio-2' in methods:\n",
    "                for col in calc_cols:\n",
    "                    df_out.loc[cell_index, 'rat2_{}'.format(col)] =  ( df_out.loc[cell_index, col] - df_out.loc[gauge_index, col].values ) / (df_out.loc[gauge_index, col].values+1e-6)\n",
    "            \n",
    "            if 'double' in methods: \n",
    "                for col in calc_cols:\n",
    "                    df_out.loc[cell_index, 'sim_{}'.format(col)] = df_out.loc[cell_index, col]\n",
    "                    df_out.loc[cell_index, 'obs_{}'.format(col)] = df_out.loc[gauge_index, col].values[0]\n",
    "        \n",
    "        else:\n",
    "            print('No gauge? :', ix)\n",
    "            df_out = df_out.drop(index=cell_index)\n",
    "                    \n",
    "    ## drop calc_cols\n",
    "    df_out = df_out.drop(columns=calc_cols)\n",
    "    \n",
    "    gauge_idx = [row for row in df_out.index if 'gauge' in row] \n",
    "    return df_out.drop(index=gauge_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2a49006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No gauge? : 6123150\n",
      "No gauge? : 6139340\n",
      "No gauge? : 6196020\n",
      "No gauge? : 6196040\n",
      "No gauge? : 6196400\n",
      "No gauge? : 6196600\n",
      "No gauge? : 6196650\n",
      "No gauge? : 6216610\n",
      "No gauge? : 6216630\n",
      "No gauge? : 6216800\n",
      "No gauge? : 6216810\n",
      "No gauge? : 6217100\n",
      "No gauge? : 6401800\n",
      "No gauge? : 6421103\n",
      "No gauge? : 6421900\n",
      "No gauge? : 6604365\n",
      "No gauge? : 6605410\n",
      "No gauge? : 6854150\n",
      "No gauge? : 6854591\n",
      "No gauge? : 6943100\n",
      "No gauge? : 6970869\n",
      "No gauge? : 6971401\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clag-0-all</th>\n",
       "      <th>clag-1-all</th>\n",
       "      <th>clag-0-seasonal_1</th>\n",
       "      <th>clag-1-seasonal_1</th>\n",
       "      <th>clag-0-seasonal_2</th>\n",
       "      <th>clag-1-seasonal_2</th>\n",
       "      <th>clag-0-seasonal_3</th>\n",
       "      <th>clag-1-seasonal_3</th>\n",
       "      <th>clag-0-seasonal_4</th>\n",
       "      <th>clag-1-seasonal_4</th>\n",
       "      <th>...</th>\n",
       "      <th>diff_lf-t-all</th>\n",
       "      <th>diff_lf-t-seasonal_1</th>\n",
       "      <th>diff_lf-t-seasonal_2</th>\n",
       "      <th>diff_lf-t-seasonal_3</th>\n",
       "      <th>diff_lf-t-seasonal_4</th>\n",
       "      <th>diff_pks-all</th>\n",
       "      <th>diff_pks-seasonal_1</th>\n",
       "      <th>diff_pks-seasonal_2</th>\n",
       "      <th>diff_pks-seasonal_3</th>\n",
       "      <th>diff_pks-seasonal_4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6119010_11</th>\n",
       "      <td>0.635464</td>\n",
       "      <td>0.544323</td>\n",
       "      <td>0.672078</td>\n",
       "      <td>0.537085</td>\n",
       "      <td>0.512745</td>\n",
       "      <td>0.412076</td>\n",
       "      <td>0.559240</td>\n",
       "      <td>0.462092</td>\n",
       "      <td>0.685505</td>\n",
       "      <td>0.589351</td>\n",
       "      <td>...</td>\n",
       "      <td>2.727273</td>\n",
       "      <td>4.481481</td>\n",
       "      <td>1.045455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>328.303894</td>\n",
       "      <td>124.265259</td>\n",
       "      <td>134.376465</td>\n",
       "      <td>0.843933</td>\n",
       "      <td>355.143555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6119010_12</th>\n",
       "      <td>0.635464</td>\n",
       "      <td>0.544067</td>\n",
       "      <td>0.671657</td>\n",
       "      <td>0.536465</td>\n",
       "      <td>0.512970</td>\n",
       "      <td>0.412044</td>\n",
       "      <td>0.558202</td>\n",
       "      <td>0.460883</td>\n",
       "      <td>0.685622</td>\n",
       "      <td>0.589109</td>\n",
       "      <td>...</td>\n",
       "      <td>2.863636</td>\n",
       "      <td>4.518519</td>\n",
       "      <td>1.086957</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>316.262268</td>\n",
       "      <td>123.354004</td>\n",
       "      <td>124.884583</td>\n",
       "      <td>0.791443</td>\n",
       "      <td>348.151978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6119010_13</th>\n",
       "      <td>0.589989</td>\n",
       "      <td>0.522277</td>\n",
       "      <td>0.698611</td>\n",
       "      <td>0.582520</td>\n",
       "      <td>0.475808</td>\n",
       "      <td>0.396183</td>\n",
       "      <td>0.589318</td>\n",
       "      <td>0.514527</td>\n",
       "      <td>0.607216</td>\n",
       "      <td>0.541708</td>\n",
       "      <td>...</td>\n",
       "      <td>16.045455</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.493506</td>\n",
       "      <td>-335.368896</td>\n",
       "      <td>-422.802002</td>\n",
       "      <td>-343.057007</td>\n",
       "      <td>-211.039612</td>\n",
       "      <td>-314.461548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6119010_14</th>\n",
       "      <td>0.709440</td>\n",
       "      <td>0.570481</td>\n",
       "      <td>0.822756</td>\n",
       "      <td>0.639700</td>\n",
       "      <td>0.689205</td>\n",
       "      <td>0.525949</td>\n",
       "      <td>0.606942</td>\n",
       "      <td>0.444466</td>\n",
       "      <td>0.748927</td>\n",
       "      <td>0.623231</td>\n",
       "      <td>...</td>\n",
       "      <td>26.181818</td>\n",
       "      <td>7.227273</td>\n",
       "      <td>2.880000</td>\n",
       "      <td>4.181818</td>\n",
       "      <td>7.817143</td>\n",
       "      <td>384.582581</td>\n",
       "      <td>232.074402</td>\n",
       "      <td>315.945068</td>\n",
       "      <td>392.262695</td>\n",
       "      <td>497.655151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6119010_15</th>\n",
       "      <td>0.612656</td>\n",
       "      <td>0.531497</td>\n",
       "      <td>0.700380</td>\n",
       "      <td>0.568576</td>\n",
       "      <td>0.506390</td>\n",
       "      <td>0.412513</td>\n",
       "      <td>0.566742</td>\n",
       "      <td>0.485944</td>\n",
       "      <td>0.715055</td>\n",
       "      <td>0.629191</td>\n",
       "      <td>...</td>\n",
       "      <td>18.590909</td>\n",
       "      <td>6.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.703297</td>\n",
       "      <td>-240.407043</td>\n",
       "      <td>-337.260010</td>\n",
       "      <td>-271.215576</td>\n",
       "      <td>-174.113342</td>\n",
       "      <td>-193.295471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6983350_95</th>\n",
       "      <td>0.203398</td>\n",
       "      <td>0.227645</td>\n",
       "      <td>0.419789</td>\n",
       "      <td>0.451424</td>\n",
       "      <td>-0.090568</td>\n",
       "      <td>-0.074078</td>\n",
       "      <td>0.188674</td>\n",
       "      <td>0.222501</td>\n",
       "      <td>0.318391</td>\n",
       "      <td>0.345153</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.888889</td>\n",
       "      <td>-1.333333</td>\n",
       "      <td>-990.893250</td>\n",
       "      <td>-918.340454</td>\n",
       "      <td>-828.721924</td>\n",
       "      <td>-1178.187866</td>\n",
       "      <td>-893.339539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6983350_96</th>\n",
       "      <td>0.574453</td>\n",
       "      <td>0.580212</td>\n",
       "      <td>0.581299</td>\n",
       "      <td>0.579456</td>\n",
       "      <td>0.617861</td>\n",
       "      <td>0.618089</td>\n",
       "      <td>0.578885</td>\n",
       "      <td>0.576491</td>\n",
       "      <td>0.707021</td>\n",
       "      <td>0.715589</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.888889</td>\n",
       "      <td>-1.333333</td>\n",
       "      <td>897.935181</td>\n",
       "      <td>-345.025330</td>\n",
       "      <td>830.376587</td>\n",
       "      <td>142.813416</td>\n",
       "      <td>-235.543823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6983350_97</th>\n",
       "      <td>0.234536</td>\n",
       "      <td>0.260122</td>\n",
       "      <td>0.485846</td>\n",
       "      <td>0.515676</td>\n",
       "      <td>-0.075006</td>\n",
       "      <td>-0.045764</td>\n",
       "      <td>0.231765</td>\n",
       "      <td>0.261977</td>\n",
       "      <td>0.365475</td>\n",
       "      <td>0.394853</td>\n",
       "      <td>...</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>6.181818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.888889</td>\n",
       "      <td>-1.111111</td>\n",
       "      <td>-981.974487</td>\n",
       "      <td>-908.394470</td>\n",
       "      <td>-821.743774</td>\n",
       "      <td>-1171.813660</td>\n",
       "      <td>-885.640259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6983350_98</th>\n",
       "      <td>0.231623</td>\n",
       "      <td>0.258969</td>\n",
       "      <td>0.471694</td>\n",
       "      <td>0.502134</td>\n",
       "      <td>0.008418</td>\n",
       "      <td>0.038318</td>\n",
       "      <td>0.273371</td>\n",
       "      <td>0.306237</td>\n",
       "      <td>0.313030</td>\n",
       "      <td>0.340908</td>\n",
       "      <td>...</td>\n",
       "      <td>10.555556</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>-959.348145</td>\n",
       "      <td>-878.026733</td>\n",
       "      <td>-795.523682</td>\n",
       "      <td>-1150.757751</td>\n",
       "      <td>-865.178528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6983350_99</th>\n",
       "      <td>0.208036</td>\n",
       "      <td>0.233330</td>\n",
       "      <td>0.426323</td>\n",
       "      <td>0.458662</td>\n",
       "      <td>-0.106108</td>\n",
       "      <td>-0.084386</td>\n",
       "      <td>0.190649</td>\n",
       "      <td>0.223533</td>\n",
       "      <td>0.331424</td>\n",
       "      <td>0.355856</td>\n",
       "      <td>...</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>4.818182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.888889</td>\n",
       "      <td>-1.333333</td>\n",
       "      <td>-990.906372</td>\n",
       "      <td>-918.462219</td>\n",
       "      <td>-828.491516</td>\n",
       "      <td>-1178.298035</td>\n",
       "      <td>-893.173523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42993 rows × 216 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            clag-0-all  clag-1-all  clag-0-seasonal_1  clag-1-seasonal_1  \\\n",
       "ID                                                                         \n",
       "6119010_11    0.635464    0.544323           0.672078           0.537085   \n",
       "6119010_12    0.635464    0.544067           0.671657           0.536465   \n",
       "6119010_13    0.589989    0.522277           0.698611           0.582520   \n",
       "6119010_14    0.709440    0.570481           0.822756           0.639700   \n",
       "6119010_15    0.612656    0.531497           0.700380           0.568576   \n",
       "...                ...         ...                ...                ...   \n",
       "6983350_95    0.203398    0.227645           0.419789           0.451424   \n",
       "6983350_96    0.574453    0.580212           0.581299           0.579456   \n",
       "6983350_97    0.234536    0.260122           0.485846           0.515676   \n",
       "6983350_98    0.231623    0.258969           0.471694           0.502134   \n",
       "6983350_99    0.208036    0.233330           0.426323           0.458662   \n",
       "\n",
       "            clag-0-seasonal_2  clag-1-seasonal_2  clag-0-seasonal_3  \\\n",
       "ID                                                                    \n",
       "6119010_11           0.512745           0.412076           0.559240   \n",
       "6119010_12           0.512970           0.412044           0.558202   \n",
       "6119010_13           0.475808           0.396183           0.589318   \n",
       "6119010_14           0.689205           0.525949           0.606942   \n",
       "6119010_15           0.506390           0.412513           0.566742   \n",
       "...                       ...                ...                ...   \n",
       "6983350_95          -0.090568          -0.074078           0.188674   \n",
       "6983350_96           0.617861           0.618089           0.578885   \n",
       "6983350_97          -0.075006          -0.045764           0.231765   \n",
       "6983350_98           0.008418           0.038318           0.273371   \n",
       "6983350_99          -0.106108          -0.084386           0.190649   \n",
       "\n",
       "            clag-1-seasonal_3  clag-0-seasonal_4  clag-1-seasonal_4  ...  \\\n",
       "ID                                                                   ...   \n",
       "6119010_11           0.462092           0.685505           0.589351  ...   \n",
       "6119010_12           0.460883           0.685622           0.589109  ...   \n",
       "6119010_13           0.514527           0.607216           0.541708  ...   \n",
       "6119010_14           0.444466           0.748927           0.623231  ...   \n",
       "6119010_15           0.485944           0.715055           0.629191  ...   \n",
       "...                       ...                ...                ...  ...   \n",
       "6983350_95           0.222501           0.318391           0.345153  ...   \n",
       "6983350_96           0.576491           0.707021           0.715589  ...   \n",
       "6983350_97           0.261977           0.365475           0.394853  ...   \n",
       "6983350_98           0.306237           0.313030           0.340908  ...   \n",
       "6983350_99           0.223533           0.331424           0.355856  ...   \n",
       "\n",
       "            diff_lf-t-all  diff_lf-t-seasonal_1  diff_lf-t-seasonal_2  \\\n",
       "ID                                                                      \n",
       "6119010_11       2.727273              4.481481              1.045455   \n",
       "6119010_12       2.863636              4.518519              1.086957   \n",
       "6119010_13      16.045455              6.000000              0.000000   \n",
       "6119010_14      26.181818              7.227273              2.880000   \n",
       "6119010_15      18.590909              6.083333              0.000000   \n",
       "...                   ...                   ...                   ...   \n",
       "6983350_95       0.555556              6.000000              0.000000   \n",
       "6983350_96      -2.333333              0.000000              0.000000   \n",
       "6983350_97       1.666667              6.181818              0.000000   \n",
       "6983350_98      10.555556              4.800000              0.000000   \n",
       "6983350_99       4.333333              4.818182              0.000000   \n",
       "\n",
       "            diff_lf-t-seasonal_3  diff_lf-t-seasonal_4  diff_pks-all  \\\n",
       "ID                                                                     \n",
       "6119010_11              0.000000             -0.142857    328.303894   \n",
       "6119010_12              0.000000             -0.142857    316.262268   \n",
       "6119010_13              0.000000              8.493506   -335.368896   \n",
       "6119010_14              4.181818              7.817143    384.582581   \n",
       "6119010_15              0.000000              9.703297   -240.407043   \n",
       "...                          ...                   ...           ...   \n",
       "6983350_95             -0.888889             -1.333333   -990.893250   \n",
       "6983350_96             -0.888889             -1.333333    897.935181   \n",
       "6983350_97             -0.888889             -1.111111   -981.974487   \n",
       "6983350_98              0.000000              1.333333   -959.348145   \n",
       "6983350_99             -0.888889             -1.333333   -990.906372   \n",
       "\n",
       "            diff_pks-seasonal_1  diff_pks-seasonal_2  diff_pks-seasonal_3  \\\n",
       "ID                                                                          \n",
       "6119010_11           124.265259           134.376465             0.843933   \n",
       "6119010_12           123.354004           124.884583             0.791443   \n",
       "6119010_13          -422.802002          -343.057007          -211.039612   \n",
       "6119010_14           232.074402           315.945068           392.262695   \n",
       "6119010_15          -337.260010          -271.215576          -174.113342   \n",
       "...                         ...                  ...                  ...   \n",
       "6983350_95          -918.340454          -828.721924         -1178.187866   \n",
       "6983350_96          -345.025330           830.376587           142.813416   \n",
       "6983350_97          -908.394470          -821.743774         -1171.813660   \n",
       "6983350_98          -878.026733          -795.523682         -1150.757751   \n",
       "6983350_99          -918.462219          -828.491516         -1178.298035   \n",
       "\n",
       "            diff_pks-seasonal_4  \n",
       "ID                               \n",
       "6119010_11           355.143555  \n",
       "6119010_12           348.151978  \n",
       "6119010_13          -314.461548  \n",
       "6119010_14           497.655151  \n",
       "6119010_15          -193.295471  \n",
       "...                         ...  \n",
       "6983350_95          -893.339539  \n",
       "6983350_96          -235.543823  \n",
       "6983350_97          -885.640259  \n",
       "6983350_98          -865.178528  \n",
       "6983350_99          -893.173523  \n",
       "\n",
       "[42993 rows x 216 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_similarity = calc_similarity(df_signatures, 'tag', calc_cols, methods=[similarity_method])\n",
    "\n",
    "df_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91941402",
   "metadata": {},
   "source": [
    "Import required scikit-learn functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f64aa99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, QuantileTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# algorithms \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.svm import SVC \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# evaluation \n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, mean_squared_error, balanced_accuracy_score "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9591ef9",
   "metadata": {},
   "source": [
    "Split a training & evaluation and test set. First investigate location of targets. Split datasets based on those distributions for equal division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b67686a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 gauges / buffers fall outside the buffer zone of 4:\n",
      "         d_X_cell    d_Y_cell\n",
      "count   23.000000   23.000000\n",
      "mean   -20.826087   -6.086957\n",
      "std     55.257786   54.266106\n",
      "min   -214.000000 -250.000000\n",
      "25%    -11.500000   -1.000000\n",
      "50%     -4.000000    6.000000\n",
      "75%      2.000000   10.500000\n",
      "max     13.000000   30.000000\n",
      "\n",
      "Analyse size of buffer or searching range with number of hits:\n",
      "In a search range of 0 cells, 224 matches are found\n",
      "In a search range of 1 cells, 262 matches are found\n",
      "In a search range of 2 cells, 28 matches are found\n",
      "In a search range of 3 cells, 8 matches are found\n",
      "In a search range of 4 cells, 3 matches are found\n",
      "\n",
      "Show result of split:\n",
      "Train val set:\n",
      "n_buffer\n",
      "0.0    190.0\n",
      "Name: target, dtype: float64\n",
      "\n",
      "Test set\n",
      "n_buffer\n",
      "0.0    34.0\n",
      "Name: target, dtype: float64\n",
      "\n",
      "462 samples in total train val set\n",
      "86 samples in total train val set\n"
     ]
    }
   ],
   "source": [
    "## first split data based on being inside or outside the buffer zone (=4)\n",
    "df_outside_buffer = df_similarity[ df_similarity['target'] == -1].copy() \n",
    "gauge_outside = df_outside_buffer['tag'].unique().astype(int)\n",
    "\n",
    "fn_gauge = Path(r\"C:\\Users\\mvand\\Documents\\Master EE\\Year 4\\Thesis\\data\\training_data_S1\")  / \"V1_grdc_efas_selection-cartesius-snapped-1.csv\" \n",
    "meta_gauge = pd.read_csv(fn_gauge, index_col = 0)\n",
    "\n",
    "## analyse large shifts \n",
    "df_meta_gauge = meta_gauge.loc[gauge_outside]\n",
    "print('{} gauges / buffers fall outside the buffer zone of 4:'.format(len(gauge_outside)))\n",
    "print(df_meta_gauge[['d_X_cell', 'd_Y_cell']].describe())\n",
    "# print(df_meta_gauge[['d_X_cell', 'd_Y_cell']])\n",
    "\n",
    "## copy data to new dataframe \n",
    "df_predict_buffer = df_similarity.copy() #.drop(index=df_outside_buffer.index) \n",
    "## set all outside buffer vals to 0 \n",
    "df_predict_buffer.loc[df_outside_buffer.index, 'target'] = 0. \n",
    "## add target column for in-buffer prediction \n",
    "df_predict_buffer['in_buffer'] = 1 \n",
    "df_predict_buffer.loc[ df_predict_buffer['tag'].isin(gauge_outside)  , 'in_buffer'] = 0. \n",
    "\n",
    "print('\\nAnalyse size of buffer or searching range with number of hits:')\n",
    "for buffer_size in np.sort(df_predict_buffer['n_buffer'].unique()):\n",
    "    n_hits =df_predict_buffer[ (df_predict_buffer['n_buffer'] == buffer_size) & (df_predict_buffer['target']==1.) ]['target'].sum()\n",
    "    print('In a search range of {:.0f} cells, {:.0f} matches are found'.format(buffer_size, n_hits))\n",
    "\n",
    "\n",
    "## get buffers based on location of match \n",
    "buffer_0 = df_predict_buffer[ (df_predict_buffer['n_buffer'] == 0) & (df_predict_buffer['target']==1) ]['tag'].unique()\n",
    "buffer_1 = df_predict_buffer[ (df_predict_buffer['n_buffer'] == 1) & (df_predict_buffer['target']==1) ]['tag'].unique()\n",
    "buffer_2 = df_predict_buffer[ (df_predict_buffer['n_buffer'] == 2) & (df_predict_buffer['target']==1) ]['tag'].unique()\n",
    "buffer_3 = df_predict_buffer[ (df_predict_buffer['n_buffer'] == 3) & (df_predict_buffer['target']==1) ]['tag'].unique()\n",
    "buffer_4 = df_predict_buffer[ (df_predict_buffer['n_buffer'] == 4) & (df_predict_buffer['target']==1) ]['tag'].unique()\n",
    "\n",
    "## split with same ratio \n",
    "train_val_0, test_0 = train_test_split(buffer_0, test_size=0.15, random_state=21)\n",
    "train_val_1, test_1 = train_test_split(buffer_1, test_size=0.15, random_state=21)\n",
    "train_val_2, test_2 = train_test_split(buffer_2, test_size=0.15, random_state=21)\n",
    "train_val_3, test_3 = train_test_split(buffer_3, test_size=0.15, random_state=21)\n",
    "train_val_4, test_4 = train_test_split(buffer_4, test_size=0.15, random_state=21)\n",
    "train_val_out, test_out = train_test_split(gauge_outside, test_size=0.15, random_state=21)\n",
    "\n",
    "## combine all sets \n",
    "train_val_sets = (train_val_0, train_val_1, train_val_2, train_val_3, train_val_4, train_val_out)\n",
    "test_sets = (test_0, test_1, test_2, test_3, test_4, test_out)\n",
    "\n",
    "id_train_val = np.concatenate( train_val_sets )\n",
    "id_test = np.concatenate( test_sets )\n",
    "\n",
    "## split data \n",
    "df_train_val = df_predict_buffer[ df_predict_buffer['tag'].isin(id_train_val)].copy()\n",
    "df_test = df_predict_buffer[ df_predict_buffer['tag'].isin(id_test) ].copy()\n",
    "\n",
    "## VARY SEARCHING AREA\n",
    "## drop all rows with n_buffer values > set_buffer_size\n",
    "## if match outside buffer, remaining set will be all zeros --> no buffer in current search area \n",
    "df_train_val = df_train_val[ df_train_val['n_buffer'] <= set_buffer_size ].copy()\n",
    "df_test = df_test[ df_test['n_buffer'] <= set_buffer_size ].copy()\n",
    "\n",
    "## update in_buffer values \n",
    "for gauge_id in id_train_val:\n",
    "    idx = df_train_val[ df_train_val['tag']== gauge_id].index \n",
    "    sum_target = df_train_val.loc[idx, 'target'].sum()\n",
    "    if sum_target < 1:\n",
    "        df_train_val.loc[idx, 'in_buffer'] = 0 \n",
    "\n",
    "for gauge_id in id_test:\n",
    "    idx = df_test[ df_test['tag']== gauge_id].index \n",
    "    sum_target = df_test.loc[idx, 'target'].sum()\n",
    "    if sum_target < 1:\n",
    "        df_test.loc[idx, 'in_buffer'] = 0 \n",
    "\n",
    "######         \n",
    "print('\\nShow result of split:')\n",
    "print('Train val set:')\n",
    "print( df_train_val[df_train_val['target']==1].groupby(by='n_buffer')['target'].sum() )\n",
    "print('\\nTest set')\n",
    "print( df_test[df_test['target']==1].groupby(by='n_buffer')['target'].sum() )\n",
    "print(f'\\n{len(id_train_val)} samples in total train val set')\n",
    "print(f'{len(id_test)} samples in total train val set')\n",
    "\n",
    "## list feature columns \n",
    "feature_columns = df_similarity.columns.values \n",
    "\n",
    "## remove non_feature cols \n",
    "if omit_coords:\n",
    "    for col in non_feature_cols+target_col+coord_cols:\n",
    "        col_ix = np.where(feature_columns==col)[0]\n",
    "        feature_columns = np.delete(feature_columns, col_ix)\n",
    "\n",
    "else:\n",
    "    for col in non_feature_cols+target_col:\n",
    "        col_ix = np.where(feature_columns==col)[0]\n",
    "#         feature_columns = np.delete(feature_columns, col_ix)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a79bd1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_ix = np.concatenate((id_train_val, id_test))\n",
    "\n",
    "# subset = meta_gauge.loc[all_ix, ['d_X_cell', 'd_Y_cell']]\n",
    "\n",
    "# subset['buffer_shift'] = 0 \n",
    "# x0 = 0 \n",
    "# y0 = 0 \n",
    "\n",
    "# for ix in subset.index:\n",
    "#     row = subset.loc[ix]\n",
    "    \n",
    "#     x_shift = ((row['d_X_cell'])**2)**0.5 \n",
    "#     y_shift = ((row['d_Y_cell'])**2)**0.5 \n",
    "#     max_shift = np.max([x_shift, y_shift])\n",
    "      \n",
    "#     subset.loc[ix,'buffer_shift'] = max_shift\n",
    "        \n",
    "# analyse_distances = subset['buffer_shift'].value_counts()\n",
    "\n",
    "# x_vals = analyse_distances.index\n",
    "# y_vals = analyse_distances.values \n",
    "\n",
    "\n",
    "# save_dir = Path(r\"C:\\Users\\mvand\\Documents\\Master EE\\Year 4\\Thesis\\meetings\\2021_06_15_EWC_sympo\\media\") \n",
    "# fn_1 = save_dir / 'shift_bins.png'\n",
    "# fn_2 = save_dir / 'shift_bins_zoom.png'\n",
    "\n",
    "# plt.figure(figsize=(12,4))\n",
    "# plt.bar(x_vals, y_vals)\n",
    "# plt.yscale('log')\n",
    "# plt.grid()\n",
    "# plt.xlabel('Shift nearest cell to matched cell', size=14);\n",
    "# plt.ylabel('n observations (log-scaled)', size=14);\n",
    "# plt.xticks(size=12);\n",
    "# plt.yticks(size=12);\n",
    "# plt.savefig(fn_1);\n",
    "\n",
    "# plt.figure(figsize=(8,4))\n",
    "# plt.bar(x_vals, y_vals)\n",
    "# plt.xlim(-1, 5)\n",
    "# plt.grid()\n",
    "# plt.xlabel('Shift nearest cell to matched cell', size=14);\n",
    "# plt.ylabel('n observations', size=14);\n",
    "# plt.xticks(size=12);\n",
    "# plt.yticks(size=12);\n",
    "# plt.savefig(fn_2);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a48f8b1",
   "metadata": {},
   "source": [
    "Load or calculate benchmark predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdecaf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmarks(df, id_col, target_col, obs_dir = None, sim_dir = None,\n",
    "               methods = ['CC', 'NSE', 'RMSE', 'KGE'],\n",
    "               fn_out = None): \n",
    "    \n",
    "    df_out = pd.DataFrame() \n",
    "    \n",
    "    df_out['ID'] = df.index.values \n",
    "    df_out[id_col] = df[id_col].values \n",
    "    df_out[target_col] = df[target_col].values \n",
    "    df_out = df_out.set_index('ID')\n",
    "    \n",
    "    buffer_idx = df_out[id_col].unique()\n",
    "    \n",
    "    for i in tqdm(range(len(buffer_idx))):\n",
    "        buffer_id = buffer_idx[i]\n",
    "        \n",
    "        _out = df_out[ df_out[id_col] == buffer_id] \n",
    "                \n",
    "        for method in methods:\n",
    "            \n",
    "            out_col = '{}_hat-{}'.format(target_col, method)\n",
    "            \n",
    "            if method in ['NSE', 'RMSE', 'KGE']:\n",
    "                \n",
    "                assert obs_dir.exists() , '[ERROR] obs_dir not specified or not found'\n",
    "                assert sim_dir.exists(), '[ERROR] sim_dir not specified or not found'\n",
    "                \n",
    "                fn_obs = obs_dir / '{}_Q_Day.Cmd.txt'.format(buffer_id)\n",
    "                fn_sim = sim_dir / 'buffer_{}_size-4.nc'.format(buffer_id) \n",
    "                \n",
    "                ## load observations\n",
    "                df_obs = pd.read_csv(fn_obs, skiprows=36, delimiter=';', encoding='cp850')\n",
    "                df_obs['Q_obs'] = df_obs[' Value'] \n",
    "                df_obs['date'] = pd.to_datetime(df_obs['YYYY-MM-DD'], yearfirst=True,\n",
    "                                               format='%Y-%m-%d')\n",
    "                df_obs = df_obs.drop(columns=[' Value', 'YYYY-MM-DD', 'hh:mm'])\n",
    "                df_obs = df_obs.set_index('date')\n",
    "                df_obs.loc[ df_obs['Q_obs'] == -999., 'Q_obs'] = np.nan \n",
    "                df_obs = df_obs.loc[df_obs.index >= '1991']\n",
    "                \n",
    "                ## load simulations \n",
    "                ds_sim = xr.open_dataset(fn_sim)\n",
    "                df_sim_list = ds_sim.to_dataframe().reset_index() \n",
    "                \n",
    "                df_sim = pd.DataFrame()\n",
    "                df_sim['date'] = pd.to_datetime( df_sim_list['time'].unique() )\n",
    "                df_sim  = df_sim.set_index('date')\n",
    "                \n",
    "                for i, x_cell in enumerate( df_sim_list['x'].unique() ):\n",
    "                    for j, y_cell in enumerate( df_sim_list['y'].unique() ):\n",
    "                        \n",
    "                        _df = df_sim_list[ (df_sim_list['x'] == x_cell) & (df_sim_list['y'] == y_cell) ]\n",
    "                        \n",
    "                        cell_id = '{}_{}{}'.format(buffer_id, int(i+1), int(j+1)) \n",
    "                        time = pd.to_datetime(_df['time'])\n",
    "                        df_sim.loc[time, cell_id] = _df['dis24'].values \n",
    "                \n",
    "                ## set simulations to observations extent \n",
    "                max_date = df_obs.tail(1).index.values[0]\n",
    "                min_date = df_obs.head(1).index.values[0]\n",
    "                df_sim = df_sim.loc[ (df_sim.index>=min_date) & (df_sim.index <= max_date)].copy()    \n",
    "                \n",
    "                ## mask and drop NaN values based on observations \n",
    "                gauge_mask = df_obs[df_obs['Q_obs'].isnull()>0].index \n",
    "                df_obs = df_obs.drop(index=gauge_mask)\n",
    "                df_sim = df_sim.drop(index=gauge_mask)   \n",
    "                \n",
    "                ## calculate selected metrics in the buffer \n",
    "                for cell in df_sim.columns:\n",
    "                    \n",
    "                    if method == 'NSE':\n",
    "                        try:\n",
    "                            res = he.evaluator(he.nse, df_sim[cell].values, df_obs['Q_obs'].values )[0]\n",
    "                        except:\n",
    "                            print(buffer_id)\n",
    "                            print(df_obs.head(2))\n",
    "                            print(df_obs.tail(2))\n",
    "                            print(df_sim.head(2).index)\n",
    "                            print(df_sim.tail(2).index)\n",
    "                            \n",
    "                        \n",
    "                    if method == 'RMSE':\n",
    "                        res = he.evaluator(he.rmse, df_sim[cell].values, df_obs['Q_obs'].values )[0]  \n",
    "                        \n",
    "                    if method == 'KGE':\n",
    "                        res, r, alpha, beta = he.evaluator(he.kge, df_sim[cell].values, df_obs['Q_obs'].values ) \n",
    "\n",
    "                    df_out.loc[cell, method] = res \n",
    "                \n",
    "                if method == 'NSE':\n",
    "                    y_hat_ix = df_out.loc[_out.index, method].idxmax()\n",
    "                if method == 'RMSE':\n",
    "                    y_hat_ix = df_out.loc[_out.index, method].idxmin()\n",
    "                if method == 'KGE':\n",
    "                    y_hat_ix = df_out.loc[_out.index, method].idxmax()\n",
    "                        \n",
    "                \n",
    "                df_out.loc[_out.index, out_col] = 0 \n",
    "                df_out.loc[y_hat_ix, out_col] = 1\n",
    "\n",
    "            if method == 'CC': \n",
    "                df_out.loc[ _out.index, out_col ] = 0 \n",
    "                center_ix = df[ (df['n_buffer'] == 0) & (df[id_col]==buffer_id)].index\n",
    "                df_out.loc[center_ix, out_col] = 1 \n",
    "            \n",
    "    df_out = df_out.dropna(axis=0)\n",
    "                \n",
    "    if fn_out == None:\n",
    "        return df_out\n",
    "    else:\n",
    "        df_out.to_csv(fn_out)\n",
    "        return fn_out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f5bd0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load or calculate benchmarks \n",
    "calc_bench = False\n",
    "fn_bench = signature_dir / \"S1_benchmarks.csv\" \n",
    "\n",
    "if calc_bench:\n",
    "\n",
    "    base_dir = Path(r\"C:\\Users\\mvand\\Documents\\Master EE\\Year 4\\Thesis\\data\\training_data_S1\") \n",
    "\n",
    "    obs_dir = base_dir / \"V1\" \n",
    "    sim_dir = base_dir / \"efas_output_nc\"\n",
    "\n",
    "    df_benchmarks = benchmarks(df_similarity, 'tag', target_col[0], \n",
    "                               obs_dir, sim_dir) #, fn_out = fn_bench)\n",
    "\n",
    "else:\n",
    "    df_benchmarks = pd.read_csv(fn_bench, index_col=0)\n",
    "\n",
    "df_benchmarks\n",
    "\n",
    "df_benchmarks['range_target'] = df_similarity['range_target']\n",
    "df_benchmarks.loc[ df_benchmarks['range_target'].isnull() ] = 0 \n",
    "\n",
    "benchmarks_train_val = df_benchmarks[df_benchmarks['tag'].isin(id_train_val)]\n",
    "benchmarks_test = df_benchmarks[df_benchmarks['tag'].isin(id_test)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91986f17",
   "metadata": {},
   "source": [
    "If necessary, data can be transformed into a grid for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8634ff97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_grid(df, feature_cols, target_col, range_col=None, n_elements=9):\n",
    "    \n",
    "    ## create  multidimensional array with NaN value: -999. \n",
    "    feature_grid = np.ones((len(feature_cols), n_elements, n_elements)) * -999. \n",
    "    target_grid = np.zeros((n_elements, n_elements))\n",
    "    \n",
    "    range_grid = None \n",
    "    if range_col != None:\n",
    "        range_grid = np.zeros((n_elements, n_elements))\n",
    "        \n",
    "    ## fill array \n",
    "    for ix in df.index:\n",
    "        grid_x, grid_y = int(ix.split('_')[-1][0]), int(ix.split('_')[-1][1]) \n",
    "                \n",
    "        feature_grid[:, int(grid_y-1), int(grid_x-1) ] = df.loc[ix, feature_cols].values \n",
    "        target_grid[int(grid_y-1),int(grid_x-1)] = df.loc[ix, target_col] \n",
    "        \n",
    "        if range_col!= None:\n",
    "            range_grid[int(grid_y-1),int(grid_x-1)] = df.loc[ix, range_col]\n",
    "                     \n",
    "    return feature_grid, target_grid, range_grid\n",
    "\n",
    "def reshape_to_grid(df, feature_cols, target_col, range_col=None, id_col='tag', buffer_size=4):\n",
    "    \n",
    "    n_elements = int(1 + (2*buffer_size))\n",
    "    \n",
    "    ## create emtpy output grids \n",
    "    grid_features = np.zeros((df[id_col].nunique(), len(feature_cols), n_elements, n_elements )) \n",
    "    grid_targets = np.zeros((df[id_col].nunique(), n_elements, n_elements))\n",
    "    \n",
    "    grid_range = None\n",
    "    if range_col != None:\n",
    "        grid_range = np.zeros((df[id_col].nunique(), n_elements, n_elements))\n",
    "    \n",
    "    ## reshape each buffer \n",
    "    for i, idx in tqdm(enumerate(df[id_col].unique())):\n",
    "                \n",
    "        df_buffer = df[ df[id_col] == idx ] \n",
    "        \n",
    "        grid_buffer_features, grid_buffer_target, grid_buffer_range = feature_grid(df_buffer, feature_cols, \n",
    "                                                                                   target_col, range_col, n_elements) \n",
    "            \n",
    "        grid_features[i] = grid_buffer_features\n",
    "        grid_targets[i] = grid_buffer_target\n",
    "        \n",
    "        if range_col != None:\n",
    "            grid_range[i] = grid_buffer_range\n",
    "    \n",
    "    if range_col == None:\n",
    "        return grid_features, grid_targets \n",
    "    \n",
    "    return grid_features, grid_targets, grid_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e26f14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "462it [00:00, 635.46it/s]\n",
      "86it [00:00, 581.77it/s]\n",
      "462it [00:00, 705.35it/s]\n",
      "462it [00:00, 660.43it/s]\n",
      "86it [00:00, 609.93it/s]\n"
     ]
    }
   ],
   "source": [
    "if grid_format:\n",
    "    \n",
    "    ## retransform datarame to grids \n",
    "    n_elements = min( max(2, int(set_buffer_size+1)) , 4)\n",
    "    \n",
    "    ## CASE 1 \n",
    "    ## for determining target in buffer \n",
    "    X1_train_val, y1_train_val= reshape_to_grid(df_train_val, feature_columns,\n",
    "                                                target_buffer[0], buffer_size=n_elements) \n",
    "    X1_test, y1_test = reshape_to_grid(df_test, feature_columns, \n",
    "                                       target_buffer[0], buffer_size=n_elements) \n",
    "    \n",
    "    ## flatten X1 \n",
    "    n_samples, n_features, n_rows, n_cols = X1_train_val.shape \n",
    "    X1_train_val_flatten = np.reshape(X1_train_val, (n_samples, int(n_features*n_rows*n_cols) ) )\n",
    "    X1_test_flatten = np.reshape(X1_test, (len(X1_test), int(n_features*n_rows*n_cols))  )\n",
    "    \n",
    "    \n",
    "    ## for later plotting  \n",
    "    coords, dummy_target = reshape_to_grid(df_train_val, ['x', 'y',],\n",
    "                                      target_buffer[0], buffer_size = n_elements)\n",
    "    \n",
    "    ## simplify y1 values to single vector \n",
    "    y1_train_val_vector = np.zeros(n_samples)\n",
    "    for i in range(n_samples):\n",
    "        y1_train_val_vector[i] = np.max(y1_train_val[i])\n",
    "    \n",
    "    y1_test_vector = np.zeros(len(X1_test))\n",
    "    for i in range(len(y1_test_vector)):\n",
    "        y1_test_vector[i] = np.max(y1_test[i])\n",
    "    \n",
    "    \n",
    "    ## CASE 2 \n",
    "    ## for determining location of cell \n",
    "    ## for determining target in buffer \n",
    "    X2_train_val, y2_train_val, y2_train_val_range = reshape_to_grid(df_train_val, feature_columns,\n",
    "                                                                     target_col[0], range_col = range_col,\n",
    "                                                                     buffer_size=n_elements) \n",
    "    \n",
    "    X2_test, y2_test, y2_test_range = reshape_to_grid(df_test, feature_columns, \n",
    "                                                      target_col[0], range_col=range_col, \n",
    "                                                      buffer_size=n_elements) \n",
    "    \n",
    "    ## flatten X2 \n",
    "    n_samples, n_features, n_rows, n_cols = X2_train_val.shape \n",
    "    X2_train_val_flatten = np.reshape(X1_train_val, (n_samples, int(n_features*n_rows*n_cols) ) )\n",
    "    y2_train_val_flatten = np.reshape(y2_train_val, (n_samples, int(n_rows*n_cols) ) )\n",
    "    y2_train_val_range_flatten = np.reshape(y2_train_val_range, (n_samples, int(n_rows*n_cols) ) )\n",
    "    \n",
    "    n_test_samples = len(X2_test)\n",
    "    X2_test_flatten = np.reshape(X2_test, (n_test_samples, int(n_features*n_rows*n_cols))  )  \n",
    "    y2_test_flatten = np.reshape(y2_test, (n_test_samples, int(n_rows*n_cols) ) )\n",
    "    y2_test_range_flatten = np.reshape(y2_test_range, (n_test_samples, int(n_rows*n_cols) ) )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c64caa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568d0cff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "262596f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## sum y_target vals to see distribution \n",
    "# x_ticks = np.arange(0,9) + 0.5 \n",
    "# labels = np.arange(-4, 5, 1)\n",
    "\n",
    "# train_sum = y2_train_val.sum(axis=0)\n",
    "# test_sum = y2_test.sum(axis=0)\n",
    "# total_sum = train_sum + test_sum\n",
    "\n",
    "# plt.figure(figsize=(12,8))\n",
    "# sns.heatmap( total_sum , cmap =  'Blues', annot=True, fmt='g', \n",
    "#            cbar = False, annot_kws={\"size\":16})\n",
    "\n",
    "# plt.xticks(x_ticks, ['{}'.format(v) for v in labels], size = 14);\n",
    "# plt.yticks(x_ticks, ['{}'.format(v) for v in labels][::-1], rotation=0, size = 14);\n",
    "# plt.xlabel('Shift from center coordinate', size = 18);\n",
    "# plt.ylabel('Shift from center coordinate', size = 18);\n",
    "\n",
    "# save_dir = Path(r\"C:\\Users\\mvand\\Documents\\Master EE\\Year 4\\Thesis\\meetings\\2021_06_15_EWC_sympo\\media\") \n",
    "# fn = save_dir / 'grid_shift_view.png'\n",
    "# plt.savefig(fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab838881",
   "metadata": {},
   "source": [
    "Now, with k-fold cross validation find optimal algorithm settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6bf1c24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_performances(y, y_hat, k, model_name, grid_format = False,\n",
    "                          id_col='tag', target_col='range_target'):\n",
    "    \n",
    "    ## customize classifcation evaluation\n",
    "    TP = 0 \n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0 \n",
    "    n_targets = 0 \n",
    "    \n",
    "    if grid_format:\n",
    "        \n",
    "        n_samples, n_classes = y.shape \n",
    "        n_cells = n_samples * n_classes \n",
    "        n_samples, n_classes = y.shape       \n",
    "        \n",
    "        ## reshape to count TP, TN, FP, FN \n",
    "        for sample in range(len(y)):\n",
    "            \n",
    "            row_y = y[sample]\n",
    "            row_y_hat = y_hat[sample]\n",
    "            \n",
    "            ix_y = np.where(row_y > 0)[0]\n",
    "            ix_y_hat = np.where(row_y_hat > 0)[0]\n",
    "            \n",
    "            ## positve target label exists \n",
    "            if len(ix_y) > 0:\n",
    "                n_targets += 1 \n",
    "            \n",
    "                ## if a positive target label exists, and \n",
    "                ## prediction contains a positive label, \n",
    "                ## prediction is either true or false \n",
    "                if len(ix_y_hat) > 0:\n",
    "                    \n",
    "                    ## if prediction true, TP = 1 and TN are remaining cells \n",
    "                    if ix_y_hat[0] in ix_y:\n",
    "                        TP += 1 \n",
    "                        TN += (n_classes-1)\n",
    "                    \n",
    "                    ## if prediction is false, TP = 1 and TN are remaining cells \n",
    "                    if ix_y_hat[0] not in ix_y:\n",
    "                        FP += 1                    # or sum(row_y) ?, so 1-3 false positives)\n",
    "                        TN += (n_classes-1)        # or n_classes - sum(row_y)\n",
    "                \n",
    "                ## if a positive target label exists, but\n",
    "                ## prediction is all zeros, false negatives are counted \n",
    "                else: \n",
    "                    FN += 1 \n",
    "                    TN += (n_classes-1)\n",
    "            \n",
    "            ## no positive labels \n",
    "            if len(ix_y) == 0:\n",
    "                ## and prediction correctly all zero as well \n",
    "                if len(ix_y_hat) == 0:\n",
    "                    TN += n_classes \n",
    "                    \n",
    "                ## else one label is incorrectly labelled positive \n",
    "                ## rest is true negative \n",
    "                else:\n",
    "                    FP += 1\n",
    "                    TN += (n_classes-1)\n",
    "                    \n",
    "    else:\n",
    "        \n",
    "        ## reshape to count TP, TN, FP, FN \n",
    "        y['hat'] = y_hat\n",
    "        n_cells = len(y) \n",
    "        \n",
    "        for ix in y[id_col].unique():\n",
    "            _buffer = y[ y[id_col] == ix ]\n",
    "            \n",
    "            n_classes = len(_buffer)\n",
    "            \n",
    "            _y = _buffer[target_col]\n",
    "            _y_hat = _buffer['hat']\n",
    "            \n",
    "            ## if positive target\n",
    "            if _y.sum() > 0:\n",
    "                n_targets += 1 \n",
    "                _y_ix = _y[ _y>0].index.values \n",
    "\n",
    "                ## if positive prediction \n",
    "                if _y_hat.sum() > 0:\n",
    "                    _y_hat_ix = _y_hat.idxmax() \n",
    "                    \n",
    "                    ## if correct prediction \n",
    "                    if _y_hat_ix in _y_ix:\n",
    "                        TP += 1 \n",
    "                        TN += (n_classes-1)\n",
    "                    ## incorrect prediction\n",
    "                    else:\n",
    "                        FP += 1\n",
    "                        TN += (n_classes-1)\n",
    "                        \n",
    "                ## if no positive prediction\n",
    "                else:\n",
    "                    FN += 1 \n",
    "                    TN += (n_classes-1)\n",
    "                    \n",
    "            ## if no positive targets \n",
    "            else:\n",
    "                ## if no positive predictions \n",
    "                if _y_hat.sum() == 0:\n",
    "                    TN += n_classes\n",
    "                ## if positive predictions \n",
    "                else:\n",
    "                    FP += 1\n",
    "                    TN += (n_classes-1)\n",
    "    \n",
    "    ## calculate metrics \n",
    "    acc = (TP+TN) / (TP+TN+FP+FN)\n",
    "    b_acc = 0.5 * ( (TP/(TP+FN+1e-6)) + (TN/(TN+FP+1e-6))  )\n",
    "    prec = TP / (TP+FP+1e-6)\n",
    "    rec = TP / (TP+FN+1e-6) \n",
    "    f1 = (prec*rec) / (prec+rec+1e-6) \n",
    "    hit_rate = TP / n_targets \n",
    "\n",
    "    return_df = pd.DataFrame({\n",
    "                            'k': [k],\n",
    "                            'model': [model_name],\n",
    "                            'accuracy': [acc],\n",
    "                            'balanced_acc': [b_acc],\n",
    "                            'precision': [prec],\n",
    "                            'recall': [rec],\n",
    "                            'f1': [f1],\n",
    "                            'hit_rate': [hit_rate],\n",
    "                            'n': [n_targets],\n",
    "                            'N': [n_cells],\n",
    "                            'TP': [TP],\n",
    "                            'TN': [TN],\n",
    "                            'FP': [FP],\n",
    "                            'FN': [FN]\n",
    "                              })     \n",
    "    return return_df\n",
    "\n",
    "\n",
    "def buffer_classifier(df_val, y_prob, p0_col, p1_col,\n",
    "                     prediction_col, id_col, prob_threshold):\n",
    "        \n",
    "    df_val[p0_col] = y_prob[:,0]\n",
    "    df_val[p1_col] = y_prob[:,1] \n",
    "    df_val[prediction_col] = 0.\n",
    "    \n",
    "    for ix in df_val[id_col].unique():\n",
    "        max_ix = df_val[ (df_val[id_col]==ix) ][p1_col].idxmax()\n",
    "        \n",
    "        if df_val.loc[max_ix, p1_col] >= prob_threshold:\n",
    "            df_val.loc[max_ix, prediction_col] = 1.\n",
    "\n",
    "    return df_val\n",
    "\n",
    "def k_foldCV(X, y = [],  grid_format = False, K = 5, \n",
    "             id_col = None, feature_cols = [], target_col = None,\n",
    "             range_col = None, y_range=[],\n",
    "             n_subsample = 1, do_norm = True, do_scale = True, \n",
    "             do_PCA = True, n_pca = 0.9, whiten_pca = False,\n",
    "             methods = ['LR-2'], prob_threshold = 0.5, \n",
    "             return_classification = False, gauge_id_list = [],\n",
    "             take_subsample = True):\n",
    "             \n",
    "#              benchmarks = ['CC', 'NSE', 'RMSE', 'KGE'],\n",
    "#              df_benchmarks = None):\n",
    "\n",
    "    df_performance = pd.DataFrame()\n",
    "    \n",
    "    ## for returning classification \n",
    "    if grid_format:\n",
    "#         ds_collect_val = xr.Dataset() \n",
    "        \n",
    "        n_cells = y.shape[1] \n",
    "        ## assume square grid \n",
    "        n_buffer = int(n_cells**0.5)\n",
    "    else:\n",
    "        df_collect_val = pd.DataFrame()\n",
    "        \n",
    "    \n",
    "    if grid_format:\n",
    "        assert len(y) > 0, 'target column not specified'\n",
    "        sort_ix = list(range(len(X)))\n",
    "                \n",
    "        \n",
    "    if not grid_format:\n",
    "        assert id_col != None, 'buffer id col not specified'\n",
    "        assert target_col != None, 'buffer id col not specified'\n",
    "        assert len(feature_cols) > 0, 'feature columns not specified'\n",
    "        \n",
    "        sort_ix = X[id_col].unique()\n",
    "    \n",
    "    \n",
    "    ## shuffle and split ids \n",
    "    np.random.seed(26)\n",
    "    np.random.shuffle(sort_ix)\n",
    "    \n",
    "    ## split in K folds \n",
    "    k_split_idx = np.array_split(sort_ix, K)\n",
    "    \n",
    "    for k in tqdm(range(K)):\n",
    "        \n",
    "        ## split samples \n",
    "        id_val = k_split_idx[k]\n",
    "        id_train = np.setdiff1d(sort_ix, id_val)\n",
    "        \n",
    "        if grid_format:\n",
    "            X_train = X[id_train]\n",
    "            y_train = y[id_train]\n",
    "            \n",
    "            X_val = X[id_val]\n",
    "            y_val = y[id_val]\n",
    "            \n",
    "            if len(y_range) > 0:\n",
    "                y_val_range = y_range[id_val]\n",
    "        \n",
    "        if not grid_format:\n",
    "            \n",
    "            set_train = X[ X[id_col].isin(id_train) ]\n",
    "            set_val = X[ X[id_col].isin(id_val)]\n",
    "            \n",
    "            if take_subsample:\n",
    "                ## subsample training set only \n",
    "                set_train_1 = set_train[ set_train[target_col] == 1 ]\n",
    "\n",
    "                set_train_0_ix = []\n",
    "                for i, idx in enumerate(id_train):                    \n",
    "                    _df = set_train[ (set_train[target_col] != 1) & (set_train[id_col] == idx) ] \n",
    "                    if len(_df) > 0:     \n",
    "                        subsamples = _df.sample(n=n_subsample).index \n",
    "                        for sample in subsamples:\n",
    "                            set_train_0_ix.append(sample)\n",
    "\n",
    "                set_train_0 = set_train.loc[set_train_0_ix]\n",
    "\n",
    "                ## sample in_buffer = 0 samples \n",
    "                set_not_in_buffer = set_train[set_train['in_buffer']!=1]\n",
    "                n_sample = set_not_in_buffer[id_col].nunique()\n",
    "\n",
    "                ## add subsampled sets             \n",
    "                sample_train = set_train_1.append(set_train_0)\n",
    "                sample_train = sample_train.append( set_not_in_buffer.sample(n=n_sample) )\n",
    "            \n",
    "            ## or upsample!\n",
    "#             else:\n",
    "            \n",
    "            ## split X and y \n",
    "            X_train = sample_train[feature_cols]\n",
    "            y_train = sample_train[target_col]\n",
    "\n",
    "            X_val = set_val[feature_cols]\n",
    "            y_val = set_val[target_col]\n",
    "            df_y_val = set_val.copy() #[[id_col, target_col]].copy()#.to_frame()\n",
    "\n",
    "            if range_col is not None:\n",
    "                y_val_range = set_val[[range_col, id_col]].copy()\n",
    "        \n",
    "        ## preprocess training data \n",
    "        if do_norm:\n",
    "            min_samples = len(X_train)\n",
    "            nm = QuantileTransformer(output_distribution='normal',\n",
    "                                    n_quantiles = int(min(1000, min_samples)))\n",
    "            X_train = nm.fit_transform(X_train)\n",
    "        else:\n",
    "            nm = None \n",
    "        \n",
    "        if do_scale:\n",
    "            sc = MinMaxScaler([0,1])\n",
    "            X_train = sc.fit_transform(X_train)\n",
    "        else:\n",
    "            sc = None\n",
    "        \n",
    "        if do_PCA:\n",
    "            pca = PCA(n_components = n_pca, whiten = whiten_pca)\n",
    "            pca.fit(X_train)\n",
    "            X_train = pca.transform(X_train)\n",
    "        else:\n",
    "            pca = None \n",
    "            \n",
    "        ## prepare trainin data \n",
    "        if do_norm:\n",
    "            X_val = nm.transform(X_val)\n",
    "        if do_scale:\n",
    "            X_val = sc.transform(X_val)\n",
    "        if do_PCA:\n",
    "            X_val = pca.transform(X_val)        \n",
    "        \n",
    "        ##  test models \n",
    "        for method in methods: \n",
    "            \n",
    "            prediction_col = '{}_{}'.format(method, target_col)\n",
    "            p0_col = '{}_p0'.format(method)\n",
    "            p1_col = '{}_p1'.format(method)\n",
    "                        \n",
    "            if 'LR' in method:\n",
    "                \n",
    "                ## train model \n",
    "                lr = LogisticRegression(max_iter=1000)\n",
    "                \n",
    "                if grid_format:\n",
    "                    lr = OneVsRestClassifier( lr )\n",
    "                \n",
    "                lr.fit(X_train, y_train)\n",
    "                \n",
    "                ## evaluate                                 \n",
    "                if 'LR-1' in method:\n",
    "                    y_val_hat = lr.predict(X_val)\n",
    "                        \n",
    "                if 'LR-2' in method:\n",
    "                    y_val_hat_prob = lr.predict_proba(X_val)\n",
    "                    \n",
    "                    if grid_format:\n",
    "                        y_val_hat = np.where( y_val_hat_prob >= prob_threshold, 1, 0)\n",
    "                    else:\n",
    "                        \n",
    "                        df_y_val = buffer_classifier(df_y_val, y_val_hat_prob,\n",
    "                                                    p0_col, p1_col, prediction_col,\n",
    "                                                    id_col, prob_threshold)\n",
    "                        \n",
    "                        y_val_hat = df_y_val[prediction_col]  \n",
    "\n",
    "                        \n",
    "                \n",
    "            if 'RF' in method:\n",
    "                \n",
    "                ## train model \n",
    "                rfc = RandomForestClassifier()                \n",
    "                rfc.fit(X_train, y_train)\n",
    "                \n",
    "                if 'RF-1' in method:\n",
    "                    y_val_hat = rfc.predict(X_val)\n",
    "                     \n",
    "                if 'RF-2' in method:\n",
    "                    _y_val_hat_prob = rfc.predict_proba(X_val)\n",
    "                    \n",
    "                    if grid_format:\n",
    "                        y_val_hat = np.zeros(y_val.shape)\n",
    "                        y_val_hat_prob = np.zeros(y_val.shape)\n",
    "\n",
    "                        for sample in range(len(_y_val_hat_prob)):\n",
    "                            if _y_val_hat_prob[sample].shape[1] >1:\n",
    "                                p1 = _y_val_hat_prob[sample][:,1]\n",
    "                                _y_val_hat = np.zeros(len(p1)) \n",
    "                                \n",
    "                                ix_max = p1.argmax() \n",
    "                                \n",
    "                                if p1[p1.argmax()] >= prob_threshold:\n",
    "                                    _y_val_hat[ix_max] = 1. \n",
    "\n",
    "                                y_val_hat[:,sample] = _y_val_hat \n",
    "                                y_val_hat_prob[:,sample] += p1 \n",
    "                    \n",
    "                    else:\n",
    "                        y_val_hat_prob = _y_val_hat_prob \n",
    "                        df_y_val = buffer_classifier(df_y_val, y_val_hat_prob,\n",
    "                                                    p0_col, p1_col, prediction_col,\n",
    "                                                    id_col, prob_threshold)\n",
    "                        \n",
    "                        y_val_hat = df_y_val[prediction_col]\n",
    "                    \n",
    "            if 'SVM' in method:\n",
    "                \n",
    "                svc = SVC(probability=True)\n",
    "                \n",
    "                if grid_format:\n",
    "                    svc = OneVsRestClassifier(svc) \n",
    "                \n",
    "                svc.fit(X_train, y_train)\n",
    "                \n",
    "                if 'SVM-1' in method:\n",
    "                    y_val_hat = svc.predict(X_val)\n",
    "                    \n",
    "                if 'SVM-2' in method:\n",
    "                    y_val_hat_prob = svc.predict_proba(X_val)\n",
    "                    \n",
    "                    if grid_format:\n",
    "                        y_val_hat = np.where( y_val_hat_prob >= prob_threshold, 1, 0)\n",
    "                        \n",
    "                    else:   \n",
    "                        \n",
    "                        df_y_val = buffer_classifier(df_y_val, y_val_hat_prob,\n",
    "                                                    p0_col, p1_col, prediction_col,\n",
    "                                                    id_col, prob_threshold)\n",
    "                        \n",
    "                        y_val_hat = df_y_val[prediction_col]                \n",
    "\n",
    "            if 'k-nn' in method:\n",
    "                \n",
    "                if not do_PCA:\n",
    "                    \n",
    "                    if not do_norm:\n",
    "                        min_samples = len(X_train)\n",
    "                        nm = QuantileTransformer(output_distribution='normal',\n",
    "                                                n_quantiles = int(min(1000, min_samples)))\n",
    "                        X_train = nm.fit_transform(X_train)\n",
    "                        X_val = nm.transform(X_train)\n",
    "                    \n",
    "                    if not do_scale:\n",
    "                        sc = MinMaxScaler([0,1])\n",
    "                        X_train = sc.fit_transorm(X_train)\n",
    "                        X_val = sc.transform(X_val)\n",
    "                    \n",
    "                    pca = PCA(n_components = n_pca)\n",
    "                    pca.fit(X_train)\n",
    "                    X_train = pca.transform(X_train)\n",
    "                    X_val = pca.transform(X_val)\n",
    "                \n",
    "                knn = KNeighborsClassifier() \n",
    "                                \n",
    "                knn.fit(X_train, y_train) \n",
    "                \n",
    "                if 'k-nn-1' in method:\n",
    "                    y_val_hat = knn.predict(X_val)\n",
    "                    \n",
    "                if 'k-nn-2' in method:\n",
    "                    _y_val_hat_prob = knn.predict_proba(X_val)\n",
    "                    \n",
    "                    if grid_format:\n",
    "                        y_val_hat = np.zeros(y_val.shape)\n",
    "                        y_val_hat_prob = np.zeros(y_val.shape)\n",
    "\n",
    "                        for sample in range(len(_y_val_hat_prob)):\n",
    "                            if _y_val_hat_prob[sample].shape[1] >1:\n",
    "                                p1 = _y_val_hat_prob[sample][:,1]\n",
    "                                _y_val_hat = np.zeros(len(p1)) \n",
    "                                \n",
    "                                ix_max = p1.argmax() \n",
    "                                \n",
    "                                if p1[p1.argmax()] >= prob_threshold:\n",
    "                                    _y_val_hat[ix_max] = 1. \n",
    "\n",
    "                                y_val_hat[:,sample] = _y_val_hat\n",
    "                                y_val_hat_prob[:,sample] += p1 \n",
    "                    \n",
    "                    else: \n",
    "                        y_val_hat_prob = _y_val_hat_prob \n",
    "                        df_y_val = buffer_classifier(df_y_val, y_val_hat_prob,\n",
    "                                                    p0_col, p1_col, prediction_col,\n",
    "                                                    id_col, prob_threshold)\n",
    "                        \n",
    "                        y_val_hat = df_y_val[prediction_col]\n",
    "                    \n",
    "            \n",
    "                               \n",
    "            ## calculate perforamnce \n",
    "            df_performance = df_performance.append( benchmark_performances(y_val_range, y_val_hat,\n",
    "                                                                          k, method, grid_format=grid_format) )\n",
    "            if not grid_format:\n",
    "                try:\n",
    "                    df_collect_val.loc[df_y_val.index, df_y_val.columns] = df_y_val \n",
    "                except:\n",
    "                    df_collect_val = df_collect_val.append(df_y_val)\n",
    "            \n",
    "            else:\n",
    "                \n",
    "               ## reshape data back to grid \n",
    "                n_samples = len(id_val)\n",
    "                y_val_gridded = y_val.reshape((n_samples, n_buffer, n_buffer)) \n",
    "                y_val_hat_gridded = y_val_hat.reshape((n_samples, n_buffer, n_buffer))\n",
    "                \n",
    "                if len(y_range) > 0:\n",
    "                    y_val_range_gridded = y_val_range.reshape((n_samples, n_buffer, n_buffer))\n",
    "                \n",
    "                try:\n",
    "                    y_hat_p1_gridded = y_val_hat_prob.reshape((n_samples, n_buffer, n_buffer))\n",
    "                except:\n",
    "                    y_hat_p1_gridded = [] \n",
    "                \n",
    "                if len(gauge_id_list) == 0:\n",
    "                    ix_y_val = [ f'{idv}_{method}_y' for idv in id_val]\n",
    "                    ix_y_val_hat = [f'{idv}_{method}_y_hat' for idv in id_val] \n",
    "                    ix_y_val_range = [f'{idv}_{method}_y_range' for idv in id_val] \n",
    "                    ix_y_hat_p1 = [f'{idv}_{method}_y_p1' for idv in id_val] \n",
    "                \n",
    "                if len(gauge_id_list) > 0:\n",
    "                    ix_y_val = [ f'{gauge_id_list[idg]}_{method}_y' for idg in id_val ]\n",
    "                    ix_y_val_hat = [f'{gauge_id_list[idg]}_{method}_y_hat' for idg in id_val] \n",
    "                    ix_y_val_range = [f'{gauge_id_list[idg]}_{method}_y_range' for idg in id_val] \n",
    "                    ix_y_hat_p1 = [f'{gauge_id_list[idg]}_{method}_y_p1' for idg in id_val] \n",
    "                \n",
    "                \n",
    "                \n",
    "                ds_y_val = xr.DataArray(data=y_val_gridded,\n",
    "                                        dims=['type', 'x', 'y'],\n",
    "                                        coords = dict( val=(['type'], ix_y_val) )\n",
    "                                       )\n",
    "                \n",
    "                ## initilize or concatenate                 \n",
    "                try:\n",
    "                    ds_collect_val = xr.concat([ds_collect_val, ds_y_val], dim='type')\n",
    "                except:\n",
    "                    ds_collect_val = ds_y_val \n",
    "                \n",
    "                ## concat remaining \n",
    "                ds_y_val_hat = xr.DataArray(data=y_val_hat_gridded,\n",
    "                                            dims=['type', 'x', 'y'],\n",
    "                                            coords = dict( val=(['type'], ix_y_val_hat) )\n",
    "                                           )\n",
    "                \n",
    "                ds_collect_val = xr.concat( [ds_collect_val, ds_y_val_hat], dim='type')\n",
    "                \n",
    "                if len(y_range) > 0:\n",
    "                    \n",
    "                    ds_y_val_range = xr.DataArray(data=y_val_range_gridded,\n",
    "                                                dims=['type', 'x', 'y'],\n",
    "                                                coords = dict( val=(['type'], ix_y_val_range) )\n",
    "                                           )\n",
    "                    ds_collect_val = xr.concat( [ds_collect_val, ds_y_val_range], dim='type' )\n",
    "                \n",
    "                if len(y_hat_p1_gridded) > 0:\n",
    "                    \n",
    "                    ds_y_val_range = xr.DataArray(data=y_hat_p1_gridded,\n",
    "                                                dims=['type', 'x', 'y'],\n",
    "                                                coords = dict( val=(['type'], ix_y_hat_p1) )\n",
    "                                           )\n",
    "                    \n",
    "                    ds_collect_val = xr.concat( [ds_collect_val, ds_y_val_range], dim='type' )                    \n",
    "                    \n",
    "    if return_classification:\n",
    "        if grid_format:\n",
    "            return df_performance, ds_collect_val    \n",
    "        else:\n",
    "            return df_performance, df_collect_val                 \n",
    "    else:\n",
    "        return df_performance \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1a67ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## K-fold settings \n",
    "\n",
    "n_folds = 10 \n",
    "algorithms = ['LR-1', 'LR-2', 'RF-1', 'RF-2', 'SVM-1', 'SVM-2', 'k-nn-1', 'k-nn-2'] \n",
    "\n",
    "stat_cols = ['accuracy', 'balanced_acc', 'precision', 'recall', 'f1',  \n",
    "             'TP', 'TN', 'FP', 'FN', 'hit_rate', 'n', 'N']\n",
    "\n",
    "std_cols = ['accuracy', 'balanced_acc', 'precision', 'recall', 'f1',  \n",
    "             'TP', 'TN', 'FP', 'FN', 'hit_rate']\n",
    "\n",
    "test_dir = Path(r\"C:\\Users\\mvand\\Documents\\Master EE\\Year 4\\Thesis\\data\\test_output\\PCA_analysis_1\")\n",
    "\n",
    "if use_set_1:\n",
    "    name_set = 'set1'\n",
    "else:\n",
    "    name_set = 'set2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fecb7816",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ignore traning warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d96ab70",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST K-foldCV grid\n",
    "# performance, ds_val = k_foldCV(X2_train_val_flatten,  y2_train_val_flatten, grid_format=True,\n",
    "#                         y_range = y2_train_val_range_flatten, methods= algorithms,\n",
    "#                       gauge_id_list = id_train_val, return_classification=True)\n",
    "# performance.groupby(by='model')[stat_cols].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9fa92c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST K-foldCV single pixel \n",
    "# _df, df_class_noPCA = k_foldCV( df_train_val, id_col='tag', target_col = target_col[0],\n",
    "#                                    range_col = 'range_target',feature_cols = feature_columns, \n",
    "#                                    methods = algorithms, do_PCA = True, return_classification=True)\n",
    "\n",
    "# _df.groupby(by='model')[stat_cols].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c034d5bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4c618a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb89a62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b325bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [01:20<00:00, 16.09s/it]\n",
      "  0%|                                                                                                                                                     | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:43<00:00,  8.73s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:46<00:00,  9.20s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:50<00:00, 10.11s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:49<00:00,  9.80s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:43<00:00,  8.80s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:51<00:00, 10.40s/it]\n",
      "  0%|                                                                                                                                                     | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:49<00:00,  9.93s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:49<00:00,  9.83s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:50<00:00, 10.16s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:47<00:00,  9.56s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:47<00:00,  9.49s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:49<00:00,  9.88s/it]\n",
      "  0%|                                                                                                                                                     | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:46<00:00,  9.30s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:46<00:00,  9.32s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:48<00:00,  9.79s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:51<00:00, 10.29s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:51<00:00, 10.20s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:53<00:00, 10.79s/it]\n",
      "  0%|                                                                                                                                                     | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:52<00:00, 10.50s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:44<00:00,  8.94s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:47<00:00,  9.48s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:47<00:00,  9.58s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:46<00:00,  9.33s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:47<00:00,  9.60s/it]\n",
      "  0%|                                                                                                                                                     | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:43<00:00,  8.79s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:44<00:00,  8.83s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:44<00:00,  8.96s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:46<00:00,  9.23s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:46<00:00,  9.28s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:47<00:00,  9.45s/it]\n",
      "  0%|                                                                                                                                                     | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:44<00:00,  8.95s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:47<00:00,  9.46s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:47<00:00,  9.59s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:45<00:00,  9.15s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:48<00:00,  9.79s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:50<00:00, 10.11s/it]\n",
      "  0%|                                                                                                                                                     | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:46<00:00,  9.20s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:47<00:00,  9.48s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:48<00:00,  9.65s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:48<00:00,  9.67s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:47<00:00,  9.54s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:51<00:00, 10.21s/it]\n",
      "  0%|                                                                                                                                                     | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:44<00:00,  8.95s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:47<00:00,  9.42s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:47<00:00,  9.51s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:50<00:00, 10.12s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:47<00:00,  9.57s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:49<00:00,  9.97s/it]\n",
      "  0%|                                                                                                                                                     | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:43<00:00,  8.73s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:44<00:00,  8.85s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:45<00:00,  9.06s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:45<00:00,  9.19s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:45<00:00,  9.11s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:48<00:00,  9.80s/it]\n",
      "  0%|                                                                                                                                                     | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:45<00:00,  9.16s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:48<00:00,  9.64s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:48<00:00,  9.60s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:49<00:00,  9.96s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:49<00:00,  9.88s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:52<00:00, 10.56s/it]\n"
     ]
    }
   ],
   "source": [
    "## PCA sensitivity analysis - for raster pixel \n",
    "\n",
    "df_grid_no_pca = pd.DataFrame()\n",
    "df_grid_pca60 = pd.DataFrame()\n",
    "df_grid_pca70 = pd.DataFrame()\n",
    "df_grid_pca80 = pd.DataFrame()\n",
    "df_grid_pca90 = pd.DataFrame()\n",
    "df_grid_pca95 = pd.DataFrame()\n",
    "df_grid_pca99 = pd.DataFrame() \n",
    "\n",
    "\n",
    "## training no_pca takes a long time, only do K-fold CV once \n",
    "_df, ds_class_noPCA = k_foldCV( X2_train_val_flatten,  y2_train_val_flatten, grid_format=True,\n",
    "                                y_range = y2_train_val_range_flatten, methods= algorithms,  \n",
    "                                gauge_id_list = id_train_val, return_classification=True,\n",
    "                                do_PCA = False)\n",
    "df_grid_no_pca = df_grid_no_pca.append(_df)\n",
    "\n",
    "\n",
    "for kn in range(n_folds):\n",
    "    print('Iteration {}'.format(int(kn+1)))\n",
    "    # no pca \n",
    "#     _df, ds_class_noPCA = k_foldCV( X2_train_val_flatten,  y2_train_val_flatten, grid_format=True,\n",
    "#                                     y_range = y2_train_val_range_flatten, methods= algorithms,  \n",
    "#                                     gauge_id_list = id_train_val, return_classification=True,\n",
    "#                                     do_PCA = False)\n",
    "#     df_grid_no_pca = df_grid_no_pca.append(_df)\n",
    "    \n",
    "    ## PCA 60% \n",
    "    _df, ds_class_pca60 = k_foldCV( X2_train_val_flatten,  y2_train_val_flatten, grid_format=True,\n",
    "                                    y_range = y2_train_val_range_flatten, methods= algorithms,  \n",
    "                                    gauge_id_list = id_train_val, return_classification=True,\n",
    "                                    do_PCA = True, n_pca = 0.6)\n",
    "    df_grid_pca60 = df_grid_pca60.append(_df)    \n",
    "\n",
    "    ## PCA 70% \n",
    "    _df, ds_class_pca70 = k_foldCV( X2_train_val_flatten,  y2_train_val_flatten, grid_format=True,\n",
    "                                    y_range = y2_train_val_range_flatten, methods= algorithms,  \n",
    "                                    gauge_id_list = id_train_val, return_classification=True,\n",
    "                                    do_PCA = True, n_pca = 0.7)\n",
    "    df_grid_pca70 = df_grid_pca70.append(_df) \n",
    "\n",
    "    ## PCA 80% \n",
    "    _df, ds_class_pca80 = k_foldCV( X2_train_val_flatten,  y2_train_val_flatten, grid_format=True,\n",
    "                                    y_range = y2_train_val_range_flatten, methods= algorithms,  \n",
    "                                    gauge_id_list = id_train_val, return_classification=True,\n",
    "                                    do_PCA = True, n_pca = 0.8)\n",
    "    df_grid_pca80 = df_grid_pca80.append(_df) \n",
    "\n",
    "    ## PCA 90% \n",
    "    _df, ds_class_pca90 = k_foldCV( X2_train_val_flatten,  y2_train_val_flatten, grid_format=True,\n",
    "                                    y_range = y2_train_val_range_flatten, methods= algorithms,  \n",
    "                                    gauge_id_list = id_train_val, return_classification=True,\n",
    "                                    do_PCA = True, n_pca = 0.9)\n",
    "    df_grid_pca90 = df_grid_pca90.append(_df) \n",
    "\n",
    "    ## PCA 95% \n",
    "    _df, ds_class_pca95 = k_foldCV( X2_train_val_flatten,  y2_train_val_flatten, grid_format=True,\n",
    "                                    y_range = y2_train_val_range_flatten, methods= algorithms,  \n",
    "                                    gauge_id_list = id_train_val, return_classification=True,\n",
    "                                    do_PCA = True, n_pca = 0.95)\n",
    "    df_grid_pca95 = df_grid_pca95.append(_df) \n",
    "\n",
    "    ## PCA 99% \n",
    "    _df, ds_class_pca99 = k_foldCV( X2_train_val_flatten,  y2_train_val_flatten, grid_format=True,\n",
    "                                    y_range = y2_train_val_range_flatten, methods= algorithms,  \n",
    "                                    gauge_id_list = id_train_val, return_classification=True,\n",
    "                                    do_PCA = True, n_pca = 0.99)\n",
    "    df_grid_pca99 = df_grid_pca99.append(_df) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a501f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2003b633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7684bd93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb341769",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ab3badb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mvand\\Documents\\Master EE\\Year 4\\Thesis\\data\\test_output\\PCA_analysis_1\\alg_pf_grid_noPCA_set2_0_diff.csv\n",
      "C:\\Users\\mvand\\Documents\\Master EE\\Year 4\\Thesis\\data\\test_output\\PCA_analysis_1\\alg_cl_grid_noPCA_set2_0_diff.nc\n",
      "C:\\Users\\mvand\\Documents\\Master EE\\Year 4\\Thesis\\data\\test_output\\PCA_analysis_1\\alg_pf_grid_60PCA_set2_0_diff.csv\n",
      "C:\\Users\\mvand\\Documents\\Master EE\\Year 4\\Thesis\\data\\test_output\\PCA_analysis_1\\alg_cl_grid_60PCA_set2_0_diff.nc\n",
      "C:\\Users\\mvand\\Documents\\Master EE\\Year 4\\Thesis\\data\\test_output\\PCA_analysis_1\\alg_pf_grid_70PCA_set2_0_diff.csv\n",
      "C:\\Users\\mvand\\Documents\\Master EE\\Year 4\\Thesis\\data\\test_output\\PCA_analysis_1\\alg_cl_grid_70PCA_set2_0_diff.nc\n",
      "C:\\Users\\mvand\\Documents\\Master EE\\Year 4\\Thesis\\data\\test_output\\PCA_analysis_1\\alg_pf_grid_80PCA_set2_0_diff.csv\n",
      "C:\\Users\\mvand\\Documents\\Master EE\\Year 4\\Thesis\\data\\test_output\\PCA_analysis_1\\alg_cl_grid_80PCA_set2_0_diff.nc\n",
      "C:\\Users\\mvand\\Documents\\Master EE\\Year 4\\Thesis\\data\\test_output\\PCA_analysis_1\\alg_pf_grid_90PCA_set2_0_diff.csv\n",
      "C:\\Users\\mvand\\Documents\\Master EE\\Year 4\\Thesis\\data\\test_output\\PCA_analysis_1\\alg_cl_grid_90PCA_set2_0_diff.nc\n",
      "C:\\Users\\mvand\\Documents\\Master EE\\Year 4\\Thesis\\data\\test_output\\PCA_analysis_1\\alg_pf_grid_95PCA_set2_0_diff.csv\n",
      "C:\\Users\\mvand\\Documents\\Master EE\\Year 4\\Thesis\\data\\test_output\\PCA_analysis_1\\alg_cl_grid_95PCA_set2_0_diff.nc\n",
      "C:\\Users\\mvand\\Documents\\Master EE\\Year 4\\Thesis\\data\\test_output\\PCA_analysis_1\\alg_pf_grid_99PCA_set2_0_diff.csv\n",
      "C:\\Users\\mvand\\Documents\\Master EE\\Year 4\\Thesis\\data\\test_output\\PCA_analysis_1\\alg_cl_grid_99PCA_set2_0_diff.nc\n"
     ]
    }
   ],
   "source": [
    "list_df_pf = [\n",
    "    df_grid_no_pca, \n",
    "    df_grid_pca60,\n",
    "    df_grid_pca70, \n",
    "    df_grid_pca80,\n",
    "    df_grid_pca90, \n",
    "    df_grid_pca95, \n",
    "    df_grid_pca99]\n",
    "\n",
    "list_ds_cl = [\n",
    "    ds_class_noPCA, \n",
    "    ds_class_pca60, \n",
    "    ds_class_pca70,\n",
    "    ds_class_pca80,\n",
    "    ds_class_pca90, \n",
    "    ds_class_pca95, \n",
    "    ds_class_pca99]\n",
    "\n",
    "\n",
    "\n",
    "fn_df_pf = [\n",
    "    f'alg_pf_grid_noPCA_{name_set}_{set_buffer_size}_{similarity_method}.csv', \n",
    "    f'alg_pf_grid_60PCA_{name_set}_{set_buffer_size}_{similarity_method}.csv',\n",
    "    f'alg_pf_grid_70PCA_{name_set}_{set_buffer_size}_{similarity_method}.csv',\n",
    "    f'alg_pf_grid_80PCA_{name_set}_{set_buffer_size}_{similarity_method}.csv',\n",
    "    f'alg_pf_grid_90PCA_{name_set}_{set_buffer_size}_{similarity_method}.csv',\n",
    "    f'alg_pf_grid_95PCA_{name_set}_{set_buffer_size}_{similarity_method}.csv',\n",
    "    f'alg_pf_grid_99PCA_{name_set}_{set_buffer_size}_{similarity_method}.csv']\n",
    "\n",
    "fn_ds_cl = [\n",
    "    f'alg_cl_grid_noPCA_{name_set}_{set_buffer_size}_{similarity_method}.nc', \n",
    "    f'alg_cl_grid_60PCA_{name_set}_{set_buffer_size}_{similarity_method}.nc',\n",
    "    f'alg_cl_grid_70PCA_{name_set}_{set_buffer_size}_{similarity_method}.nc',\n",
    "    f'alg_cl_grid_80PCA_{name_set}_{set_buffer_size}_{similarity_method}.nc',\n",
    "    f'alg_cl_grid_90PCA_{name_set}_{set_buffer_size}_{similarity_method}.nc',\n",
    "    f'alg_cl_grid_95PCA_{name_set}_{set_buffer_size}_{similarity_method}.nc',\n",
    "    f'alg_cl_grid_99PCA_{name_set}_{set_buffer_size}_{similarity_method}.nc']\n",
    "\n",
    "for i, out_df in enumerate(list_df_pf):\n",
    "    \n",
    "    fn_df = test_dir / fn_df_pf[i]\n",
    "#     out_df.to_csv(fn_df)\n",
    "    print(fn_df)\n",
    "    \n",
    "    fn_ds = test_dir / fn_ds_cl[i]\n",
    "#     list_ds_cl[i].to_netcdf(fn_ds)\n",
    "    print(fn_ds)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7d46e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## reset warnings \n",
    "warnings.filterwarnings('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb77c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mvand\\anaconda3\\envs\\sandbox\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "\r",
      "  0%|                                                                                                                                                     | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:19<00:00,  3.94s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:16<00:00,  3.27s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:16<00:00,  3.28s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:15<00:00,  3.13s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:16<00:00,  3.33s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:17<00:00,  3.52s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:18<00:00,  3.71s/it]\n",
      "  0%|                                                                                                                                                     | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:19<00:00,  3.82s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:15<00:00,  3.11s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:16<00:00,  3.20s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:16<00:00,  3.34s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:17<00:00,  3.45s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:17<00:00,  3.58s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:18<00:00,  3.68s/it]\n",
      "  0%|                                                                                                                                                     | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:20<00:00,  4.06s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:15<00:00,  3.16s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:15<00:00,  3.14s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:16<00:00,  3.29s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:16<00:00,  3.34s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:17<00:00,  3.56s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:18<00:00,  3.77s/it]\n",
      "  0%|                                                                                                                                                     | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:19<00:00,  3.83s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:15<00:00,  3.11s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:15<00:00,  3.09s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:16<00:00,  3.35s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:16<00:00,  3.22s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:16<00:00,  3.34s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:18<00:00,  3.67s/it]\n",
      "  0%|                                                                                                                                                     | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:19<00:00,  3.95s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:15<00:00,  3.11s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:16<00:00,  3.22s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:15<00:00,  3.11s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:17<00:00,  3.43s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:17<00:00,  3.52s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:17<00:00,  3.50s/it]\n",
      "  0%|                                                                                                                                                     | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:18<00:00,  3.65s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:14<00:00,  2.88s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:14<00:00,  2.89s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:15<00:00,  3.04s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:15<00:00,  3.06s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:15<00:00,  3.16s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:16<00:00,  3.36s/it]\n",
      "  0%|                                                                                                                                                     | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:17<00:00,  3.53s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:14<00:00,  2.80s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:14<00:00,  2.82s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:14<00:00,  2.97s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:15<00:00,  3.08s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:16<00:00,  3.27s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:18<00:00,  3.62s/it]\n",
      "  0%|                                                                                                                                                     | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:19<00:00,  3.85s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:15<00:00,  3.06s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:15<00:00,  3.14s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:16<00:00,  3.31s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:16<00:00,  3.23s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:16<00:00,  3.25s/it]\n",
      " 60%|████████████████████████████████████████████████████████████████████████████████████▌                                                        | 3/5 [00:10<00:07,  3.57s/it]"
     ]
    }
   ],
   "source": [
    "## PCA sensitivity analysis - for single pixel \n",
    "\n",
    "df_single_no_pca = pd.DataFrame()\n",
    "df_single_pca60 = pd.DataFrame()\n",
    "df_single_pca70 = pd.DataFrame()\n",
    "df_single_pca80 = pd.DataFrame()\n",
    "df_single_pca90 = pd.DataFrame()\n",
    "df_single_pca95 = pd.DataFrame()\n",
    "df_single_pca99 = pd.DataFrame() \n",
    "\n",
    "for kn in range(n_folds):\n",
    "    print('Iteration {}'.format(int(kn+1)))\n",
    "    ## no pca \n",
    "    _df, df_class_noPCA = k_foldCV( df_train_val, id_col='tag', target_col = target_col[0],\n",
    "                                   range_col = 'range_target',feature_cols = feature_columns, \n",
    "                                   methods = algorithms, do_PCA = False, return_classification=True)\n",
    "    df_single_no_pca = df_single_no_pca.append(_df)\n",
    "\n",
    "    ## PCA 60% \n",
    "    _df, df_class_PCA60 = k_foldCV( df_train_val, id_col='tag', target_col = target_col[0], \n",
    "                                   range_col = 'range_target', feature_cols = feature_columns, \n",
    "                                   methods = algorithms, do_PCA = True, n_pca = 0.6, return_classification=True )\n",
    "    \n",
    "    df_single_pca60 = df_single_pca60.append(_df)    \n",
    "\n",
    "    ## PCA 70% \n",
    "    _df, df_class_PCA70 = k_foldCV( df_train_val, id_col='tag', target_col = target_col[0], \n",
    "                                   range_col = 'range_target', feature_cols = feature_columns, \n",
    "                                   methods = algorithms, do_PCA = True, n_pca = 0.7, return_classification=True)\n",
    "    df_single_pca70 = df_single_pca70.append(_df) \n",
    "\n",
    "    ## PCA 80% \n",
    "    _df, df_class_PCA80 = k_foldCV( df_train_val, id_col='tag', target_col = target_col[0],\n",
    "                                   range_col = 'range_target', feature_cols = feature_columns,\n",
    "                                   methods = algorithms, do_PCA = True, n_pca = 0.8, return_classification=True )\n",
    "    df_single_pca80 = df_single_pca80.append(_df) \n",
    "\n",
    "    ## PCA 90% \n",
    "    _df, df_class_PCA90 = k_foldCV( df_train_val, id_col='tag', target_col = target_col[0], \n",
    "                                   range_col = 'range_target', feature_cols = feature_columns, \n",
    "                                   methods = algorithms, do_PCA = True, n_pca = 0.9, return_classification=True )\n",
    "    df_single_pca90 = df_single_pca90.append(_df) \n",
    "\n",
    "    ## PCA 95% \n",
    "    _df, df_class_PCA95 = k_foldCV( df_train_val, id_col='tag', target_col = target_col[0], \n",
    "                                   range_col = 'range_target', feature_cols = feature_columns, \n",
    "                                   methods = algorithms, do_PCA = True, n_pca = 0.95, return_classification=True )\n",
    "    df_single_pca95 = df_single_pca95.append(_df) \n",
    "\n",
    "    ## PCA 99% \n",
    "    _df, df_class_PCA99 = k_foldCV( df_train_val, id_col='tag', target_col = target_col[0], \n",
    "                                   range_col = 'range_target', feature_cols = feature_columns,\n",
    "                                   methods = algorithms, do_PCA = True, n_pca = 0.99, return_classification=True )\n",
    "    df_single_pca99 = df_single_pca99.append(_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8ae535",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SAVE RESULTS \n",
    "list_df_pf = [df_single_no_pca, df_single_pca60, df_single_pca70, df_single_pca80, \n",
    "              df_single_pca90, df_single_pca95, df_single_pca99 ]\n",
    "\n",
    "list_df_cl = [df_class_noPCA, df_class_PCA60, df_class_PCA70, df_class_PCA80, \n",
    "              df_class_PCA90, df_class_PCA95, df_class_PCA99]\n",
    "\n",
    "fn_df_pf = [f'alg_pf_single_noPCA_{name_set}_{set_buffer_size}_{similarity_method}.csv', \n",
    "            f'alg_pf_single_60PCA_{name_set}_{set_buffer_size}_{similarity_method}.csv',\n",
    "            f'alg_pf_single_70PCA_{name_set}_{set_buffer_size}_{similarity_method}.csv',\n",
    "            f'alg_pf_single_80PCA_{name_set}_{set_buffer_size}_{similarity_method}.csv',\n",
    "            f'alg_pf_single_90PCA_{name_set}_{set_buffer_size}_{similarity_method}.csv',\n",
    "            f'alg_pf_single_95PCA_{name_set}_{set_buffer_size}_{similarity_method}.csv',\n",
    "            f'alg_pf_single_99PCA_{name_set}_{set_buffer_size}_{similarity_method}.csv']\n",
    "\n",
    "fn_df_cl = [f'alg_cl_single_noPCA_{name_set}_{set_buffer_size}_{similarity_method}.csv', \n",
    "            f'alg_cl_single_60PCA_{name_set}_{set_buffer_size}_{similarity_method}.csv',\n",
    "            f'alg_cl_single_70PCA_{name_set}_{set_buffer_size}_{similarity_method}.csv',\n",
    "            f'alg_cl_single_80PCA_{name_set}_{set_buffer_size}_{similarity_method}.csv',\n",
    "            f'alg_cl_single_90PCA_{name_set}_{set_buffer_size}_{similarity_method}.csv',\n",
    "            f'alg_cl_single_95PCA_{name_set}_{set_buffer_size}_{similarity_method}.csv',\n",
    "            f'alg_cl_single_99PCA_{name_set}_{set_buffer_size}_{similarity_method}.csv']\n",
    "\n",
    "for i, out_df in enumerate(list_df_pf):\n",
    "    \n",
    "    fn_pf = test_dir / fn_df_pf[i]\n",
    "#     out_df.to_csv(fn_pf)\n",
    "    print(fn_pf)\n",
    "    \n",
    "    fn_cl = test_dir / fn_df_cl[i]\n",
    "#     list_df_cl[i].to_csv(fn_cl)\n",
    "    print(fn_cl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6ebcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_df_pf = [df_grid_no_pca, df_grid_pca60, df_grid_pca70, df_grid_pca80,\n",
    "#               df_grid_pca90, df_grid_pca95, df_grid_pca99]\n",
    "\n",
    "# list_ds_cl = [ds_class_noPCA, ds_class_pca60, ds_class_pca70, ds_class_pca80,\n",
    "#               ds_class_pca90, ds_class_pca95, ds_class_pca99]\n",
    "\n",
    "\n",
    "# fn_df_pf = ['alg_pf_set1_grid_npPCA.csv', \n",
    "#             'alg_pf_set1_grid_60PCA.csv',\n",
    "#             'alg_pf_set1_grid_70PCA.csv',\n",
    "#             'alg_pf_set1_grid_80PCA.csv',\n",
    "#             'alg_pf_set1_grid_90PCA.csv',\n",
    "#             'alg_pf_set1_grid_95PCA.csv',\n",
    "#             'alg_pf_set1_grid_99PCA.csv']\n",
    "\n",
    "# fn_ds_cl = ['alg_cl_set1_grid_npPCA.nc', \n",
    "#             'alg_cl_set1_grid_60PCA.nc',\n",
    "#             'alg_cl_set1_grid_70PCA.nc',\n",
    "#             'alg_cl_set1_grid_80PCA.nc',\n",
    "#             'alg_cl_set1_grid_90PCA.nc',\n",
    "#             'alg_cl_set1_grid_95PCA.nc',\n",
    "#             'alg_cl_set1_grid_99PCA.nc']\n",
    "\n",
    "# for i, out_df in enumerate(list_df_pf):\n",
    "    \n",
    "#     fn_df = test_dir / fn_df_pf[i]\n",
    "# #     out_df.to_csv(fn_df)\n",
    "    \n",
    "#     fn_ds = test_dir / fn_ds_cl[i]\n",
    "# #     list_ds_cl[i].to_netcdf(fn_ds)\n",
    "#     print(fn_df)\n",
    "#     print(fn_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc5bc15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dab097",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d206fb93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf7d2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## prepare training & validation set - single grid \n",
    "\n",
    "# subsample \n",
    "df_train_1 = df_train_val[ df_train_val[target_col[0]] == 1]\n",
    "_df_train_0 = df_train_val[ df_train_val[target_col[0]] == 0]\n",
    "ix_df_train_0 = []\n",
    "\n",
    "## sample for each 0 sample \n",
    "for i, idx in enumerate(_df_train_0['tag'].unique()):\n",
    "    sub_df_train_0 = _df_train_0[_df_train_0['tag']==idx]\n",
    "    ## get sample\n",
    "    subsample = sub_df_train_0.sample(n=1).index.values[0]\n",
    "    ix_df_train_0.append(subsample)\n",
    "df_train_0 = df_train_val.loc[ix_df_train_0]\n",
    "df_train = df_train_1.append(df_train_0)\n",
    "\n",
    "# add a number of samples not in buffer as well \n",
    "df_not_in_buffer = df_train_val[df_train_val['in_buffer']!=1]\n",
    "n_sample = df_not_in_buffer['tag'].nunique()\n",
    "\n",
    "df_train = df_train.append( df_not_in_buffer.sample(n=n_sample) ).copy()\n",
    "\n",
    "print('Division of target values before subsampling: ')\n",
    "print( df_train_val['target'].value_counts())\n",
    "print('\\nand after subsampling: ')\n",
    "print(df_train['target'].value_counts())\n",
    "\n",
    "X_train = df_train[feature_columns]\n",
    "y_train = df_train[target_col[0]]\n",
    "y_train_range = df_train['range_target']\n",
    "\n",
    "# normalize \n",
    "min_samples = len(X_train) \n",
    "nm = QuantileTransformer( output_distribution='normal',\n",
    "                        n_quantiles = int(min(1000, min_samples)))\n",
    "nm.fit(X_train)\n",
    "X_train = nm.transform(X_train)\n",
    "\n",
    "# scale \n",
    "sc = MinMaxScaler([0,1])\n",
    "sc.fit(X_train)\n",
    "X_train = sc.transform(X_train)\n",
    "\n",
    "# pca \n",
    "print(f'\\nnumber of features before PCA = {X_train.shape[1]}')\n",
    "pca = PCA(n_components=0.9)\n",
    "pca.fit(X_train)\n",
    "X_train = pca.transform(X_train)\n",
    "print(f'number of features after PCA = {X_train.shape[1]} with 90% variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c5cf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot PC_i vs PC_j\n",
    "pc_i = 0\n",
    "pc_j = 1\n",
    "\n",
    "ix_0 = np.where(y_train==0)[0]\n",
    "ix_1 = np.where(y_train==1)[0]\n",
    "\n",
    "plt.scatter( X_train[ix_0,pc_i], X_train[ix_0,pc_j], color = 'b', label='0' ) \n",
    "plt.scatter( X_train[ix_1,pc_i], X_train[ix_1,pc_j], color = 'r', label='1'  ) \n",
    "\n",
    "plt.xlabel('PC{}'.format(pc_i+1));\n",
    "plt.ylabel('PC{}'.format(pc_j+1));\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3205dfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## apply same transformations and splits on test set \n",
    "X_test = df_test[feature_columns]\n",
    "y_test = df_test[target_col[0]]\n",
    "y_test_range = df_test['range_target'] \n",
    "\n",
    "## normalize \n",
    "X_test = nm.transform(X_test)\n",
    "\n",
    "## scale \n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "## pca \n",
    "X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7235660",
   "metadata": {},
   "outputs": [],
   "source": [
    "## train single pixel classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a75e25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81e7044",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODEL 1 - target in buffer, yes or no?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae60a4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96617eb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2781abf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ef6c9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0035d231",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODEL 2 - if target in buffer, where? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb78ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733ed039",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6233d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f04d04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb384cc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2141ec7d",
   "metadata": {},
   "source": [
    "After finding best settings, use train_val set to train an algorithm, evaluate the final performance with the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cbfc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff71ee1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8c2af6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
