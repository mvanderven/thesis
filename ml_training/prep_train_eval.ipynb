{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3cd6b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from pathlib import Path\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import xarray as xr\n",
    "import hydroeval as he \n",
    "import seaborn as sns \n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1760f988",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO DO  \n",
    "## - with subsampling, slightly increase nr 0s? for not occuring gauges \n",
    "## - if grid_format: return results of classification (p0, p1, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fc73e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SETTINGS \n",
    "\n",
    "## which dataset to use \n",
    "## 1 --> most samples, less features \n",
    "## 2 --> less samples (about 20 less?), complete set of features \n",
    "use_set_1 = True\n",
    "\n",
    "## omit coordinates as feature \n",
    "omit_coords = True\n",
    "\n",
    "## set size of buffer search \n",
    "set_buffer_size = 6\n",
    "\n",
    "## what similarity to use: diff, abs, ratio-1, ratio-2, double \n",
    "similarity_method = 'diff'\n",
    "\n",
    "## grid training or cell training \n",
    "grid_format = True\n",
    "\n",
    "## apply PCA \n",
    "do_pca = True "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bccdba",
   "metadata": {},
   "source": [
    "Load dataset with features. On each row, a different simulation of observation sample can be found, in each column feature values or location metadata is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05fcc648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nm-all</th>\n",
       "      <th>Ns-all</th>\n",
       "      <th>N-gof-all</th>\n",
       "      <th>Lm-all</th>\n",
       "      <th>Ls-all</th>\n",
       "      <th>L-gof-all</th>\n",
       "      <th>Gu-all</th>\n",
       "      <th>Ga-all</th>\n",
       "      <th>Gev-gof-all</th>\n",
       "      <th>Gk-all</th>\n",
       "      <th>...</th>\n",
       "      <th>flv-seasonal_1</th>\n",
       "      <th>hf-f-all</th>\n",
       "      <th>hf-f-seasonal_1</th>\n",
       "      <th>hf-t-all</th>\n",
       "      <th>lf-f-all</th>\n",
       "      <th>lf-f-seasonal_1</th>\n",
       "      <th>lf-t-seasonal_1</th>\n",
       "      <th>pks-all</th>\n",
       "      <th>tag</th>\n",
       "      <th>dem_flag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6119010_11</th>\n",
       "      <td>102.035362</td>\n",
       "      <td>106.090645</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.221500</td>\n",
       "      <td>0.863208</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.306767</td>\n",
       "      <td>0.012089</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.925012</td>\n",
       "      <td>...</td>\n",
       "      <td>164.721008</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.954545</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>4.481481</td>\n",
       "      <td>666.803894</td>\n",
       "      <td>6119010</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6119010_12</th>\n",
       "      <td>100.917046</td>\n",
       "      <td>105.168777</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.209119</td>\n",
       "      <td>0.864435</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.603184</td>\n",
       "      <td>0.012195</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.920779</td>\n",
       "      <td>...</td>\n",
       "      <td>163.700439</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.090909</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>4.518519</td>\n",
       "      <td>654.762268</td>\n",
       "      <td>6119010</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6119010_13</th>\n",
       "      <td>0.544169</td>\n",
       "      <td>0.542255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.002645</td>\n",
       "      <td>0.867551</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.300217</td>\n",
       "      <td>2.365214</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.007072</td>\n",
       "      <td>...</td>\n",
       "      <td>266.693390</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.727273</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>3.131104</td>\n",
       "      <td>6119010</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6119010_14</th>\n",
       "      <td>162.321777</td>\n",
       "      <td>138.910507</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.760090</td>\n",
       "      <td>0.835850</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.828016</td>\n",
       "      <td>0.009233</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.365474</td>\n",
       "      <td>...</td>\n",
       "      <td>317.679565</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>7.227273</td>\n",
       "      <td>723.082581</td>\n",
       "      <td>6119010</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6119010_15</th>\n",
       "      <td>17.163115</td>\n",
       "      <td>16.641987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.442999</td>\n",
       "      <td>0.888787</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.676133</td>\n",
       "      <td>0.077067</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.063609</td>\n",
       "      <td>...</td>\n",
       "      <td>248.398575</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>6.083333</td>\n",
       "      <td>98.092957</td>\n",
       "      <td>6119010</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6983350_96</th>\n",
       "      <td>522.018127</td>\n",
       "      <td>325.349060</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.080506</td>\n",
       "      <td>0.592225</td>\n",
       "      <td>0.0</td>\n",
       "      <td>375.648448</td>\n",
       "      <td>0.003942</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.574376</td>\n",
       "      <td>...</td>\n",
       "      <td>60.206692</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1890.435181</td>\n",
       "      <td>6983350</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6983350_97</th>\n",
       "      <td>1.769354</td>\n",
       "      <td>1.644365</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.271318</td>\n",
       "      <td>0.743112</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.029579</td>\n",
       "      <td>0.779967</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.157798</td>\n",
       "      <td>...</td>\n",
       "      <td>73.878761</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>6.181818</td>\n",
       "      <td>10.525513</td>\n",
       "      <td>6983350</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6983350_98</th>\n",
       "      <td>6.406629</td>\n",
       "      <td>5.884712</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.530997</td>\n",
       "      <td>0.797030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.759185</td>\n",
       "      <td>0.217946</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.185247</td>\n",
       "      <td>...</td>\n",
       "      <td>92.737022</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>33.151855</td>\n",
       "      <td>6983350</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6983350_99</th>\n",
       "      <td>0.280780</td>\n",
       "      <td>0.267437</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.570457</td>\n",
       "      <td>0.738658</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.160464</td>\n",
       "      <td>4.795705</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.102273</td>\n",
       "      <td>...</td>\n",
       "      <td>92.291611</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.444444</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>4.818182</td>\n",
       "      <td>1.593628</td>\n",
       "      <td>6983350</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6983350_gauge</th>\n",
       "      <td>413.776970</td>\n",
       "      <td>228.897089</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.840650</td>\n",
       "      <td>0.653553</td>\n",
       "      <td>0.0</td>\n",
       "      <td>310.799590</td>\n",
       "      <td>0.005603</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.267775</td>\n",
       "      <td>...</td>\n",
       "      <td>30.588420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>992.500000</td>\n",
       "      <td>6983350</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47234 rows × 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Nm-all      Ns-all  N-gof-all    Lm-all    Ls-all  \\\n",
       "ID                                                                     \n",
       "6119010_11     102.035362  106.090645        0.0  4.221500  0.863208   \n",
       "6119010_12     100.917046  105.168777        0.0  4.209119  0.864435   \n",
       "6119010_13       0.544169    0.542255        0.0 -1.002645  0.867551   \n",
       "6119010_14     162.321777  138.910507        0.0  4.760090  0.835850   \n",
       "6119010_15      17.163115   16.641987        0.0  2.442999  0.888787   \n",
       "...                   ...         ...        ...       ...       ...   \n",
       "6983350_96     522.018127  325.349060        0.0  6.080506  0.592225   \n",
       "6983350_97       1.769354    1.644365        0.0  0.271318  0.743112   \n",
       "6983350_98       6.406629    5.884712        0.0  1.530997  0.797030   \n",
       "6983350_99       0.280780    0.267437        0.0 -1.570457  0.738658   \n",
       "6983350_gauge  413.776970  228.897089        0.0  5.840650  0.653553   \n",
       "\n",
       "               L-gof-all      Gu-all    Ga-all  Gev-gof-all    Gk-all  ...  \\\n",
       "ID                                                                     ...   \n",
       "6119010_11           0.0   54.306767  0.012089          0.0  0.925012  ...   \n",
       "6119010_12           0.0   53.603184  0.012195          0.0  0.920779  ...   \n",
       "6119010_13           0.0    0.300217  2.365214          0.0  1.007072  ...   \n",
       "6119010_14           0.0   99.828016  0.009233          0.0  1.365474  ...   \n",
       "6119010_15           0.0    9.676133  0.077067          0.0  1.063609  ...   \n",
       "...                  ...         ...       ...          ...       ...  ...   \n",
       "6983350_96           0.0  375.648448  0.003942          0.0  2.574376  ...   \n",
       "6983350_97           0.0    1.029579  0.779967          0.0  1.157798  ...   \n",
       "6983350_98           0.0    3.759185  0.217946          0.0  1.185247  ...   \n",
       "6983350_99           0.0    0.160464  4.795705          0.0  1.102273  ...   \n",
       "6983350_gauge        0.0  310.799590  0.005603          0.0  3.267775  ...   \n",
       "\n",
       "               flv-seasonal_1  hf-f-all  hf-f-seasonal_1  hf-t-all  lf-f-all  \\\n",
       "ID                                                                             \n",
       "6119010_11         164.721008  0.590909              0.0  2.954545  0.363636   \n",
       "6119010_12         163.700439  0.590909              0.0  3.090909  0.363636   \n",
       "6119010_13         266.693390  0.409091              0.0  1.727273  0.409091   \n",
       "6119010_14         317.679565  0.045455              0.0  0.136364  1.000000   \n",
       "6119010_15         248.398575  0.136364              0.0  0.363636  0.454545   \n",
       "...                       ...       ...              ...       ...       ...   \n",
       "6983350_96          60.206692  0.000000              0.0  0.000000  0.111111   \n",
       "6983350_97          73.878761  0.333333              0.0  1.000000  0.111111   \n",
       "6983350_98          92.737022  0.444444              0.0  1.000000  0.444444   \n",
       "6983350_99          92.291611  0.555556              0.0  1.444444  0.222222   \n",
       "6983350_gauge       30.588420  0.000000              0.0  0.000000  0.222222   \n",
       "\n",
       "               lf-f-seasonal_1  lf-t-seasonal_1      pks-all      tag  \\\n",
       "ID                                                                      \n",
       "6119010_11            0.636364         4.481481   666.803894  6119010   \n",
       "6119010_12            0.636364         4.518519   654.762268  6119010   \n",
       "6119010_13            0.636364         6.000000     3.131104  6119010   \n",
       "6119010_14            0.545455         7.227273   723.082581  6119010   \n",
       "6119010_15            0.500000         6.083333    98.092957  6119010   \n",
       "...                        ...              ...          ...      ...   \n",
       "6983350_96            0.000000         0.000000  1890.435181  6983350   \n",
       "6983350_97            0.555556         6.181818    10.525513  6983350   \n",
       "6983350_98            0.444444         4.800000    33.151855  6983350   \n",
       "6983350_99            0.555556         4.818182     1.593628  6983350   \n",
       "6983350_gauge         0.000000         0.000000   992.500000  6983350   \n",
       "\n",
       "               dem_flag  \n",
       "ID                       \n",
       "6119010_11            0  \n",
       "6119010_12            0  \n",
       "6119010_13            0  \n",
       "6119010_14            0  \n",
       "6119010_15            0  \n",
       "...                 ...  \n",
       "6983350_96            0  \n",
       "6983350_97            0  \n",
       "6983350_98            0  \n",
       "6983350_99            0  \n",
       "6983350_gauge         0  \n",
       "\n",
       "[47234 rows x 84 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signature_dir = Path(r\"C:\\Users\\mvand\\Documents\\Master EE\\Year 4\\Thesis\\data\\training_data_S1\\signatures_nc_V1_output\") \n",
    " \n",
    "if use_set_1:\n",
    "    fn_signatures = signature_dir / \"S1_merged_signatures_v2-cleanup-1.csv\"\n",
    "else:\n",
    "    fn_signatures = signature_dir / \"S1_merged_signatures_v2-cleanup-2.csv\" \n",
    "\n",
    "df_signatures = pd.read_csv(fn_signatures, index_col = 0)\n",
    "\n",
    "df_signatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038fca7d",
   "metadata": {},
   "source": [
    "Slightly expand the searching area by labeling the up- and downstream neigbhours of the target pixel as 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ca33ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## expand searching area by max. 2 pixels \n",
    "\n",
    "def expand_labels(df, id_col, target_col, critical_variable, \n",
    "                  p_diff = 5., n_max = 3, pixel_size = 5000., option = 1,\n",
    "                  x_coord = 'x', y_coord = 'y'):\n",
    "    \n",
    "    out_col = 'range_{}'.format(target_col)\n",
    "    df[out_col] = 0.\n",
    "    \n",
    "    for bix in df[id_col].unique():\n",
    "        \n",
    "        ## select buffer and target value \n",
    "        _buffer = df[ df[id_col] == bix ] \n",
    "        \n",
    "        ## discard gauge values \n",
    "        buffer_ix = [ix for ix in _buffer.index if not 'gauge' in ix]\n",
    "        _sim = _buffer.loc[buffer_ix] \n",
    "        \n",
    "        ## select target row \n",
    "        target = _sim[ _sim[target_col] == 1.]\n",
    "        \n",
    "        ## if target value found\n",
    "        if len(target) > 0:\n",
    "            \n",
    "            ## search criterium \n",
    "            target_var = target[critical_variable].values[0] \n",
    "            \n",
    "            ## look to adjacent pixels only \n",
    "            target_X_coord = target[x_coord].values[0]\n",
    "            target_Y_coord = target[y_coord].values[0] \n",
    "            x_filter = (_sim[x_coord] >= target_X_coord - (pixel_size+1.) ) & (_sim[x_coord] <= target_X_coord + (pixel_size+1.))\n",
    "            y_filter = (_sim[y_coord] >= target_Y_coord - (pixel_size+1.) ) & (_sim[y_coord] <= target_Y_coord + (pixel_size+1.))\n",
    "            _sim = _sim[ x_filter & y_filter]\n",
    "\n",
    "            ## calculate absolute percentual difference \n",
    "            d_var = (( ((_sim[critical_variable] - target_var)**2)**0.5 / target_var)*100 ).sort_values()\n",
    "            \n",
    "            ## OPTION 1 - select top three (including original target pixel) based on difference \n",
    "            if option == 1:\n",
    "                expand_targets = d_var.head(n_max).index \n",
    "\n",
    "            ## OPTION 2 - also include a percentual limit (1% - 5%) \n",
    "            if option == 2:\n",
    "                expand_targets = d_var[ d_var <= p_diff].index\n",
    "                if len(expand_targets) > n_max:\n",
    "                    expand_targets = expand_targets[:int(n_max-1)]\n",
    "\n",
    "            ## relabel \n",
    "            if target.index[0] in expand_targets:\n",
    "                df.loc[expand_targets, out_col] = 1 \n",
    "            else:\n",
    "                print('check ', bix)            \n",
    "\n",
    "    return df, out_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be3b8c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_signatures, range_col = expand_labels(df_signatures, 'tag', 'target', 'Nm-all', option = 1)#, p_diff = 1.) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d799edab",
   "metadata": {},
   "source": [
    "Set the label categories - identify feature columns and target labels. Used for later processing of similarity and splitting X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9167ee0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## column descriptors \n",
    "columns = df_signatures.columns.values \n",
    "\n",
    "## target value to predict \n",
    "target_col = ['target'] \n",
    "\n",
    "## target for continued search \n",
    "target_buffer = ['in_buffer']\n",
    "\n",
    "## should be omitted from dataset - but could be useful for easy selection \n",
    "## dem_flag only 1 for gauge values\n",
    "non_feature_cols = ['n_buffer', 'tag', 'dem_flag', range_col, 'in_buffer'] \n",
    "\n",
    "## coord cols - could be ommitted from dataset, risk for overfitting\n",
    "## and also takes away focus on selection based on timeseries\n",
    "coord_cols = ['x', 'y', 'lat', 'lon']\n",
    "\n",
    "## features that do not have to be transformed by subtraction \n",
    "## and cross-correlation properties, that are already a similarity property \n",
    "## as they are cross-correlation values of simulations and observations\n",
    "non_similarity_cols = [col for col in columns if 'clag' in col]\n",
    "\n",
    "feature_cols = [col for col in columns if (col not in non_feature_cols) & (not 'target' in col)]\n",
    "\n",
    "## if coord_cols ommitted:\n",
    "if omit_coords:\n",
    "    feature_cols = [col for col in feature_cols if col not in coord_cols] \n",
    "\n",
    "## columns on which to perform similarity calculations:\n",
    "calc_cols = [col for col in feature_cols if col not in non_similarity_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011f1d68",
   "metadata": {},
   "source": [
    "Set the similarity metric used by the algorithm to find a best matching simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ebcecc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate similarity per buffer \n",
    "\n",
    "\n",
    "def calc_similarity(df, buffer_col, calc_cols,\n",
    "                   methods = ['diff']):\n",
    "    \n",
    "    df_out = df.copy() \n",
    "    \n",
    "    buffer_idx = df[buffer_col].unique() \n",
    "    \n",
    "    for ix in buffer_idx:\n",
    "        df_buffer = df[ df[buffer_col] == ix] \n",
    "        \n",
    "        cell_index = [row for row in df_buffer.index if not 'gauge' in row]\n",
    "        gauge_index = [row for row in df_buffer.index if 'gauge' in row] \n",
    "        \n",
    "        if len(gauge_index) > 0:\n",
    "                \n",
    "            if 'diff' in methods:            \n",
    "                for col in calc_cols:\n",
    "                    df_out.loc[cell_index, 'diff_{}'.format(col)] = df_out.loc[cell_index, col] - df_out.loc[gauge_index, col].values\n",
    "\n",
    "            if 'abs' in methods:\n",
    "                for col in calc_cols:\n",
    "                     df_out.loc[cell_index, 'abs_{}'.format(col)] = ((df_out.loc[cell_index, col] - df_out.loc[gauge_index, col].values)**2)**0.5 \n",
    "\n",
    "            if 'ratio-1' in methods:\n",
    "                for col in calc_cols:\n",
    "                    df_out.loc[cell_index, 'rat1_{}'.format(col)] = 1 - (( df_out.loc[cell_index, col] / (df_out.loc[gauge_index, col].values+1e-6)  )**2)**0.5\n",
    "\n",
    "            if 'ratio-2' in methods:\n",
    "                for col in calc_cols:\n",
    "                    df_out.loc[cell_index, 'rat2_{}'.format(col)] =  ( df_out.loc[cell_index, col] - df_out.loc[gauge_index, col].values ) / (df_out.loc[gauge_index, col].values+1e-6)\n",
    "            \n",
    "            if 'double' in methods: \n",
    "                for col in calc_cols:\n",
    "                    df_out.loc[cell_index, 'sim_{}'.format(col)] = df_out.loc[cell_index, col]\n",
    "                    df_out.loc[cell_index, 'obs_{}'.format(col)] = df_out.loc[gauge_index, col].values[0]\n",
    "        \n",
    "        else:\n",
    "            print('No gauge? :', ix)\n",
    "            df_out = df_out.drop(index=cell_index)\n",
    "                    \n",
    "    ## drop calc_cols\n",
    "    df_out = df_out.drop(columns=calc_cols)\n",
    "    \n",
    "    gauge_idx = [row for row in df_out.index if 'gauge' in row] \n",
    "    return df_out.drop(index=gauge_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2a49006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clag-0-all</th>\n",
       "      <th>clag-1-all</th>\n",
       "      <th>clag-0-seasonal_1</th>\n",
       "      <th>clag-1-seasonal_1</th>\n",
       "      <th>n_buffer</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>target</th>\n",
       "      <th>...</th>\n",
       "      <th>diff_fhv-seasonal_1</th>\n",
       "      <th>diff_flv-all</th>\n",
       "      <th>diff_flv-seasonal_1</th>\n",
       "      <th>diff_hf-f-all</th>\n",
       "      <th>diff_hf-f-seasonal_1</th>\n",
       "      <th>diff_hf-t-all</th>\n",
       "      <th>diff_lf-f-all</th>\n",
       "      <th>diff_lf-f-seasonal_1</th>\n",
       "      <th>diff_lf-t-seasonal_1</th>\n",
       "      <th>diff_pks-all</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6119010_11</th>\n",
       "      <td>0.635464</td>\n",
       "      <td>0.544323</td>\n",
       "      <td>0.672078</td>\n",
       "      <td>0.537085</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3422500.0</td>\n",
       "      <td>2352500.0</td>\n",
       "      <td>43.678226</td>\n",
       "      <td>-1.161527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8108.892578</td>\n",
       "      <td>-1091.800362</td>\n",
       "      <td>47.776888</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>-0.045455</td>\n",
       "      <td>2.818182</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>4.481481</td>\n",
       "      <td>328.303894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6119010_12</th>\n",
       "      <td>0.635464</td>\n",
       "      <td>0.544067</td>\n",
       "      <td>0.671657</td>\n",
       "      <td>0.536465</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3422500.0</td>\n",
       "      <td>2347500.0</td>\n",
       "      <td>43.633678</td>\n",
       "      <td>-1.152881</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7921.990234</td>\n",
       "      <td>-1094.935036</td>\n",
       "      <td>46.756319</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>-0.045455</td>\n",
       "      <td>2.954545</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>4.518519</td>\n",
       "      <td>316.262268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6119010_13</th>\n",
       "      <td>0.589989</td>\n",
       "      <td>0.522277</td>\n",
       "      <td>0.698611</td>\n",
       "      <td>0.582520</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3422500.0</td>\n",
       "      <td>2342500.0</td>\n",
       "      <td>43.589130</td>\n",
       "      <td>-1.144252</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-14805.981384</td>\n",
       "      <td>-713.167183</td>\n",
       "      <td>149.749269</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>-0.045455</td>\n",
       "      <td>1.590909</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>-335.368896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6119010_14</th>\n",
       "      <td>0.709440</td>\n",
       "      <td>0.570481</td>\n",
       "      <td>0.822756</td>\n",
       "      <td>0.639700</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3422500.0</td>\n",
       "      <td>2337500.0</td>\n",
       "      <td>43.544575</td>\n",
       "      <td>-1.135641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>11030.318359</td>\n",
       "      <td>-133.411201</td>\n",
       "      <td>200.735445</td>\n",
       "      <td>-0.090909</td>\n",
       "      <td>-0.045455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>7.227273</td>\n",
       "      <td>384.582581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6119010_15</th>\n",
       "      <td>0.612656</td>\n",
       "      <td>0.531497</td>\n",
       "      <td>0.700380</td>\n",
       "      <td>0.568576</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3422500.0</td>\n",
       "      <td>2332500.0</td>\n",
       "      <td>43.500019</td>\n",
       "      <td>-1.127047</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-11723.430176</td>\n",
       "      <td>-538.359810</td>\n",
       "      <td>131.454454</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.045455</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>6.083333</td>\n",
       "      <td>-240.407043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6983350_95</th>\n",
       "      <td>0.203398</td>\n",
       "      <td>0.227645</td>\n",
       "      <td>0.419789</td>\n",
       "      <td>0.451424</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6502500.0</td>\n",
       "      <td>2872500.0</td>\n",
       "      <td>45.141037</td>\n",
       "      <td>38.478489</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-14710.986877</td>\n",
       "      <td>-476.378696</td>\n",
       "      <td>48.027616</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>1.555556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>-990.893250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6983350_96</th>\n",
       "      <td>0.574453</td>\n",
       "      <td>0.580212</td>\n",
       "      <td>0.581299</td>\n",
       "      <td>0.579456</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6502500.0</td>\n",
       "      <td>2867500.0</td>\n",
       "      <td>45.099716</td>\n",
       "      <td>38.455753</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2752.287109</td>\n",
       "      <td>-400.673678</td>\n",
       "      <td>29.618272</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>897.935181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6983350_97</th>\n",
       "      <td>0.234536</td>\n",
       "      <td>0.260122</td>\n",
       "      <td>0.485846</td>\n",
       "      <td>0.515676</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6502500.0</td>\n",
       "      <td>2862500.0</td>\n",
       "      <td>45.058388</td>\n",
       "      <td>38.433064</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-14548.157715</td>\n",
       "      <td>-373.953403</td>\n",
       "      <td>43.290342</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>6.181818</td>\n",
       "      <td>-981.974487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6983350_98</th>\n",
       "      <td>0.231623</td>\n",
       "      <td>0.258969</td>\n",
       "      <td>0.471694</td>\n",
       "      <td>0.502134</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6502500.0</td>\n",
       "      <td>2857500.0</td>\n",
       "      <td>45.017052</td>\n",
       "      <td>38.410419</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-14094.062805</td>\n",
       "      <td>-313.008975</td>\n",
       "      <td>62.148603</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>-959.348145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6983350_99</th>\n",
       "      <td>0.208036</td>\n",
       "      <td>0.233330</td>\n",
       "      <td>0.426323</td>\n",
       "      <td>0.458662</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6502500.0</td>\n",
       "      <td>2852500.0</td>\n",
       "      <td>44.975704</td>\n",
       "      <td>38.387817</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-14710.501587</td>\n",
       "      <td>-425.817844</td>\n",
       "      <td>61.703191</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.444444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>4.818182</td>\n",
       "      <td>-990.906372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46639 rows × 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            clag-0-all  clag-1-all  clag-0-seasonal_1  clag-1-seasonal_1  \\\n",
       "ID                                                                         \n",
       "6119010_11    0.635464    0.544323           0.672078           0.537085   \n",
       "6119010_12    0.635464    0.544067           0.671657           0.536465   \n",
       "6119010_13    0.589989    0.522277           0.698611           0.582520   \n",
       "6119010_14    0.709440    0.570481           0.822756           0.639700   \n",
       "6119010_15    0.612656    0.531497           0.700380           0.568576   \n",
       "...                ...         ...                ...                ...   \n",
       "6983350_95    0.203398    0.227645           0.419789           0.451424   \n",
       "6983350_96    0.574453    0.580212           0.581299           0.579456   \n",
       "6983350_97    0.234536    0.260122           0.485846           0.515676   \n",
       "6983350_98    0.231623    0.258969           0.471694           0.502134   \n",
       "6983350_99    0.208036    0.233330           0.426323           0.458662   \n",
       "\n",
       "            n_buffer          x          y        lat        lon  target  ...  \\\n",
       "ID                                                                        ...   \n",
       "6119010_11       4.0  3422500.0  2352500.0  43.678226  -1.161527     0.0  ...   \n",
       "6119010_12       4.0  3422500.0  2347500.0  43.633678  -1.152881     0.0  ...   \n",
       "6119010_13       4.0  3422500.0  2342500.0  43.589130  -1.144252     0.0  ...   \n",
       "6119010_14       4.0  3422500.0  2337500.0  43.544575  -1.135641     0.0  ...   \n",
       "6119010_15       4.0  3422500.0  2332500.0  43.500019  -1.127047     0.0  ...   \n",
       "...              ...        ...        ...        ...        ...     ...  ...   \n",
       "6983350_95       4.0  6502500.0  2872500.0  45.141037  38.478489     0.0  ...   \n",
       "6983350_96       4.0  6502500.0  2867500.0  45.099716  38.455753     0.0  ...   \n",
       "6983350_97       4.0  6502500.0  2862500.0  45.058388  38.433064     0.0  ...   \n",
       "6983350_98       4.0  6502500.0  2857500.0  45.017052  38.410419     0.0  ...   \n",
       "6983350_99       4.0  6502500.0  2852500.0  44.975704  38.387817     0.0  ...   \n",
       "\n",
       "            diff_fhv-seasonal_1  diff_flv-all  diff_flv-seasonal_1  \\\n",
       "ID                                                                   \n",
       "6119010_11          8108.892578  -1091.800362            47.776888   \n",
       "6119010_12          7921.990234  -1094.935036            46.756319   \n",
       "6119010_13        -14805.981384   -713.167183           149.749269   \n",
       "6119010_14         11030.318359   -133.411201           200.735445   \n",
       "6119010_15        -11723.430176   -538.359810           131.454454   \n",
       "...                         ...           ...                  ...   \n",
       "6983350_95        -14710.986877   -476.378696            48.027616   \n",
       "6983350_96         -2752.287109   -400.673678            29.618272   \n",
       "6983350_97        -14548.157715   -373.953403            43.290342   \n",
       "6983350_98        -14094.062805   -313.008975            62.148603   \n",
       "6983350_99        -14710.501587   -425.817844            61.703191   \n",
       "\n",
       "            diff_hf-f-all  diff_hf-f-seasonal_1  diff_hf-t-all  diff_lf-f-all  \\\n",
       "ID                                                                              \n",
       "6119010_11       0.454545             -0.045455       2.818182       0.318182   \n",
       "6119010_12       0.454545             -0.045455       2.954545       0.318182   \n",
       "6119010_13       0.272727             -0.045455       1.590909       0.363636   \n",
       "6119010_14      -0.090909             -0.045455       0.000000       0.954545   \n",
       "6119010_15       0.000000             -0.045455       0.227273       0.409091   \n",
       "...                   ...                   ...            ...            ...   \n",
       "6983350_95       0.777778              0.111111       1.555556       0.000000   \n",
       "6983350_96       0.000000              0.000000       0.000000      -0.111111   \n",
       "6983350_97       0.333333              0.000000       1.000000      -0.111111   \n",
       "6983350_98       0.444444              0.000000       1.000000       0.222222   \n",
       "6983350_99       0.555556              0.000000       1.444444       0.000000   \n",
       "\n",
       "            diff_lf-f-seasonal_1  diff_lf-t-seasonal_1  diff_pks-all  \n",
       "ID                                                                    \n",
       "6119010_11              0.636364              4.481481    328.303894  \n",
       "6119010_12              0.636364              4.518519    316.262268  \n",
       "6119010_13              0.636364              6.000000   -335.368896  \n",
       "6119010_14              0.545455              7.227273    384.582581  \n",
       "6119010_15              0.500000              6.083333   -240.407043  \n",
       "...                          ...                   ...           ...  \n",
       "6983350_95              0.555556              6.000000   -990.893250  \n",
       "6983350_96              0.000000              0.000000    897.935181  \n",
       "6983350_97              0.555556              6.181818   -981.974487  \n",
       "6983350_98              0.444444              4.800000   -959.348145  \n",
       "6983350_99              0.555556              4.818182   -990.906372  \n",
       "\n",
       "[46639 rows x 85 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_similarity = calc_similarity(df_signatures, 'tag', calc_cols, methods=[similarity_method])\n",
    "\n",
    "df_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91941402",
   "metadata": {},
   "source": [
    "Import required scikit-learn functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f64aa99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, QuantileTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# algorithms \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.svm import SVC \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# evaluation \n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, mean_squared_error, balanced_accuracy_score "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9591ef9",
   "metadata": {},
   "source": [
    "Split a training & evaluation and test set. First investigate location of targets. Split datasets based on those distributions for equal division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b67686a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 gauges / buffers fall outside the buffer zone of 4:\n",
      "         d_X_cell    d_Y_cell\n",
      "count   23.000000   23.000000\n",
      "mean   -20.826087   -6.086957\n",
      "std     55.257786   54.266106\n",
      "min   -214.000000 -250.000000\n",
      "25%    -11.500000   -1.000000\n",
      "50%     -4.000000    6.000000\n",
      "75%      2.000000   10.500000\n",
      "max     13.000000   30.000000\n",
      "\n",
      "Analyse size of buffer or searching range with number of hits:\n",
      "In a search range of 0 cells, 244 matches are found\n",
      "In a search range of 1 cells, 284 matches are found\n",
      "In a search range of 2 cells, 32 matches are found\n",
      "In a search range of 3 cells, 9 matches are found\n",
      "In a search range of 4 cells, 3 matches are found\n",
      "\n",
      "Show result of split:\n",
      "Train val set:\n",
      "n_buffer\n",
      "0.0    207.0\n",
      "1.0    241.0\n",
      "2.0     27.0\n",
      "3.0      7.0\n",
      "4.0      2.0\n",
      "Name: target, dtype: float64\n",
      "\n",
      "Test set\n",
      "n_buffer\n",
      "0.0    37.0\n",
      "1.0    43.0\n",
      "2.0     5.0\n",
      "3.0     2.0\n",
      "4.0     1.0\n",
      "Name: target, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "## first split data based on being inside or outside the buffer zone (=4)\n",
    "df_outside_buffer = df_similarity[ df_similarity['target'] == -1].copy() \n",
    "gauge_outside = df_outside_buffer['tag'].unique().astype(int)\n",
    "\n",
    "fn_gauge = Path(r\"C:\\Users\\mvand\\Documents\\Master EE\\Year 4\\Thesis\\data\\training_data_S1\")  / \"V1_grdc_efas_selection-cartesius-snapped-1.csv\" \n",
    "meta_gauge = pd.read_csv(fn_gauge, index_col = 0)\n",
    "\n",
    "## analyse large shifts \n",
    "df_meta_gauge = meta_gauge.loc[gauge_outside]\n",
    "print('{} gauges / buffers fall outside the buffer zone of 4:'.format(len(gauge_outside)))\n",
    "print(df_meta_gauge[['d_X_cell', 'd_Y_cell']].describe())\n",
    "# print(df_meta_gauge[['d_X_cell', 'd_Y_cell']])\n",
    "\n",
    "## copy data to new dataframe \n",
    "df_predict_buffer = df_similarity.copy() #.drop(index=df_outside_buffer.index) \n",
    "## set all outside buffer vals to 0 \n",
    "df_predict_buffer.loc[df_outside_buffer.index, 'target'] = 0. \n",
    "## add target column for in-buffer prediction \n",
    "df_predict_buffer['in_buffer'] = 1 \n",
    "df_predict_buffer.loc[ df_predict_buffer['tag'].isin(gauge_outside)  , 'in_buffer'] = 0. \n",
    "\n",
    "print('\\nAnalyse size of buffer or searching range with number of hits:')\n",
    "for buffer_size in np.sort(df_predict_buffer['n_buffer'].unique()):\n",
    "    n_hits =df_predict_buffer[ (df_predict_buffer['n_buffer'] == buffer_size) & (df_predict_buffer['target']==1.) ]['target'].sum()\n",
    "    print('In a search range of {:.0f} cells, {:.0f} matches are found'.format(buffer_size, n_hits))\n",
    "\n",
    "\n",
    "## get buffers based on location of match \n",
    "buffer_0 = df_predict_buffer[ (df_predict_buffer['n_buffer'] == 0) & (df_predict_buffer['target']==1) ]['tag'].unique()\n",
    "buffer_1 = df_predict_buffer[ (df_predict_buffer['n_buffer'] == 1) & (df_predict_buffer['target']==1) ]['tag'].unique()\n",
    "buffer_2 = df_predict_buffer[ (df_predict_buffer['n_buffer'] == 2) & (df_predict_buffer['target']==1) ]['tag'].unique()\n",
    "buffer_3 = df_predict_buffer[ (df_predict_buffer['n_buffer'] == 3) & (df_predict_buffer['target']==1) ]['tag'].unique()\n",
    "buffer_4 = df_predict_buffer[ (df_predict_buffer['n_buffer'] == 4) & (df_predict_buffer['target']==1) ]['tag'].unique()\n",
    "\n",
    "## split with same ratio \n",
    "train_val_0, test_0 = train_test_split(buffer_0, test_size=0.15, random_state=21)\n",
    "train_val_1, test_1 = train_test_split(buffer_1, test_size=0.15, random_state=21)\n",
    "train_val_2, test_2 = train_test_split(buffer_2, test_size=0.15, random_state=21)\n",
    "train_val_3, test_3 = train_test_split(buffer_3, test_size=0.15, random_state=21)\n",
    "train_val_4, test_4 = train_test_split(buffer_4, test_size=0.15, random_state=21)\n",
    "train_val_out, test_out = train_test_split(gauge_outside, test_size=0.15, random_state=21)\n",
    "\n",
    "## combine all sets \n",
    "train_val_sets = (train_val_0, train_val_1, train_val_2, train_val_3, train_val_4, train_val_out)\n",
    "test_sets = (test_0, test_1, test_2, test_3, test_4, test_out)\n",
    "\n",
    "id_train_val = np.concatenate( train_val_sets )\n",
    "id_test = np.concatenate( test_sets )\n",
    "\n",
    "## split data \n",
    "df_train_val = df_predict_buffer[ df_predict_buffer['tag'].isin(id_train_val)].copy()\n",
    "df_test = df_predict_buffer[ df_predict_buffer['tag'].isin(id_test) ].copy()\n",
    "\n",
    "## VARY SEARCHING AREA\n",
    "## drop all rows with n_buffer values > set_buffer_size\n",
    "## if match outside buffer, remaining set will be all zeros --> no buffer in current search area \n",
    "df_train_val = df_train_val[ df_train_val['n_buffer'] <= set_buffer_size ].copy()\n",
    "df_test = df_test[ df_test['n_buffer'] <= set_buffer_size ].copy()\n",
    "\n",
    "print('\\nShow result of split:')\n",
    "print('Train val set:')\n",
    "print( df_train_val[df_train_val['target']==1].groupby(by='n_buffer')['target'].sum() )\n",
    "print('\\nTest set')\n",
    "print( df_test[df_test['target']==1].groupby(by='n_buffer')['target'].sum() )\n",
    "\n",
    "## list feature columns \n",
    "feature_columns = df_similarity.columns.values \n",
    "\n",
    "## remove non_feature cols \n",
    "if omit_coords:\n",
    "    for col in non_feature_cols+target_col+coord_cols:\n",
    "        col_ix = np.where(feature_columns==col)[0]\n",
    "        feature_columns = np.delete(feature_columns, col_ix)\n",
    "\n",
    "else:\n",
    "    for col in non_feature_cols+target_col:\n",
    "        col_ix = np.where(feature_columns==col)[0]\n",
    "#         feature_columns = np.delete(feature_columns, col_ix)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a79bd1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_ix = np.concatenate((id_train_val, id_test))\n",
    "\n",
    "# subset = meta_gauge.loc[all_ix, ['d_X_cell', 'd_Y_cell']]\n",
    "\n",
    "# subset['buffer_shift'] = 0 \n",
    "# x0 = 0 \n",
    "# y0 = 0 \n",
    "\n",
    "# for ix in subset.index:\n",
    "#     row = subset.loc[ix]\n",
    "    \n",
    "#     x_shift = ((row['d_X_cell'])**2)**0.5 \n",
    "#     y_shift = ((row['d_Y_cell'])**2)**0.5 \n",
    "#     max_shift = np.max([x_shift, y_shift])\n",
    "      \n",
    "#     subset.loc[ix,'buffer_shift'] = max_shift\n",
    "        \n",
    "# analyse_distances = subset['buffer_shift'].value_counts()\n",
    "\n",
    "# x_vals = analyse_distances.index\n",
    "# y_vals = analyse_distances.values \n",
    "\n",
    "\n",
    "# save_dir = Path(r\"C:\\Users\\mvand\\Documents\\Master EE\\Year 4\\Thesis\\meetings\\2021_06_15_EWC_sympo\\media\") \n",
    "# fn_1 = save_dir / 'shift_bins.png'\n",
    "# fn_2 = save_dir / 'shift_bins_zoom.png'\n",
    "\n",
    "# plt.figure(figsize=(12,4))\n",
    "# plt.bar(x_vals, y_vals)\n",
    "# plt.yscale('log')\n",
    "# plt.grid()\n",
    "# plt.xlabel('Shift nearest cell to matched cell', size=14);\n",
    "# plt.ylabel('n observations (log-scaled)', size=14);\n",
    "# plt.xticks(size=12);\n",
    "# plt.yticks(size=12);\n",
    "# plt.savefig(fn_1);\n",
    "\n",
    "# plt.figure(figsize=(8,4))\n",
    "# plt.bar(x_vals, y_vals)\n",
    "# plt.xlim(-1, 5)\n",
    "# plt.grid()\n",
    "# plt.xlabel('Shift nearest cell to matched cell', size=14);\n",
    "# plt.ylabel('n observations', size=14);\n",
    "# plt.xticks(size=12);\n",
    "# plt.yticks(size=12);\n",
    "# plt.savefig(fn_2);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a48f8b1",
   "metadata": {},
   "source": [
    "Load or calculate benchmark predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdecaf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmarks(df, id_col, target_col, obs_dir = None, sim_dir = None,\n",
    "               methods = ['CC', 'NSE', 'RMSE', 'KGE'],\n",
    "               fn_out = None): \n",
    "    \n",
    "    df_out = pd.DataFrame() \n",
    "    \n",
    "    df_out['ID'] = df.index.values \n",
    "    df_out[id_col] = df[id_col].values \n",
    "    df_out[target_col] = df[target_col].values \n",
    "    df_out = df_out.set_index('ID')\n",
    "    \n",
    "    buffer_idx = df_out[id_col].unique()\n",
    "    \n",
    "    for i in tqdm(range(len(buffer_idx))):\n",
    "        buffer_id = buffer_idx[i]\n",
    "        \n",
    "        _out = df_out[ df_out[id_col] == buffer_id] \n",
    "                \n",
    "        for method in methods:\n",
    "            \n",
    "            out_col = '{}_hat-{}'.format(target_col, method)\n",
    "            \n",
    "            if method in ['NSE', 'RMSE', 'KGE']:\n",
    "                \n",
    "                assert obs_dir.exists() , '[ERROR] obs_dir not specified or not found'\n",
    "                assert sim_dir.exists(), '[ERROR] sim_dir not specified or not found'\n",
    "                \n",
    "                fn_obs = obs_dir / '{}_Q_Day.Cmd.txt'.format(buffer_id)\n",
    "                fn_sim = sim_dir / 'buffer_{}_size-4.nc'.format(buffer_id) \n",
    "                \n",
    "                ## load observations\n",
    "                df_obs = pd.read_csv(fn_obs, skiprows=36, delimiter=';', encoding='cp850')\n",
    "                df_obs['Q_obs'] = df_obs[' Value'] \n",
    "                df_obs['date'] = pd.to_datetime(df_obs['YYYY-MM-DD'], yearfirst=True,\n",
    "                                               format='%Y-%m-%d')\n",
    "                df_obs = df_obs.drop(columns=[' Value', 'YYYY-MM-DD', 'hh:mm'])\n",
    "                df_obs = df_obs.set_index('date')\n",
    "                df_obs.loc[ df_obs['Q_obs'] == -999., 'Q_obs'] = np.nan \n",
    "                df_obs = df_obs.loc[df_obs.index >= '1991']\n",
    "                \n",
    "                ## load simulations \n",
    "                ds_sim = xr.open_dataset(fn_sim)\n",
    "                df_sim_list = ds_sim.to_dataframe().reset_index() \n",
    "                \n",
    "                df_sim = pd.DataFrame()\n",
    "                df_sim['date'] = pd.to_datetime( df_sim_list['time'].unique() )\n",
    "                df_sim  = df_sim.set_index('date')\n",
    "                \n",
    "                for i, x_cell in enumerate( df_sim_list['x'].unique() ):\n",
    "                    for j, y_cell in enumerate( df_sim_list['y'].unique() ):\n",
    "                        \n",
    "                        _df = df_sim_list[ (df_sim_list['x'] == x_cell) & (df_sim_list['y'] == y_cell) ]\n",
    "                        \n",
    "                        cell_id = '{}_{}{}'.format(buffer_id, int(i+1), int(j+1)) \n",
    "                        time = pd.to_datetime(_df['time'])\n",
    "                        df_sim.loc[time, cell_id] = _df['dis24'].values \n",
    "                \n",
    "                ## set simulations to observations extent \n",
    "                max_date = df_obs.tail(1).index.values[0]\n",
    "                min_date = df_obs.head(1).index.values[0]\n",
    "                df_sim = df_sim.loc[ (df_sim.index>=min_date) & (df_sim.index <= max_date)].copy()    \n",
    "                \n",
    "                ## mask and drop NaN values based on observations \n",
    "                gauge_mask = df_obs[df_obs['Q_obs'].isnull()>0].index \n",
    "                df_obs = df_obs.drop(index=gauge_mask)\n",
    "                df_sim = df_sim.drop(index=gauge_mask)   \n",
    "                \n",
    "                ## calculate selected metrics in the buffer \n",
    "                for cell in df_sim.columns:\n",
    "                    \n",
    "                    if method == 'NSE':\n",
    "                        try:\n",
    "                            res = he.evaluator(he.nse, df_sim[cell].values, df_obs['Q_obs'].values )[0]\n",
    "                        except:\n",
    "                            print(buffer_id)\n",
    "                            print(df_obs.head(2))\n",
    "                            print(df_obs.tail(2))\n",
    "                            print(df_sim.head(2).index)\n",
    "                            print(df_sim.tail(2).index)\n",
    "                            \n",
    "                        \n",
    "                    if method == 'RMSE':\n",
    "                        res = he.evaluator(he.rmse, df_sim[cell].values, df_obs['Q_obs'].values )[0]  \n",
    "                        \n",
    "                    if method == 'KGE':\n",
    "                        res, r, alpha, beta = he.evaluator(he.kge, df_sim[cell].values, df_obs['Q_obs'].values ) \n",
    "\n",
    "                    df_out.loc[cell, method] = res \n",
    "                \n",
    "                if method == 'NSE':\n",
    "                    y_hat_ix = df_out.loc[_out.index, method].idxmax()\n",
    "                if method == 'RMSE':\n",
    "                    y_hat_ix = df_out.loc[_out.index, method].idxmin()\n",
    "                if method == 'KGE':\n",
    "                    y_hat_ix = df_out.loc[_out.index, method].idxmax()\n",
    "                        \n",
    "                \n",
    "                df_out.loc[_out.index, out_col] = 0 \n",
    "                df_out.loc[y_hat_ix, out_col] = 1\n",
    "\n",
    "            if method == 'CC': \n",
    "                df_out.loc[ _out.index, out_col ] = 0 \n",
    "                center_ix = df[ (df['n_buffer'] == 0) & (df[id_col]==buffer_id)].index\n",
    "                df_out.loc[center_ix, out_col] = 1 \n",
    "            \n",
    "    df_out = df_out.dropna(axis=0)\n",
    "                \n",
    "    if fn_out == None:\n",
    "        return df_out\n",
    "    else:\n",
    "        df_out.to_csv(fn_out)\n",
    "        return fn_out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f5bd0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load or calculate benchmarks \n",
    "calc_bench = False\n",
    "fn_bench = signature_dir / \"S1_benchmarks.csv\" \n",
    "\n",
    "if calc_bench:\n",
    "\n",
    "    base_dir = Path(r\"C:\\Users\\mvand\\Documents\\Master EE\\Year 4\\Thesis\\data\\training_data_S1\") \n",
    "\n",
    "    obs_dir = base_dir / \"V1\" \n",
    "    sim_dir = base_dir / \"efas_output_nc\"\n",
    "\n",
    "    df_benchmarks = benchmarks(df_similarity, 'tag', target_col[0], \n",
    "                               obs_dir, sim_dir) #, fn_out = fn_bench)\n",
    "\n",
    "else:\n",
    "    df_benchmarks = pd.read_csv(fn_bench, index_col=0)\n",
    "\n",
    "df_benchmarks\n",
    "\n",
    "df_benchmarks['range_target'] = df_similarity['range_target']\n",
    "df_benchmarks.loc[ df_benchmarks['range_target'].isnull() ] = 0 \n",
    "\n",
    "benchmarks_train_val = df_benchmarks[df_benchmarks['tag'].isin(id_train_val)]\n",
    "benchmarks_test = df_benchmarks[df_benchmarks['tag'].isin(id_test)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91986f17",
   "metadata": {},
   "source": [
    "If necessary, data can be transformed into a grid for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8634ff97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_grid(df, feature_cols, target_col, range_col=None, n_elements=9):\n",
    "    \n",
    "    ## create  multidimensional array with NaN value: -999. \n",
    "    feature_grid = np.ones((len(feature_cols), n_elements, n_elements)) * -999. \n",
    "    target_grid = np.zeros((n_elements, n_elements))\n",
    "    \n",
    "    range_grid = None \n",
    "    if range_col != None:\n",
    "        range_grid = np.zeros((n_elements, n_elements))\n",
    "        \n",
    "    ## fill array \n",
    "    for ix in df.index:\n",
    "        grid_x, grid_y = int(ix.split('_')[-1][0]), int(ix.split('_')[-1][1]) \n",
    "                \n",
    "        feature_grid[:, int(grid_y-1), int(grid_x-1) ] = df.loc[ix, feature_cols].values \n",
    "        target_grid[int(grid_y-1),int(grid_x-1)] = df.loc[ix, target_col] \n",
    "        \n",
    "        if range_col!= None:\n",
    "            range_grid[int(grid_y-1),int(grid_x-1)] = df.loc[ix, range_col]\n",
    "                     \n",
    "    return feature_grid, target_grid, range_grid\n",
    "\n",
    "def reshape_to_grid(df, feature_cols, target_col, range_col=None, id_col='tag', buffer_size=4):\n",
    "    \n",
    "    n_elements = int(1 + (2*buffer_size))\n",
    "    \n",
    "    ## create emtpy output grids \n",
    "    grid_features = np.zeros((df[id_col].nunique(), len(feature_cols), n_elements, n_elements )) \n",
    "    grid_targets = np.zeros((df[id_col].nunique(), n_elements, n_elements))\n",
    "    \n",
    "    grid_range = None\n",
    "    if range_col != None:\n",
    "        grid_range = np.zeros((df[id_col].nunique(), n_elements, n_elements))\n",
    "    \n",
    "    ## reshape each buffer \n",
    "    for i, idx in tqdm(enumerate(df[id_col].unique())):\n",
    "                \n",
    "        df_buffer = df[ df[id_col] == idx ] \n",
    "        \n",
    "        grid_buffer_features, grid_buffer_target, grid_buffer_range = feature_grid(df_buffer, feature_cols, \n",
    "                                                                                   target_col, range_col, n_elements) \n",
    "            \n",
    "        grid_features[i] = grid_buffer_features\n",
    "        grid_targets[i] = grid_buffer_target\n",
    "        \n",
    "        if range_col != None:\n",
    "            grid_range[i] = grid_buffer_range\n",
    "    \n",
    "    if range_col == None:\n",
    "        return grid_features, grid_targets \n",
    "    \n",
    "    return grid_features, grid_targets, grid_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e26f14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "503it [00:29, 16.78it/s]\n",
      "92it [00:05, 16.16it/s]\n",
      "503it [00:30, 16.65it/s]\n",
      "92it [00:05, 16.49it/s]\n"
     ]
    }
   ],
   "source": [
    "if grid_format:\n",
    "    \n",
    "    ds_train_val = xr.Dataset()\n",
    "    ds_test = xr.Dataset()\n",
    "    \n",
    "    ## retransform datarame to grids \n",
    "    n_elements = min(set_buffer_size, 4)\n",
    "    \n",
    "    ## CASE 1 \n",
    "    ## for determining target in buffer \n",
    "    X1_train_val, y1_train_val= reshape_to_grid(df_train_val, feature_columns,\n",
    "                                                target_buffer[0], buffer_size=n_elements) \n",
    "    X1_test, y1_test = reshape_to_grid(df_test, feature_columns, \n",
    "                                       target_buffer[0], buffer_size=n_elements) \n",
    "    \n",
    "    ## flatten X1 \n",
    "    n_samples, n_features, n_rows, n_cols = X1_train_val.shape \n",
    "    X1_train_val_flatten = np.reshape(X1_train_val, (n_samples, int(n_features*n_rows*n_cols) ) )\n",
    "    X1_test_flatten = np.reshape(X1_test, (len(X1_test), int(n_features*n_rows*n_cols))  )\n",
    "    \n",
    "    \n",
    "    ## SAVE AS XARRAY ?\n",
    "#     coords, dummy_target = reshape_to_grid(df_train_val, ['x', 'y',],\n",
    "#                                       target_buffer[0], buffer_size = n_elements)\n",
    "#     print(coords)\n",
    "    \n",
    "    \n",
    "    ## simplify y1 values to single vector \n",
    "    y1_train_val_vector = np.zeros(n_samples)\n",
    "    for i in range(n_samples):\n",
    "        y1_train_val_vector[i] = np.max(y1_train_val[i])\n",
    "    \n",
    "    y1_test_vector = np.zeros(len(X1_test))\n",
    "    for i in range(len(y1_test_vector)):\n",
    "        y1_test_vector[i] = np.max(y1_test[i])\n",
    "    \n",
    "    \n",
    "    ## CASE 2 \n",
    "    ## for determining location of cell \n",
    "    ## for determining target in buffer \n",
    "    X2_train_val, y2_train_val, y2_train_val_range = reshape_to_grid(df_train_val, feature_columns,\n",
    "                                                                     target_col[0], range_col = range_col,\n",
    "                                                                     buffer_size=n_elements) \n",
    "    \n",
    "    X2_test, y2_test, y2_test_range = reshape_to_grid(df_test, feature_columns, \n",
    "                                                      target_col[0], range_col=range_col, \n",
    "                                                      buffer_size=n_elements) \n",
    "    \n",
    "    ## flatten X2 \n",
    "    n_samples, n_features, n_rows, n_cols = X2_train_val.shape \n",
    "    X2_train_val_flatten = np.reshape(X1_train_val, (n_samples, int(n_features*n_rows*n_cols) ) )\n",
    "    y2_train_val_flatten = np.reshape(y2_train_val, (n_samples, int(n_rows*n_cols) ) )\n",
    "    y2_train_val_range_flatten = np.reshape(y2_train_val_range, (n_samples, int(n_rows*n_cols) ) )\n",
    "    \n",
    "    n_test_samples = len(X2_test)\n",
    "    X2_test_flatten = np.reshape(X2_test, (n_test_samples, int(n_features*n_rows*n_cols))  )  \n",
    "    y2_test_flatten = np.reshape(y2_test, (n_test_samples, int(n_rows*n_cols) ) )\n",
    "    y2_test_range_flatten = np.reshape(y2_test_range, (n_test_samples, int(n_rows*n_cols) ) )   \n",
    "    \n",
    "    \n",
    "    ## SAVE AS XARRAY ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c64caa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "262596f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## sum y_target vals to see distribution \n",
    "# x_ticks = np.arange(0,9) + 0.5 \n",
    "# labels = np.arange(-4, 5, 1)\n",
    "\n",
    "# train_sum = y2_train_val.sum(axis=0)\n",
    "# test_sum = y2_test.sum(axis=0)\n",
    "# total_sum = train_sum + test_sum\n",
    "\n",
    "# plt.figure(figsize=(12,8))\n",
    "# sns.heatmap( total_sum , cmap =  'Blues', annot=True, fmt='g', \n",
    "#            cbar = False, annot_kws={\"size\":16})\n",
    "\n",
    "# plt.xticks(x_ticks, ['{}'.format(v) for v in labels], size = 14);\n",
    "# plt.yticks(x_ticks, ['{}'.format(v) for v in labels][::-1], rotation=0, size = 14);\n",
    "# plt.xlabel('Shift from center coordinate', size = 18);\n",
    "# plt.ylabel('Shift from center coordinate', size = 18);\n",
    "\n",
    "# save_dir = Path(r\"C:\\Users\\mvand\\Documents\\Master EE\\Year 4\\Thesis\\meetings\\2021_06_15_EWC_sympo\\media\") \n",
    "# fn = save_dir / 'grid_shift_view.png'\n",
    "# plt.savefig(fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab838881",
   "metadata": {},
   "source": [
    "Now, with k-fold cross validation find optimal algorithm settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6bf1c24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_performances(y, y_hat, k, model_name, grid_format = False,\n",
    "                          id_col='tag', target_col='range_target'):\n",
    "    \n",
    "    ## customize classifcation evaluation\n",
    "    TP = 0 \n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0 \n",
    "    n_targets = 0 \n",
    "    \n",
    "    if grid_format:\n",
    "        \n",
    "        n_samples, n_classes = y.shape \n",
    "        n_cells = n_samples * n_classes \n",
    "        n_samples, n_classes = y.shape       \n",
    "        \n",
    "        ## reshape to count TP, TN, FP, FN \n",
    "        for sample in range(len(y)):\n",
    "            \n",
    "            row_y = y[sample]\n",
    "            row_y_hat = y_hat[sample]\n",
    "            \n",
    "            ix_y = np.where(row_y > 0)[0]\n",
    "            ix_y_hat = np.where(row_y_hat > 0)[0]\n",
    "            \n",
    "            ## positve target label exists \n",
    "            if len(ix_y) > 0:\n",
    "                n_targets += 1 \n",
    "            \n",
    "                ## if a positive target label exists, and \n",
    "                ## prediction contains a positive label, \n",
    "                ## prediction is either true or false \n",
    "                if len(ix_y_hat) > 0:\n",
    "                    \n",
    "                    ## if prediction true, TP = 1 and TN are remaining cells \n",
    "                    if ix_y_hat[0] in ix_y:\n",
    "                        TP += 1 \n",
    "                        TN += (n_classes-1)\n",
    "                    \n",
    "                    ## if prediction is false, TP = 1 and TN are remaining cells \n",
    "                    if ix_y_hat[0] not in ix_y:\n",
    "                        FP += 1                    # or sum(row_y) ?, so 1-3 false positives)\n",
    "                        TN += (n_classes-1)        # or n_classes - sum(row_y)\n",
    "                \n",
    "                ## if a positive target label exists, but\n",
    "                ## prediction is all zeros, false negatives are counted \n",
    "                else: \n",
    "                    FN += 1 \n",
    "                    TN += (n_classes-1)\n",
    "            \n",
    "            ## no positive labels \n",
    "            if len(ix_y) == 0:\n",
    "                ## and prediction correctly all zero as well \n",
    "                if len(ix_y_hat) == 0:\n",
    "                    TN += n_classes \n",
    "                    \n",
    "                ## else one label is incorrectly labelled positive \n",
    "                ## rest is true negative \n",
    "                else:\n",
    "                    FP += 1\n",
    "                    TN += (n_classes-1)\n",
    "                    \n",
    "    else:\n",
    "        \n",
    "        ## reshape to count TP, TN, FP, FN \n",
    "        y['hat'] = y_hat\n",
    "        n_cells = len(y) \n",
    "        \n",
    "        for ix in y[id_col].unique():\n",
    "            _buffer = y[ y[id_col] == ix ]\n",
    "            \n",
    "            n_classes = len(_buffer)\n",
    "            \n",
    "            _y = _buffer[target_col]\n",
    "            _y_hat = _buffer['hat']\n",
    "            \n",
    "            ## if positive target\n",
    "            if _y.sum() > 0:\n",
    "                n_targets += 1 \n",
    "                _y_ix = _y[ _y>0].index.values \n",
    "\n",
    "                ## if positive prediction \n",
    "                if _y_hat.sum() > 0:\n",
    "                    _y_hat_ix = _y_hat.idxmax() \n",
    "                    \n",
    "                    ## if correct prediction \n",
    "                    if _y_hat_ix in _y_ix:\n",
    "                        TP += 1 \n",
    "                        TN += (n_classes-1)\n",
    "                    ## incorrect prediction\n",
    "                    else:\n",
    "                        FP += 1\n",
    "                        TN += (n_classes-1)\n",
    "                        \n",
    "                ## if no positive prediction\n",
    "                else:\n",
    "                    FN += 1 \n",
    "                    TN += (n_classes-1)\n",
    "                    \n",
    "            ## if no positive targets \n",
    "            else:\n",
    "                ## if no positive predictions \n",
    "                if _y_hat.sum() == 0:\n",
    "                    TN += n_classes\n",
    "                ## if positive predictions \n",
    "                else:\n",
    "                    FP += 1\n",
    "                    TN += (n_classes-1)\n",
    "    \n",
    "    ## calculate metrics \n",
    "    acc = (TP+TN) / (TP+TN+FP+FN)\n",
    "    b_acc = 0.5 * ( (TP/(TP+FN)) + (TN/(TN+FP))  )\n",
    "    prec = TP / (TP+FP)\n",
    "    rec = TP / (TP+FN) \n",
    "    f1 = (prec*rec) / (prec+rec+1e-6) \n",
    "    hit_rate = TP / n_targets \n",
    "\n",
    "    return_df = pd.DataFrame({\n",
    "                            'k': [k],\n",
    "                            'model': [model_name],\n",
    "                            'accuracy': [acc],\n",
    "                            'balanced_acc': [b_acc],\n",
    "                            'precision': [prec],\n",
    "                            'recall': [rec],\n",
    "                            'f1': [f1],\n",
    "                            'hit_rate': [hit_rate],\n",
    "                            'n': [n_targets],\n",
    "                            'N': [n_cells],\n",
    "                            'TP': [TP],\n",
    "                            'TN': [TN],\n",
    "                            'FP': [FP],\n",
    "                            'FN': [FN]\n",
    "                              })     \n",
    "    return return_df\n",
    "\n",
    "\n",
    "def buffer_classifier(df_val, y_prob, p0_col, p1_col,\n",
    "                     prediction_col, id_col, prob_threshold):\n",
    "        \n",
    "    df_val[p0_col] = y_prob[:,0]\n",
    "    df_val[p1_col] = y_prob[:,1] \n",
    "    df_val[prediction_col] = 0.\n",
    "    \n",
    "    for ix in df_val[id_col].unique():\n",
    "        max_ix = df_val[ (df_val[id_col]==ix) ][p1_col].idxmax()\n",
    "        \n",
    "        if df_val.loc[max_ix, p1_col] >= prob_threshold:\n",
    "            df_val.loc[max_ix, prediction_col] = 1.\n",
    "\n",
    "    return df_val\n",
    "\n",
    "def k_foldCV(X, y = [],  grid_format = False, K = 5, \n",
    "             id_col = None, feature_cols = [], target_col = None,\n",
    "             range_col = None, y_range=[],\n",
    "             n_subsample = 1, do_norm = True, do_scale = True, \n",
    "             do_PCA = True, n_pca = 0.9, whiten_pca = False,\n",
    "             methods = ['LR-2'], prob_threshold = 0.5, \n",
    "             return_classification = False, gauge_id_list = []):\n",
    "             \n",
    "#              benchmarks = ['CC', 'NSE', 'RMSE', 'KGE'],\n",
    "#              df_benchmarks = None):\n",
    "\n",
    "    df_performance = pd.DataFrame()\n",
    "    \n",
    "    ## for returning classification \n",
    "    if grid_format:\n",
    "        ds_collect_val = xr.Dataset()\n",
    "        n_cells = y.shape[1] \n",
    "        ## assume square grid \n",
    "        n_buffer = int(n_cells**0.5)\n",
    "    else:\n",
    "        df_collect_val = pd.DataFrame()\n",
    "        \n",
    "    \n",
    "    if grid_format:\n",
    "        assert len(y) > 0, 'target column not specified'\n",
    "        sort_ix = list(range(len(X)))\n",
    "                \n",
    "        \n",
    "    if not grid_format:\n",
    "        assert id_col != None, 'buffer id col not specified'\n",
    "        assert target_col != None, 'buffer id col not specified'\n",
    "        assert len(feature_cols) > 0, 'feature columns not specified'\n",
    "        \n",
    "        sort_ix = X[id_col].unique()\n",
    "    \n",
    "    \n",
    "    ## shuffle and split ids \n",
    "    np.random.seed(26)\n",
    "    np.random.shuffle(sort_ix)\n",
    "    \n",
    "    ## split in K folds \n",
    "    k_split_idx = np.array_split(sort_ix, K)\n",
    "    \n",
    "    for k in tqdm(range(K)):\n",
    "        \n",
    "        ## split samples \n",
    "        id_val = k_split_idx[k]\n",
    "        id_train = np.setdiff1d(sort_ix, id_val)\n",
    "        \n",
    "        if grid_format:\n",
    "            X_train = X[id_train]\n",
    "            y_train = y[id_train]\n",
    "            \n",
    "            X_val = X[id_val]\n",
    "            y_val = y[id_val]\n",
    "            \n",
    "            if len(y_range) > 0:\n",
    "                y_val_range = y_range[id_val]\n",
    "        \n",
    "        if not grid_format:\n",
    "            \n",
    "            set_train = X[ X[id_col].isin(id_train) ]\n",
    "            set_val = X[ X[id_col].isin(id_val)]\n",
    "            \n",
    "            ## subsample training set only \n",
    "            set_train_1 = set_train[ set_train[target_col] == 1 ]\n",
    "            \n",
    "            set_train_0_ix = []\n",
    "            for i, idx in enumerate(id_train):\n",
    "                _df = set_train[ (set_train[target_col] != 1) & (set_train[id_col] == idx) ] \n",
    "                subsamples = _df.sample(n=n_subsample).index \n",
    "                for sample in subsamples:\n",
    "                    set_train_0_ix.append(sample)\n",
    "            set_train_0 = set_train.loc[set_train_0_ix]\n",
    "            \n",
    "            ## sample in_buffer = 0 samples \n",
    "            set_not_in_buffer = set_train[set_train['in_buffer']!=1]\n",
    "            n_sample = set_not_in_buffer[id_col].nunique()\n",
    "            print(n_sample)\n",
    "\n",
    "            ## add subsampled sets             \n",
    "            sample_train = set_train_1.append(set_train_0)\n",
    "            sample_train = sample_train.append( set_not_in_buffer.sample(n=n_sample) )\n",
    "            \n",
    "            ## split X and y \n",
    "            X_train = sample_train[feature_cols]\n",
    "            y_train = sample_train[target_col]\n",
    "\n",
    "            X_val = set_val[feature_cols]\n",
    "            y_val = set_val[target_col]\n",
    "            df_y_val = set_val.copy() #[[id_col, target_col]].copy()#.to_frame()\n",
    "\n",
    "            if range_col is not None:\n",
    "                y_val_range = set_val[[range_col, id_col]].copy()\n",
    "        \n",
    "        ## preprocess training data \n",
    "        if do_norm:\n",
    "            min_samples = len(X_train)\n",
    "            nm = QuantileTransformer(output_distribution='normal',\n",
    "                                    n_quantiles = int(min(1000, min_samples)))\n",
    "            X_train = nm.fit_transform(X_train)\n",
    "        else:\n",
    "            nm = None \n",
    "        \n",
    "        if do_scale:\n",
    "            sc = MinMaxScaler([0,1])\n",
    "            X_train = sc.fit_transform(X_train)\n",
    "        else:\n",
    "            sc = None\n",
    "        \n",
    "        if do_PCA:\n",
    "            pca = PCA(n_components = n_pca, whiten = whiten_pca)\n",
    "            pca.fit(X_train)\n",
    "            X_train = pca.transform(X_train)\n",
    "        else:\n",
    "            pca = None \n",
    "            \n",
    "        ## prepare trainin data \n",
    "        if do_norm:\n",
    "            X_val = nm.transform(X_val)\n",
    "        if do_scale:\n",
    "            X_val = sc.transform(X_val)\n",
    "        if do_PCA:\n",
    "            X_val = pca.transform(X_val)        \n",
    "        \n",
    "        ##  test models \n",
    "        for method in methods: \n",
    "            \n",
    "            prediction_col = '{}_{}'.format(method, target_col)\n",
    "            p0_col = '{}_p0'.format(method)\n",
    "            p1_col = '{}_p1'.format(method)\n",
    "                        \n",
    "            if 'LR' in method:\n",
    "                \n",
    "                ## train model \n",
    "                lr = LogisticRegression(max_iter=1000)\n",
    "                \n",
    "                if grid_format:\n",
    "                    lr = OneVsRestClassifier( lr )\n",
    "                \n",
    "                lr.fit(X_train, y_train)\n",
    "                \n",
    "                ## evaluate                                 \n",
    "                if 'LR-1' in method:\n",
    "                    y_val_hat = lr.predict(X_val)\n",
    "                        \n",
    "                if 'LR-2' in method:\n",
    "                    y_val_hat_prob = lr.predict_proba(X_val)\n",
    "                    \n",
    "                    if grid_format:\n",
    "                        y_val_hat = np.where( y_val_hat_prob >= prob_threshold, 1, 0)\n",
    "                    else:\n",
    "                        \n",
    "                        df_y_val = buffer_classifier(df_y_val, y_val_hat_prob,\n",
    "                                                    p0_col, p1_col, prediction_col,\n",
    "                                                    id_col, prob_threshold)\n",
    "                        \n",
    "                        y_val_hat = df_y_val[prediction_col]  \n",
    "\n",
    "                        \n",
    "                \n",
    "            if 'RF' in method:\n",
    "                \n",
    "                ## train model \n",
    "                rfc = RandomForestClassifier()                \n",
    "                rfc.fit(X_train, y_train)\n",
    "                \n",
    "                if 'RF-1' in method:\n",
    "                    y_val_hat = rfc.predict(X_val)\n",
    "                     \n",
    "                if 'RF-2' in method:\n",
    "                    _y_val_hat_prob = rfc.predict_proba(X_val)\n",
    "                    \n",
    "                    if grid_format:\n",
    "                        y_val_hat = np.zeros(y_val.shape)\n",
    "                        y_val_hat_prob = np.zeros(y_val.shape)\n",
    "\n",
    "                        for sample in range(len(_y_val_hat_prob)):\n",
    "                            if _y_val_hat_prob[sample].shape[1] >1:\n",
    "                                p1 = _y_val_hat_prob[sample][:,1]\n",
    "                                _y_val_hat = np.zeros(len(p1)) \n",
    "                                \n",
    "                                ix_max = p1.argmax() \n",
    "                                \n",
    "                                if p1[p1.argmax()] >= prob_threshold:\n",
    "                                    _y_val_hat[ix_max] = 1. \n",
    "\n",
    "                                y_val_hat[:,sample] = _y_val_hat \n",
    "                                y_val_hat_prob[:,sample] += p1 \n",
    "                    \n",
    "                    else:\n",
    "                        y_val_hat_prob = _y_val_hat_prob \n",
    "                        df_y_val = buffer_classifier(df_y_val, y_val_hat_prob,\n",
    "                                                    p0_col, p1_col, prediction_col,\n",
    "                                                    id_col, prob_threshold)\n",
    "                        \n",
    "                        y_val_hat = df_y_val[prediction_col]\n",
    "                    \n",
    "            if 'SVM' in method:\n",
    "                \n",
    "                svc = SVC(probability=True)\n",
    "                \n",
    "                if grid_format:\n",
    "                    svc = OneVsRestClassifier(svc) \n",
    "                \n",
    "                svc.fit(X_train, y_train)\n",
    "                \n",
    "                if 'SVM-1' in method:\n",
    "                    y_val_hat = svc.predict(X_val)\n",
    "                    \n",
    "                if 'SVM-2' in method:\n",
    "                    y_val_hat_prob = svc.predict_proba(X_val)\n",
    "                    \n",
    "                    if grid_format:\n",
    "                        y_val_hat = np.where( y_val_hat_prob >= prob_threshold, 1, 0)\n",
    "                        \n",
    "                    else:   \n",
    "                        \n",
    "                        df_y_val = buffer_classifier(df_y_val, y_val_hat_prob,\n",
    "                                                    p0_col, p1_col, prediction_col,\n",
    "                                                    id_col, prob_threshold)\n",
    "                        \n",
    "                        y_val_hat = df_y_val[prediction_col]                \n",
    "\n",
    "            if 'k-nn' in method:\n",
    "                \n",
    "                if not do_PCA:\n",
    "                    \n",
    "                    if not do_norm:\n",
    "                        min_samples = len(X_train)\n",
    "                        nm = QuantileTransformer(output_distribution='normal',\n",
    "                                                n_quantiles = int(min(1000, min_samples)))\n",
    "                        X_train = nm.fit_transform(X_train)\n",
    "                        X_val = nm.transform(X_train)\n",
    "                    \n",
    "                    if not do_scale:\n",
    "                        sc = MinMaxScaler([0,1])\n",
    "                        X_train = sc.fit_transorm(X_train)\n",
    "                        X_val = sc.transform(X_val)\n",
    "                    \n",
    "                    pca = PCA(n_components = n_pca)\n",
    "                    pca.fit(X_train)\n",
    "                    X_train = pca.transform(X_train)\n",
    "                    X_val = pca.transform(X_val)\n",
    "                \n",
    "                knn = KNeighborsClassifier() \n",
    "                                \n",
    "                knn.fit(X_train, y_train) \n",
    "                \n",
    "                if 'k-nn-1' in method:\n",
    "                    y_val_hat = knn.predict(X_val)\n",
    "                    \n",
    "                if 'k-nn-2' in method:\n",
    "                    _y_val_hat_prob = knn.predict_proba(X_val)\n",
    "                    \n",
    "                    if grid_format:\n",
    "                        y_val_hat = np.zeros(y_val.shape)\n",
    "                        y_val_hat_prob = np.zeros(y_val.shape)\n",
    "\n",
    "                        for sample in range(len(_y_val_hat_prob)):\n",
    "                            if _y_val_hat_prob[sample].shape[1] >1:\n",
    "                                p1 = _y_val_hat_prob[sample][:,1]\n",
    "                                _y_val_hat = np.zeros(len(p1)) \n",
    "                                \n",
    "                                ix_max = p1.argmax() \n",
    "                                \n",
    "                                if p1[p1.argmax()] >= prob_threshold:\n",
    "                                    _y_val_hat[ix_max] = 1. \n",
    "\n",
    "                                y_val_hat[:,sample] = _y_val_hat\n",
    "                                y_val_hat_prob[:,sample] += p1 \n",
    "                    \n",
    "                    else: \n",
    "                        y_val_hat_prob = _y_val_hat_prob \n",
    "                        df_y_val = buffer_classifier(df_y_val, y_val_hat_prob,\n",
    "                                                    p0_col, p1_col, prediction_col,\n",
    "                                                    id_col, prob_threshold)\n",
    "                        \n",
    "                        y_val_hat = df_y_val[prediction_col]\n",
    "                    \n",
    "            \n",
    "                               \n",
    "            ## calculate perforamnce \n",
    "            df_performance = df_performance.append( benchmark_performances(y_val_range, y_val_hat,\n",
    "                                                                          k, method, grid_format=grid_format) )\n",
    "            if not grid_format:\n",
    "                try:\n",
    "                    df_collect_val.loc[df_y_val.index, df_y_val.columns] = df_y_val \n",
    "                except:\n",
    "                    df_collect_val = df_collect_val.append(df_y_val)\n",
    "            \n",
    "            else:\n",
    "                 \n",
    "                ## reshape data back to grid \n",
    "                n_samples = len(id_val)\n",
    "                y_val_gridded = y_val.reshape((n_samples, n_buffer, n_buffer)) \n",
    "                y_val_hat_gridded = y_val_hat.reshape((n_samples, n_buffer, n_buffer))\n",
    "                \n",
    "                if len(y_range) > 0:\n",
    "                    y_val_range_gridded = y_val_range.reshape((n_samples, n_buffer, n_buffer))\n",
    "                \n",
    "                try:\n",
    "                    print(method)\n",
    "                    y_hat_p1_gridded = y_val_hat_prob.reshape((n_samples, n_buffer, n_buffer))\n",
    "                except:\n",
    "                    y_hat_p1_gridded = [] \n",
    "                                \n",
    "                for i, idv in enumerate(id_val):\n",
    "        \n",
    "                    ## layer name \n",
    "                    ## with index to retract original gauge id value \n",
    "                    if len(gauge_id_list) == 0:\n",
    "                        layer_y_hat = f'{idv}_{method}_y_hat'\n",
    "                        layer_y = f'{idv}_{method}_y' \n",
    "                        layer_y_range = f'{idv}_{method}_y_range'\n",
    "                        layer_p1_prob = f'{idv}_{method}_p1'\n",
    "                        \n",
    "                    else:\n",
    "                        gauge_id = gauge_id_list[idv]\n",
    "                        layer_y_hat = f'{gauge_id}_{method}_y_hat'\n",
    "                        layer_y = f'{gauge_id}_{method}_y'\n",
    "                        layer_y_range = f'{gauge_id}_{method}_y_range'\n",
    "                        layer_p1_prob = f'{gauge_id}_{method}_p1'\n",
    "                    \n",
    "                    ## reshape data to grid \n",
    "                    ds_collect_val[layer_y_hat] = ((\"x\", \"y\"), y_val_hat_gridded[i] )\n",
    "                    ds_collect_val[layer_y] = ((\"x\", \"y\"), y_val_gridded[i] )\n",
    "                    \n",
    "                    if len(y_range) > 0:\n",
    "                        ds_collect_val[layer_y_range] = ((\"x\", \"y\"), y_val_range_gridded[i] )\n",
    "                    \n",
    "                    if len(y_hat_p1_gridded) > 0:\n",
    "                        ds_collect_val[layer_p1_prob] = ((\"x\", \"y\"), y_hat_p1_gridded[i])\n",
    "                    \n",
    "\n",
    "    if return_classification:\n",
    "        if grid_format:\n",
    "            return df_performance, ds_collect_val    \n",
    "        else:\n",
    "            return df_performance, df_collect_val                 \n",
    "    else:\n",
    "        return df_performance \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1a67ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## K-fold settings \n",
    "\n",
    "n_folds = 1 \n",
    "algorithms = ['LR-1', 'LR-2', 'RF-1', 'RF-2', 'SVM-1', 'SVM-2', 'k-nn-1', 'k-nn-2'] \n",
    "\n",
    "stat_cols = ['accuracy', 'balanced_acc', 'precision', 'recall', 'f1',  \n",
    "             'TP', 'TN', 'FP', 'FN', 'hit_rate', 'n', 'N']\n",
    "\n",
    "std_cols = ['accuracy', 'balanced_acc', 'precision', 'recall', 'f1',  \n",
    "             'TP', 'TN', 'FP', 'FN', 'hit_rate']\n",
    "\n",
    "test_dir = Path(r\"C:\\Users\\mvand\\Documents\\Master EE\\Year 4\\Thesis\\data\\test_output\\PCA_analysis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d96ab70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                     | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-nn-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████████████████████████▏                                                                                                                | 1/5 [00:14<00:57, 14.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-nn-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████████████████████████████████████████████████████████▍                                                                                    | 2/5 [00:41<01:06, 22.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-nn-2\n"
     ]
    }
   ],
   "source": [
    "### no warnings: RF and k-nn, \n",
    "### LR1,2 give non-fatal warning, returns [n_samples, n_pixels/classes] for both  ?\n",
    "### SVM1,2 give non-fatal warning, returns [n_samples, n_pixels/classes] for both ?\n",
    "### RF-2 returns a list with length 81 (n_pixels/classes), and each item in list has lenght 101 (n_samples), with\n",
    "### 1 or 2 columns \n",
    "### knn-2 returns a list with length 81 (n_pixels/classes), and each item in list has lenght 101 (n_samples), with\n",
    "### 1 or 2 columns \n",
    "\n",
    "performance, ds_val = k_foldCV(X2_train_val_flatten,  y2_train_val_flatten, grid_format=True,\n",
    "                        y_range = y2_train_val_range_flatten, methods=['k-nn-2'], \n",
    "                      gauge_id_list = id_train_val, return_classification=True) # methods = algorithms) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "718dabc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>balanced_acc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>TP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>n</th>\n",
       "      <th>N</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>k-nn-2</th>\n",
       "      <td>0.988268</td>\n",
       "      <td>0.506169</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>0.012189</td>\n",
       "      <td>1.2</td>\n",
       "      <td>8051.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>94.6</td>\n",
       "      <td>0.012355</td>\n",
       "      <td>96.8</td>\n",
       "      <td>8148.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        accuracy  balanced_acc  precision    recall        f1   TP      TN  \\\n",
       "model                                                                        \n",
       "k-nn-2  0.988268      0.506169   0.633333  0.012463  0.012189  1.2  8051.8   \n",
       "\n",
       "         FP    FN  hit_rate     n       N  \n",
       "model                                      \n",
       "k-nn-2  1.0  94.6  0.012355  96.8  8148.6  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance.groupby(by='model')[stat_cols].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecb48e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e4795e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _df, df_class_noPCA = k_foldCV( df_train_val, id_col='tag', target_col = target_col[0],\n",
    "#                                    range_col = 'range_target',feature_cols = feature_columns, \n",
    "#                                    methods = algorithms, do_PCA = True, return_classification=True)\n",
    "\n",
    "# _df.groupby(by='model')[stat_cols].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ee8a44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b325bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PCA sensitivity analysis - for raster pixel \n",
    "\n",
    "df_grid_no_pca = pd.DataFrame()\n",
    "df_grid_pca60 = pd.DataFrame()\n",
    "df_grid_pca70 = pd.DataFrame()\n",
    "df_grid_pca80 = pd.DataFrame()\n",
    "df_grid_pca90 = pd.DataFrame()\n",
    "df_grid_pca95 = pd.DataFrame()\n",
    "df_grid_pca99 = pd.DataFrame() \n",
    "\n",
    "for kn in range(n_folds):\n",
    "    print('Iteration {}'.format(int(kn+1)))\n",
    "    ## no pca \n",
    "#     _df = k_foldCV( df_train_val, id_col='tag', target_col = target_col[0], range_col = 'range_target',\n",
    "#                   feature_cols = feature_columns, methods = algorithms, do_PCA = False)\n",
    "#     df_grid_no_pca =df_grid_no_pca.append(_df)\n",
    "\n",
    "#     ## PCA 60% \n",
    "#     _df = k_foldCV( df_train_val, id_col='tag', target_col = target_col[0], range_col = 'range_target',\n",
    "#                   feature_cols = feature_columns, methods = algorithms, do_PCA = True, n_pca = 0.6 )\n",
    "#     df_grid_pca60 = df_grid_pca60.append(_df)    \n",
    "\n",
    "#     ## PCA 70% \n",
    "#     _df = k_foldCV( df_train_val, id_col='tag', target_col = target_col[0], range_col = 'range_target',\n",
    "#                   feature_cols = feature_columns, methods = algorithms, do_PCA = True, n_pca = 0.7 )\n",
    "#     df_gridle_pca70 = df_grid_pca70.append(_df) \n",
    "\n",
    "#     ## PCA 80% \n",
    "#     _df = k_foldCV( df_train_val, id_col='tag', target_col = target_col[0], range_col = 'range_target',\n",
    "#                   feature_cols = feature_columns, methods = algorithms, do_PCA = True, n_pca = 0.8 )\n",
    "#     df_grid_pca80 = df_grid_pca80.append(_df) \n",
    "\n",
    "#     ## PCA 90% \n",
    "#     _df = k_foldCV( df_train_val, id_col='tag', target_col = target_col[0], range_col = 'range_target',\n",
    "#                   feature_cols = feature_columns, methods = algorithms, do_PCA = True, n_pca = 0.9 )\n",
    "#     df_grid_pca90 = df_grid_pca90.append(_df) \n",
    "\n",
    "#     ## PCA 95% \n",
    "#     _df = k_foldCV( df_train_val, id_col='tag', target_col = target_col[0], range_col = 'range_target',\n",
    "#                   feature_cols = feature_columns, methods = algorithms, do_PCA = True, n_pca = 0.95 )\n",
    "#     df_grid_pca95 = df_grid_pca95.append(_df) \n",
    "\n",
    "#     ## PCA 99% \n",
    "#     _df = k_foldCV( df_train_val, id_col='tag', target_col = target_col[0], range_col = 'range_target',\n",
    "#                   feature_cols = feature_columns, methods = algorithms, do_PCA = True, n_pca = 0.99 )\n",
    "#     df_grid_pca99 = df_grid_pca99.append(_df) \n",
    "\n",
    "\n",
    "# list_df = [df_single_no_pca, df_single_pca60, df_single_pca70, df_single_pca80, df_single_pca90, df_single_pca95, df_single_pca99 ]\n",
    "\n",
    "# list_name = ['alg_set-2_grid_noPCA.csv',\n",
    "#             'alg_set-2_grid_60PCA.csv',\n",
    "#             'alg_set-2_grid_70PCA.csv',\n",
    "#             'alg_set-2_grid_80PCA.csv',\n",
    "#             'alg_set-2_grid_90PCA.csv',\n",
    "#             'alg_set-2_grid_95PCA.csv',\n",
    "#             'alg_set-2_grid_99PCA.csv',]\n",
    "\n",
    "# for i, out_df in enumerate(list_df):\n",
    "    \n",
    "#     fn = test_dir / list_name[i]\n",
    "#     print(fn)\n",
    "#     out_df.to_csv(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb77c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PCA sensitivity analysis - for single pixel \n",
    "\n",
    "df_single_no_pca = pd.DataFrame()\n",
    "df_single_pca60 = pd.DataFrame()\n",
    "df_single_pca70 = pd.DataFrame()\n",
    "df_single_pca80 = pd.DataFrame()\n",
    "df_single_pca90 = pd.DataFrame()\n",
    "df_single_pca95 = pd.DataFrame()\n",
    "df_single_pca99 = pd.DataFrame() \n",
    "\n",
    "for kn in range(n_folds):\n",
    "    print('Iteration {}'.format(int(kn+1)))\n",
    "    ## no pca \n",
    "    _df, df_class_noPCA = k_foldCV( df_train_val, id_col='tag', target_col = target_col[0],\n",
    "                                   range_col = 'range_target',feature_cols = feature_columns, \n",
    "                                   methods = algorithms, do_PCA = False, return_classification=True)\n",
    "#     df_single_no_pca = df_single_no_pca.append(_df)\n",
    "\n",
    "#     ## PCA 60% \n",
    "#     _df, df_class_PCA60 = k_foldCV( df_train_val, id_col='tag', target_col = target_col[0], \n",
    "#                                    range_col = 'range_target', feature_cols = feature_columns, \n",
    "#                                    methods = algorithms, do_PCA = True, n_pca = 0.6, return_classification=True )\n",
    "    \n",
    "#     df_single_pca60 = df_single_pca60.append(_df)    \n",
    "\n",
    "#     ## PCA 70% \n",
    "#     _df, df_class_PCA70 = k_foldCV( df_train_val, id_col='tag', target_col = target_col[0], \n",
    "#                                    range_col = 'range_target', feature_cols = feature_columns, \n",
    "#                                    methods = algorithms, do_PCA = True, n_pca = 0.7, return_classification=True)\n",
    "#     df_single_pca70 = df_single_pca70.append(_df) \n",
    "\n",
    "#     ## PCA 80% \n",
    "#     _df, df_class_PCA80 = k_foldCV( df_train_val, id_col='tag', target_col = target_col[0],\n",
    "#                                    range_col = 'range_target', feature_cols = feature_columns,\n",
    "#                                    methods = algorithms, do_PCA = True, n_pca = 0.8, return_classification=True )\n",
    "#     df_single_pca80 = df_single_pca80.append(_df) \n",
    "\n",
    "#     ## PCA 90% \n",
    "#     _df, df_class_PCA90 = k_foldCV( df_train_val, id_col='tag', target_col = target_col[0], \n",
    "#                                    range_col = 'range_target', feature_cols = feature_columns, \n",
    "#                                    methods = algorithms, do_PCA = True, n_pca = 0.9, return_classification=True )\n",
    "#     df_single_pca90 = df_single_pca90.append(_df) \n",
    "\n",
    "#     ## PCA 95% \n",
    "#     _df, df_class_PCA95 = k_foldCV( df_train_val, id_col='tag', target_col = target_col[0], \n",
    "#                                    range_col = 'range_target', feature_cols = feature_columns, \n",
    "#                                    methods = algorithms, do_PCA = True, n_pca = 0.95, return_classification=True )\n",
    "#     df_single_pca95 = df_single_pca95.append(_df) \n",
    "\n",
    "#     ## PCA 99% \n",
    "#     _df, df_class_PCA99 = k_foldCV( df_train_val, id_col='tag', target_col = target_col[0], \n",
    "#                                    range_col = 'range_target', feature_cols = feature_columns,\n",
    "#                                    methods = algorithms, do_PCA = True, n_pca = 0.99, return_classification=True )\n",
    "#     df_single_pca99 = df_single_pca99.append(_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38d3e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SAVE RESULTS \n",
    "# list_df = [df_single_no_pca, df_single_pca60, df_single_pca70, df_single_pca80, df_single_pca90, df_single_pca95, df_single_pca99 ]\n",
    "\n",
    "# list_class = [df_class_noPCA, df_class_PCA60, df_class_PCA70, df_class_PCA80, df_class_PCA90, df_class_PCA95, df_class_PCA99]\n",
    "\n",
    "# list_perf = ['alg_set-2_single_noPCA.csv',\n",
    "#             'alg_set-2_single_60PCA.csv',\n",
    "#             'alg_set-2_single_70PCA.csv',\n",
    "#             'alg_set-2_single_80PCA.csv',\n",
    "#             'alg_set-2_single_90PCA.csv',\n",
    "#             'alg_set-2_single_95PCA.csv',\n",
    "#             'alg_set-2_single_99PCA.csv',]\n",
    "\n",
    "# list_class = [ 'alg_set-2_class_single_noPCA.csv',\n",
    "#            'alg_set-2_class_single_60PCA.csv',\n",
    "#            'alg_set-2_class_single_70PCA.csv',\n",
    "#            'alg_set-2_class_single_80PCA.csv',\n",
    "#            'alg_set-2_class_single_90PCA.csv',\n",
    "#            'alg_set-2_class_single_95PCA.csv',\n",
    "#            'alg_set-2_class_single_99PCA.csv',]\n",
    "\n",
    "# list_class = [ 'alg_set-1_class_single_noPCA.csv',\n",
    "#            'alg_set-1_class_single_60PCA.csv',\n",
    "#            'alg_set-1_class_single_70PCA.csv',\n",
    "#            'alg_set-1_class_single_80PCA.csv',\n",
    "#            'alg_set-1_class_single_90PCA.csv',\n",
    "#            'alg_set-1_class_single_95PCA.csv',\n",
    "#            'alg_set-1_class_single_99PCA.csv',]\n",
    "\n",
    "# for i, out_df in enumerate(list_class):\n",
    "\n",
    "#     fn = test_dir / list_perf[i]\n",
    "#     fn = test_dir / list_class[i]\n",
    "#     out_df.to_csv(fn)\n",
    "#     print('Saved to {}'.format(fn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6ebcd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc5bc15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dab097",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d206fb93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf7d2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## prepare training & validation set - single grid \n",
    "\n",
    "# subsample \n",
    "df_train_1 = df_train_val[ df_train_val[target_col[0]] == 1]\n",
    "_df_train_0 = df_train_val[ df_train_val[target_col[0]] == 0]\n",
    "ix_df_train_0 = []\n",
    "\n",
    "## sample for each 0 sample \n",
    "for i, idx in enumerate(_df_train_0['tag'].unique()):\n",
    "    sub_df_train_0 = _df_train_0[_df_train_0['tag']==idx]\n",
    "    ## get sample\n",
    "    subsample = sub_df_train_0.sample(n=1).index.values[0]\n",
    "    ix_df_train_0.append(subsample)\n",
    "df_train_0 = df_train_val.loc[ix_df_train_0]\n",
    "df_train = df_train_1.append(df_train_0)\n",
    "\n",
    "# add a number of samples not in buffer as well \n",
    "df_not_in_buffer = df_train_val[df_train_val['in_buffer']!=1]\n",
    "n_sample = df_not_in_buffer['tag'].nunique()\n",
    "\n",
    "df_train = df_train.append( df_not_in_buffer.sample(n=n_sample) ).copy()\n",
    "\n",
    "print('Division of target values before subsampling: ')\n",
    "print( df_train_val['target'].value_counts())\n",
    "print('\\nand after subsampling: ')\n",
    "print(df_train['target'].value_counts())\n",
    "\n",
    "X_train = df_train[feature_columns]\n",
    "y_train = df_train[target_col[0]]\n",
    "y_train_range = df_train['range_target']\n",
    "\n",
    "# normalize \n",
    "min_samples = len(X_train) \n",
    "nm = QuantileTransformer( output_distribution='normal',\n",
    "                        n_quantiles = int(min(1000, min_samples)))\n",
    "nm.fit(X_train)\n",
    "X_train = nm.transform(X_train)\n",
    "\n",
    "# scale \n",
    "sc = MinMaxScaler([0,1])\n",
    "sc.fit(X_train)\n",
    "X_train = sc.transform(X_train)\n",
    "\n",
    "# pca \n",
    "print(f'\\nnumber of features before PCA = {X_train.shape[1]}')\n",
    "pca = PCA(n_components=0.9)\n",
    "pca.fit(X_train)\n",
    "X_train = pca.transform(X_train)\n",
    "print(f'number of features after PCA = {X_train.shape[1]} with 90% variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e796437f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot PC_i vs PC_j\n",
    "pc_i = 0\n",
    "pc_j = 1\n",
    "\n",
    "ix_0 = np.where(y_train==0)[0]\n",
    "ix_1 = np.where(y_train==1)[0]\n",
    "\n",
    "plt.scatter( X_train[ix_0,pc_i], X_train[ix_0,pc_j], color = 'b', label='0' ) \n",
    "plt.scatter( X_train[ix_1,pc_i], X_train[ix_1,pc_j], color = 'r', label='1'  ) \n",
    "\n",
    "plt.xlabel('PC{}'.format(pc_i+1));\n",
    "plt.ylabel('PC{}'.format(pc_j+1));\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3205dfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## apply same transformations and splits on test set \n",
    "X_test = df_test[feature_columns]\n",
    "y_test = df_test[target_col[0]]\n",
    "y_test_range = df_test['range_target'] \n",
    "\n",
    "## normalize \n",
    "X_test = nm.transform(X_test)\n",
    "\n",
    "## scale \n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "## pca \n",
    "X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7235660",
   "metadata": {},
   "outputs": [],
   "source": [
    "## train single pixel classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a75e25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81e7044",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODEL 1 - target in buffer, yes or no?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae60a4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96617eb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2781abf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ef6c9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0035d231",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODEL 2 - if target in buffer, where? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb78ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733ed039",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6233d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f04d04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb384cc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2141ec7d",
   "metadata": {},
   "source": [
    "After finding best settings, use train_val set to train an algorithm, evaluate the final performance with the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cbfc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff71ee1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8c2af6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
