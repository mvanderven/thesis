{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3cd6b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from pathlib import Path\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import xarray as xr\n",
    "import hydroeval as he \n",
    "import seaborn as sns \n",
    "from tqdm import tqdm \n",
    "import warnings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1760f988",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO DO  \n",
    "## - ensemble voting\n",
    "## - total system setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fc73e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SETTINGS \n",
    "\n",
    "case_max = False\n",
    "\n",
    "## whole / large buffer:\n",
    "if case_max:\n",
    "\n",
    "    use_set_1=True\n",
    "    omit_coords = True \n",
    "    set_buffer_size = 6\n",
    "\n",
    "    \n",
    "    similarity_method = 'diff' \n",
    "    grid_format = True \n",
    "    do_pca = True \n",
    "    set_pca = 0.7 \n",
    "    \n",
    "    select_alg = 'SVM-2'\n",
    "\n",
    "## check if center cell is match\n",
    "else:\n",
    "    use_set_1 = True\n",
    " \n",
    "    omit_coords = True \n",
    "    set_buffer_size = 0\n",
    "    \n",
    "    grid_format = True\n",
    "    similarity_method = 'diff' \n",
    "    do_pca = False \n",
    "    \n",
    "    select_alg = 'RF-2'\n",
    "\n",
    "# use_set_1 = True \n",
    "# set_buffer_size = 6\n",
    "# omit_coords = True\n",
    "# similarity_method = 'diff'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bccdba",
   "metadata": {},
   "source": [
    "Load dataset with features. On each row, a different simulation of observation sample can be found, in each column feature values or location metadata is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05fcc648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nm-all</th>\n",
       "      <th>Ns-all</th>\n",
       "      <th>N-gof-all</th>\n",
       "      <th>Lm-all</th>\n",
       "      <th>Ls-all</th>\n",
       "      <th>L-gof-all</th>\n",
       "      <th>Gu-all</th>\n",
       "      <th>Ga-all</th>\n",
       "      <th>Gev-gof-all</th>\n",
       "      <th>Gk-all</th>\n",
       "      <th>...</th>\n",
       "      <th>flv-seasonal_1</th>\n",
       "      <th>hf-f-all</th>\n",
       "      <th>hf-f-seasonal_1</th>\n",
       "      <th>hf-t-all</th>\n",
       "      <th>lf-f-all</th>\n",
       "      <th>lf-f-seasonal_1</th>\n",
       "      <th>lf-t-seasonal_1</th>\n",
       "      <th>pks-all</th>\n",
       "      <th>tag</th>\n",
       "      <th>dem_flag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6119010_11</th>\n",
       "      <td>102.035362</td>\n",
       "      <td>106.090645</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.221500</td>\n",
       "      <td>0.863208</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.306767</td>\n",
       "      <td>0.012089</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.925012</td>\n",
       "      <td>...</td>\n",
       "      <td>164.721008</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.954545</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>4.481481</td>\n",
       "      <td>666.803894</td>\n",
       "      <td>6119010</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6119010_12</th>\n",
       "      <td>100.917046</td>\n",
       "      <td>105.168777</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.209119</td>\n",
       "      <td>0.864435</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.603184</td>\n",
       "      <td>0.012195</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.920779</td>\n",
       "      <td>...</td>\n",
       "      <td>163.700439</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.090909</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>4.518519</td>\n",
       "      <td>654.762268</td>\n",
       "      <td>6119010</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6119010_13</th>\n",
       "      <td>0.544169</td>\n",
       "      <td>0.542255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.002645</td>\n",
       "      <td>0.867551</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.300217</td>\n",
       "      <td>2.365214</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.007072</td>\n",
       "      <td>...</td>\n",
       "      <td>266.693390</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.727273</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>3.131104</td>\n",
       "      <td>6119010</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6119010_14</th>\n",
       "      <td>162.321777</td>\n",
       "      <td>138.910507</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.760090</td>\n",
       "      <td>0.835850</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.828016</td>\n",
       "      <td>0.009233</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.365474</td>\n",
       "      <td>...</td>\n",
       "      <td>317.679565</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>7.227273</td>\n",
       "      <td>723.082581</td>\n",
       "      <td>6119010</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6119010_15</th>\n",
       "      <td>17.163115</td>\n",
       "      <td>16.641987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.442999</td>\n",
       "      <td>0.888787</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.676133</td>\n",
       "      <td>0.077067</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.063609</td>\n",
       "      <td>...</td>\n",
       "      <td>248.398575</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>6.083333</td>\n",
       "      <td>98.092957</td>\n",
       "      <td>6119010</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6983350_96</th>\n",
       "      <td>522.018127</td>\n",
       "      <td>325.349060</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.080506</td>\n",
       "      <td>0.592225</td>\n",
       "      <td>0.0</td>\n",
       "      <td>375.648448</td>\n",
       "      <td>0.003942</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.574376</td>\n",
       "      <td>...</td>\n",
       "      <td>60.206692</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1890.435181</td>\n",
       "      <td>6983350</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6983350_97</th>\n",
       "      <td>1.769354</td>\n",
       "      <td>1.644365</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.271318</td>\n",
       "      <td>0.743112</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.029579</td>\n",
       "      <td>0.779967</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.157798</td>\n",
       "      <td>...</td>\n",
       "      <td>73.878761</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>6.181818</td>\n",
       "      <td>10.525513</td>\n",
       "      <td>6983350</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6983350_98</th>\n",
       "      <td>6.406629</td>\n",
       "      <td>5.884712</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.530997</td>\n",
       "      <td>0.797030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.759185</td>\n",
       "      <td>0.217946</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.185247</td>\n",
       "      <td>...</td>\n",
       "      <td>92.737022</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>33.151855</td>\n",
       "      <td>6983350</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6983350_99</th>\n",
       "      <td>0.280780</td>\n",
       "      <td>0.267437</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.570457</td>\n",
       "      <td>0.738658</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.160464</td>\n",
       "      <td>4.795705</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.102273</td>\n",
       "      <td>...</td>\n",
       "      <td>92.291611</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.444444</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>4.818182</td>\n",
       "      <td>1.593628</td>\n",
       "      <td>6983350</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6983350_gauge</th>\n",
       "      <td>413.776970</td>\n",
       "      <td>228.897089</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.840650</td>\n",
       "      <td>0.653553</td>\n",
       "      <td>0.0</td>\n",
       "      <td>310.799590</td>\n",
       "      <td>0.005603</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.267775</td>\n",
       "      <td>...</td>\n",
       "      <td>30.588420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>992.500000</td>\n",
       "      <td>6983350</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47234 rows × 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Nm-all      Ns-all  N-gof-all    Lm-all    Ls-all  \\\n",
       "ID                                                                     \n",
       "6119010_11     102.035362  106.090645        0.0  4.221500  0.863208   \n",
       "6119010_12     100.917046  105.168777        0.0  4.209119  0.864435   \n",
       "6119010_13       0.544169    0.542255        0.0 -1.002645  0.867551   \n",
       "6119010_14     162.321777  138.910507        0.0  4.760090  0.835850   \n",
       "6119010_15      17.163115   16.641987        0.0  2.442999  0.888787   \n",
       "...                   ...         ...        ...       ...       ...   \n",
       "6983350_96     522.018127  325.349060        0.0  6.080506  0.592225   \n",
       "6983350_97       1.769354    1.644365        0.0  0.271318  0.743112   \n",
       "6983350_98       6.406629    5.884712        0.0  1.530997  0.797030   \n",
       "6983350_99       0.280780    0.267437        0.0 -1.570457  0.738658   \n",
       "6983350_gauge  413.776970  228.897089        0.0  5.840650  0.653553   \n",
       "\n",
       "               L-gof-all      Gu-all    Ga-all  Gev-gof-all    Gk-all  ...  \\\n",
       "ID                                                                     ...   \n",
       "6119010_11           0.0   54.306767  0.012089          0.0  0.925012  ...   \n",
       "6119010_12           0.0   53.603184  0.012195          0.0  0.920779  ...   \n",
       "6119010_13           0.0    0.300217  2.365214          0.0  1.007072  ...   \n",
       "6119010_14           0.0   99.828016  0.009233          0.0  1.365474  ...   \n",
       "6119010_15           0.0    9.676133  0.077067          0.0  1.063609  ...   \n",
       "...                  ...         ...       ...          ...       ...  ...   \n",
       "6983350_96           0.0  375.648448  0.003942          0.0  2.574376  ...   \n",
       "6983350_97           0.0    1.029579  0.779967          0.0  1.157798  ...   \n",
       "6983350_98           0.0    3.759185  0.217946          0.0  1.185247  ...   \n",
       "6983350_99           0.0    0.160464  4.795705          0.0  1.102273  ...   \n",
       "6983350_gauge        0.0  310.799590  0.005603          0.0  3.267775  ...   \n",
       "\n",
       "               flv-seasonal_1  hf-f-all  hf-f-seasonal_1  hf-t-all  lf-f-all  \\\n",
       "ID                                                                             \n",
       "6119010_11         164.721008  0.590909              0.0  2.954545  0.363636   \n",
       "6119010_12         163.700439  0.590909              0.0  3.090909  0.363636   \n",
       "6119010_13         266.693390  0.409091              0.0  1.727273  0.409091   \n",
       "6119010_14         317.679565  0.045455              0.0  0.136364  1.000000   \n",
       "6119010_15         248.398575  0.136364              0.0  0.363636  0.454545   \n",
       "...                       ...       ...              ...       ...       ...   \n",
       "6983350_96          60.206692  0.000000              0.0  0.000000  0.111111   \n",
       "6983350_97          73.878761  0.333333              0.0  1.000000  0.111111   \n",
       "6983350_98          92.737022  0.444444              0.0  1.000000  0.444444   \n",
       "6983350_99          92.291611  0.555556              0.0  1.444444  0.222222   \n",
       "6983350_gauge       30.588420  0.000000              0.0  0.000000  0.222222   \n",
       "\n",
       "               lf-f-seasonal_1  lf-t-seasonal_1      pks-all      tag  \\\n",
       "ID                                                                      \n",
       "6119010_11            0.636364         4.481481   666.803894  6119010   \n",
       "6119010_12            0.636364         4.518519   654.762268  6119010   \n",
       "6119010_13            0.636364         6.000000     3.131104  6119010   \n",
       "6119010_14            0.545455         7.227273   723.082581  6119010   \n",
       "6119010_15            0.500000         6.083333    98.092957  6119010   \n",
       "...                        ...              ...          ...      ...   \n",
       "6983350_96            0.000000         0.000000  1890.435181  6983350   \n",
       "6983350_97            0.555556         6.181818    10.525513  6983350   \n",
       "6983350_98            0.444444         4.800000    33.151855  6983350   \n",
       "6983350_99            0.555556         4.818182     1.593628  6983350   \n",
       "6983350_gauge         0.000000         0.000000   992.500000  6983350   \n",
       "\n",
       "               dem_flag  \n",
       "ID                       \n",
       "6119010_11            0  \n",
       "6119010_12            0  \n",
       "6119010_13            0  \n",
       "6119010_14            0  \n",
       "6119010_15            0  \n",
       "...                 ...  \n",
       "6983350_96            0  \n",
       "6983350_97            0  \n",
       "6983350_98            0  \n",
       "6983350_99            0  \n",
       "6983350_gauge         0  \n",
       "\n",
       "[47234 rows x 84 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signature_dir = Path(r\"C:\\Users\\mvand\\Documents\\Master EE\\Year 4\\Thesis\\data\\training_data_S1\\signatures_nc_V1_output\") \n",
    "\n",
    "if use_set_1:\n",
    "    fn_signatures = signature_dir / \"S1_merged_signatures_v2-cleanup-1.csv\"\n",
    "else:\n",
    "    fn_signatures = signature_dir / \"S1_merged_signatures_v2-cleanup-2.csv\" \n",
    "\n",
    "df_signatures = pd.read_csv(fn_signatures, index_col = 0)\n",
    "\n",
    "df_signatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038fca7d",
   "metadata": {},
   "source": [
    "Slightly expand the searching area by labeling the up- and downstream neigbhours of the target pixel as 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ca33ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## expand searching area by max. 2 pixels \n",
    "\n",
    "def expand_labels(df, id_col, target_col, critical_variable, \n",
    "                  p_diff = 5., n_max = 3, pixel_size = 5000., option = 1,\n",
    "                  x_coord = 'x', y_coord = 'y'):\n",
    "    \n",
    "    out_col = 'range_{}'.format(target_col)\n",
    "    df[out_col] = 0.\n",
    "    \n",
    "    for bix in df[id_col].unique():\n",
    "        \n",
    "        ## select buffer and target value \n",
    "        _buffer = df[ df[id_col] == bix ] \n",
    "        \n",
    "        ## discard gauge values \n",
    "        buffer_ix = [ix for ix in _buffer.index if not 'gauge' in ix]\n",
    "        _sim = _buffer.loc[buffer_ix] \n",
    "        \n",
    "        ## select target row \n",
    "        target = _sim[ _sim[target_col] == 1.]\n",
    "        \n",
    "        ## if target value found\n",
    "        if len(target) > 0:\n",
    "            \n",
    "            ## search criterium \n",
    "            target_var = target[critical_variable].values[0] \n",
    "            \n",
    "            ## look to adjacent pixels only \n",
    "            target_X_coord = target[x_coord].values[0]\n",
    "            target_Y_coord = target[y_coord].values[0] \n",
    "            x_filter = (_sim[x_coord] >= target_X_coord - 2*(pixel_size+1.) ) & (_sim[x_coord] <= target_X_coord + 2*(pixel_size+1.))\n",
    "            y_filter = (_sim[y_coord] >= target_Y_coord - 2*(pixel_size+1.) ) & (_sim[y_coord] <= target_Y_coord + 2*(pixel_size+1.))\n",
    "            _sim = _sim[ x_filter & y_filter]\n",
    "\n",
    "            ## calculate absolute percentual difference \n",
    "            d_var = (( ((_sim[critical_variable] - target_var)**2)**0.5 / target_var)*100 ).sort_values()\n",
    "            \n",
    "            ## OPTION 1 - select top three (including original target pixel) based on difference \n",
    "            if option == 1:\n",
    "                expand_targets = d_var.head(n_max).index \n",
    "\n",
    "            ## OPTION 2 - also include a percentual limit (1% - 5%) \n",
    "            if option == 2:\n",
    "                expand_targets = d_var[ d_var <= p_diff].index\n",
    "                if len(expand_targets) > n_max:\n",
    "                    expand_targets = expand_targets[:int(n_max-1)]\n",
    "\n",
    "            ## relabel \n",
    "            if target.index[0] in expand_targets:\n",
    "                df.loc[expand_targets, out_col] = 1 \n",
    "            else:\n",
    "                print('check ', bix)            \n",
    "\n",
    "    return df, out_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be3b8c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_signatures, range_col = expand_labels(df_signatures, 'tag', 'target', 'Nm-all', option = 2, p_diff = 1.) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d799edab",
   "metadata": {},
   "source": [
    "Set the label categories - identify feature columns and target labels. Used for later processing of similarity and splitting X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9167ee0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## column descriptors \n",
    "columns = df_signatures.columns.values \n",
    "\n",
    "## target value to predict \n",
    "target_col = ['target'] \n",
    "\n",
    "## target for continued search \n",
    "target_buffer = ['in_buffer']\n",
    "\n",
    "## should be omitted from dataset - but could be useful for easy selection \n",
    "## dem_flag only 1 for gauge values\n",
    "non_feature_cols = ['n_buffer', 'tag', 'dem_flag', range_col, 'in_buffer'] \n",
    "\n",
    "## coord cols - could be ommitted from dataset, risk for overfitting\n",
    "## and also takes away focus on selection based on timeseries\n",
    "coord_cols = ['x', 'y', 'lat', 'lon']\n",
    "\n",
    "## features that do not have to be transformed by subtraction \n",
    "## and cross-correlation properties, that are already a similarity property \n",
    "## as they are cross-correlation values of simulations and observations\n",
    "non_similarity_cols = [col for col in columns if 'clag' in col]\n",
    "\n",
    "feature_cols = [col for col in columns if (col not in non_feature_cols) & (not 'target' in col)]\n",
    "\n",
    "## if coord_cols ommitted:\n",
    "if omit_coords:\n",
    "    feature_cols = [col for col in feature_cols if col not in coord_cols] \n",
    "\n",
    "## columns on which to perform similarity calculations:\n",
    "calc_cols = [col for col in feature_cols if col not in non_similarity_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011f1d68",
   "metadata": {},
   "source": [
    "Set the similarity metric used by the algorithm to find a best matching simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ebcecc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate similarity per buffer \n",
    "\n",
    "\n",
    "def calc_similarity(df, buffer_col, calc_cols,\n",
    "                   methods = ['diff']):\n",
    "    \n",
    "    df_out = df.copy() \n",
    "    \n",
    "    buffer_idx = df[buffer_col].unique() \n",
    "    \n",
    "    for ix in buffer_idx:\n",
    "        df_buffer = df[ df[buffer_col] == ix] \n",
    "        \n",
    "        cell_index = [row for row in df_buffer.index if not 'gauge' in row]\n",
    "        gauge_index = [row for row in df_buffer.index if 'gauge' in row] \n",
    "        \n",
    "        if len(gauge_index) > 0:\n",
    "                \n",
    "            if 'diff' in methods:            \n",
    "                for col in calc_cols:\n",
    "                    df_out.loc[cell_index, 'diff_{}'.format(col)] = df_out.loc[cell_index, col] - df_out.loc[gauge_index, col].values\n",
    "\n",
    "            if 'abs' in methods:\n",
    "                for col in calc_cols:\n",
    "                     df_out.loc[cell_index, 'abs_{}'.format(col)] = ((df_out.loc[cell_index, col] - df_out.loc[gauge_index, col].values)**2)**0.5 \n",
    "\n",
    "            if 'ratio-1' in methods:\n",
    "                for col in calc_cols:\n",
    "                    df_out.loc[cell_index, 'rat1_{}'.format(col)] = 1 - (( df_out.loc[cell_index, col] / (df_out.loc[gauge_index, col].values+1e-6)  )**2)**0.5\n",
    "\n",
    "            if 'ratio-2' in methods:\n",
    "                for col in calc_cols:\n",
    "                    df_out.loc[cell_index, 'rat2_{}'.format(col)] =  ( df_out.loc[cell_index, col] - df_out.loc[gauge_index, col].values ) / (df_out.loc[gauge_index, col].values+1e-6)\n",
    "            \n",
    "            if 'double' in methods: \n",
    "                for col in calc_cols:\n",
    "                    df_out.loc[cell_index, 'sim_{}'.format(col)] = df_out.loc[cell_index, col]\n",
    "                    df_out.loc[cell_index, 'obs_{}'.format(col)] = df_out.loc[gauge_index, col].values[0]\n",
    "        \n",
    "        else:\n",
    "            print('No gauge? :', ix)\n",
    "            df_out = df_out.drop(index=cell_index)\n",
    "                    \n",
    "    ## drop calc_cols\n",
    "    df_out = df_out.drop(columns=calc_cols)\n",
    "    \n",
    "    gauge_idx = [row for row in df_out.index if 'gauge' in row] \n",
    "    return df_out.drop(index=gauge_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2a49006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clag-0-all</th>\n",
       "      <th>clag-1-all</th>\n",
       "      <th>clag-0-seasonal_1</th>\n",
       "      <th>clag-1-seasonal_1</th>\n",
       "      <th>n_buffer</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>target</th>\n",
       "      <th>...</th>\n",
       "      <th>diff_fhv-seasonal_1</th>\n",
       "      <th>diff_flv-all</th>\n",
       "      <th>diff_flv-seasonal_1</th>\n",
       "      <th>diff_hf-f-all</th>\n",
       "      <th>diff_hf-f-seasonal_1</th>\n",
       "      <th>diff_hf-t-all</th>\n",
       "      <th>diff_lf-f-all</th>\n",
       "      <th>diff_lf-f-seasonal_1</th>\n",
       "      <th>diff_lf-t-seasonal_1</th>\n",
       "      <th>diff_pks-all</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6119010_11</th>\n",
       "      <td>0.635464</td>\n",
       "      <td>0.544323</td>\n",
       "      <td>0.672078</td>\n",
       "      <td>0.537085</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3422500.0</td>\n",
       "      <td>2352500.0</td>\n",
       "      <td>43.678226</td>\n",
       "      <td>-1.161527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8108.892578</td>\n",
       "      <td>-1091.800362</td>\n",
       "      <td>47.776888</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>-0.045455</td>\n",
       "      <td>2.818182</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>4.481481</td>\n",
       "      <td>328.303894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6119010_12</th>\n",
       "      <td>0.635464</td>\n",
       "      <td>0.544067</td>\n",
       "      <td>0.671657</td>\n",
       "      <td>0.536465</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3422500.0</td>\n",
       "      <td>2347500.0</td>\n",
       "      <td>43.633678</td>\n",
       "      <td>-1.152881</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7921.990234</td>\n",
       "      <td>-1094.935036</td>\n",
       "      <td>46.756319</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>-0.045455</td>\n",
       "      <td>2.954545</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>4.518519</td>\n",
       "      <td>316.262268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6119010_13</th>\n",
       "      <td>0.589989</td>\n",
       "      <td>0.522277</td>\n",
       "      <td>0.698611</td>\n",
       "      <td>0.582520</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3422500.0</td>\n",
       "      <td>2342500.0</td>\n",
       "      <td>43.589130</td>\n",
       "      <td>-1.144252</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-14805.981384</td>\n",
       "      <td>-713.167183</td>\n",
       "      <td>149.749269</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>-0.045455</td>\n",
       "      <td>1.590909</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>-335.368896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6119010_14</th>\n",
       "      <td>0.709440</td>\n",
       "      <td>0.570481</td>\n",
       "      <td>0.822756</td>\n",
       "      <td>0.639700</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3422500.0</td>\n",
       "      <td>2337500.0</td>\n",
       "      <td>43.544575</td>\n",
       "      <td>-1.135641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>11030.318359</td>\n",
       "      <td>-133.411201</td>\n",
       "      <td>200.735445</td>\n",
       "      <td>-0.090909</td>\n",
       "      <td>-0.045455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>7.227273</td>\n",
       "      <td>384.582581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6119010_15</th>\n",
       "      <td>0.612656</td>\n",
       "      <td>0.531497</td>\n",
       "      <td>0.700380</td>\n",
       "      <td>0.568576</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3422500.0</td>\n",
       "      <td>2332500.0</td>\n",
       "      <td>43.500019</td>\n",
       "      <td>-1.127047</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-11723.430176</td>\n",
       "      <td>-538.359810</td>\n",
       "      <td>131.454454</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.045455</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>6.083333</td>\n",
       "      <td>-240.407043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6983350_95</th>\n",
       "      <td>0.203398</td>\n",
       "      <td>0.227645</td>\n",
       "      <td>0.419789</td>\n",
       "      <td>0.451424</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6502500.0</td>\n",
       "      <td>2872500.0</td>\n",
       "      <td>45.141037</td>\n",
       "      <td>38.478489</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-14710.986877</td>\n",
       "      <td>-476.378696</td>\n",
       "      <td>48.027616</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>1.555556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>-990.893250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6983350_96</th>\n",
       "      <td>0.574453</td>\n",
       "      <td>0.580212</td>\n",
       "      <td>0.581299</td>\n",
       "      <td>0.579456</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6502500.0</td>\n",
       "      <td>2867500.0</td>\n",
       "      <td>45.099716</td>\n",
       "      <td>38.455753</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2752.287109</td>\n",
       "      <td>-400.673678</td>\n",
       "      <td>29.618272</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>897.935181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6983350_97</th>\n",
       "      <td>0.234536</td>\n",
       "      <td>0.260122</td>\n",
       "      <td>0.485846</td>\n",
       "      <td>0.515676</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6502500.0</td>\n",
       "      <td>2862500.0</td>\n",
       "      <td>45.058388</td>\n",
       "      <td>38.433064</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-14548.157715</td>\n",
       "      <td>-373.953403</td>\n",
       "      <td>43.290342</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>6.181818</td>\n",
       "      <td>-981.974487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6983350_98</th>\n",
       "      <td>0.231623</td>\n",
       "      <td>0.258969</td>\n",
       "      <td>0.471694</td>\n",
       "      <td>0.502134</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6502500.0</td>\n",
       "      <td>2857500.0</td>\n",
       "      <td>45.017052</td>\n",
       "      <td>38.410419</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-14094.062805</td>\n",
       "      <td>-313.008975</td>\n",
       "      <td>62.148603</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>-959.348145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6983350_99</th>\n",
       "      <td>0.208036</td>\n",
       "      <td>0.233330</td>\n",
       "      <td>0.426323</td>\n",
       "      <td>0.458662</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6502500.0</td>\n",
       "      <td>2852500.0</td>\n",
       "      <td>44.975704</td>\n",
       "      <td>38.387817</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-14710.501587</td>\n",
       "      <td>-425.817844</td>\n",
       "      <td>61.703191</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.444444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>4.818182</td>\n",
       "      <td>-990.906372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46639 rows × 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            clag-0-all  clag-1-all  clag-0-seasonal_1  clag-1-seasonal_1  \\\n",
       "ID                                                                         \n",
       "6119010_11    0.635464    0.544323           0.672078           0.537085   \n",
       "6119010_12    0.635464    0.544067           0.671657           0.536465   \n",
       "6119010_13    0.589989    0.522277           0.698611           0.582520   \n",
       "6119010_14    0.709440    0.570481           0.822756           0.639700   \n",
       "6119010_15    0.612656    0.531497           0.700380           0.568576   \n",
       "...                ...         ...                ...                ...   \n",
       "6983350_95    0.203398    0.227645           0.419789           0.451424   \n",
       "6983350_96    0.574453    0.580212           0.581299           0.579456   \n",
       "6983350_97    0.234536    0.260122           0.485846           0.515676   \n",
       "6983350_98    0.231623    0.258969           0.471694           0.502134   \n",
       "6983350_99    0.208036    0.233330           0.426323           0.458662   \n",
       "\n",
       "            n_buffer          x          y        lat        lon  target  ...  \\\n",
       "ID                                                                        ...   \n",
       "6119010_11       4.0  3422500.0  2352500.0  43.678226  -1.161527     0.0  ...   \n",
       "6119010_12       4.0  3422500.0  2347500.0  43.633678  -1.152881     0.0  ...   \n",
       "6119010_13       4.0  3422500.0  2342500.0  43.589130  -1.144252     0.0  ...   \n",
       "6119010_14       4.0  3422500.0  2337500.0  43.544575  -1.135641     0.0  ...   \n",
       "6119010_15       4.0  3422500.0  2332500.0  43.500019  -1.127047     0.0  ...   \n",
       "...              ...        ...        ...        ...        ...     ...  ...   \n",
       "6983350_95       4.0  6502500.0  2872500.0  45.141037  38.478489     0.0  ...   \n",
       "6983350_96       4.0  6502500.0  2867500.0  45.099716  38.455753     0.0  ...   \n",
       "6983350_97       4.0  6502500.0  2862500.0  45.058388  38.433064     0.0  ...   \n",
       "6983350_98       4.0  6502500.0  2857500.0  45.017052  38.410419     0.0  ...   \n",
       "6983350_99       4.0  6502500.0  2852500.0  44.975704  38.387817     0.0  ...   \n",
       "\n",
       "            diff_fhv-seasonal_1  diff_flv-all  diff_flv-seasonal_1  \\\n",
       "ID                                                                   \n",
       "6119010_11          8108.892578  -1091.800362            47.776888   \n",
       "6119010_12          7921.990234  -1094.935036            46.756319   \n",
       "6119010_13        -14805.981384   -713.167183           149.749269   \n",
       "6119010_14         11030.318359   -133.411201           200.735445   \n",
       "6119010_15        -11723.430176   -538.359810           131.454454   \n",
       "...                         ...           ...                  ...   \n",
       "6983350_95        -14710.986877   -476.378696            48.027616   \n",
       "6983350_96         -2752.287109   -400.673678            29.618272   \n",
       "6983350_97        -14548.157715   -373.953403            43.290342   \n",
       "6983350_98        -14094.062805   -313.008975            62.148603   \n",
       "6983350_99        -14710.501587   -425.817844            61.703191   \n",
       "\n",
       "            diff_hf-f-all  diff_hf-f-seasonal_1  diff_hf-t-all  diff_lf-f-all  \\\n",
       "ID                                                                              \n",
       "6119010_11       0.454545             -0.045455       2.818182       0.318182   \n",
       "6119010_12       0.454545             -0.045455       2.954545       0.318182   \n",
       "6119010_13       0.272727             -0.045455       1.590909       0.363636   \n",
       "6119010_14      -0.090909             -0.045455       0.000000       0.954545   \n",
       "6119010_15       0.000000             -0.045455       0.227273       0.409091   \n",
       "...                   ...                   ...            ...            ...   \n",
       "6983350_95       0.777778              0.111111       1.555556       0.000000   \n",
       "6983350_96       0.000000              0.000000       0.000000      -0.111111   \n",
       "6983350_97       0.333333              0.000000       1.000000      -0.111111   \n",
       "6983350_98       0.444444              0.000000       1.000000       0.222222   \n",
       "6983350_99       0.555556              0.000000       1.444444       0.000000   \n",
       "\n",
       "            diff_lf-f-seasonal_1  diff_lf-t-seasonal_1  diff_pks-all  \n",
       "ID                                                                    \n",
       "6119010_11              0.636364              4.481481    328.303894  \n",
       "6119010_12              0.636364              4.518519    316.262268  \n",
       "6119010_13              0.636364              6.000000   -335.368896  \n",
       "6119010_14              0.545455              7.227273    384.582581  \n",
       "6119010_15              0.500000              6.083333   -240.407043  \n",
       "...                          ...                   ...           ...  \n",
       "6983350_95              0.555556              6.000000   -990.893250  \n",
       "6983350_96              0.000000              0.000000    897.935181  \n",
       "6983350_97              0.555556              6.181818   -981.974487  \n",
       "6983350_98              0.444444              4.800000   -959.348145  \n",
       "6983350_99              0.555556              4.818182   -990.906372  \n",
       "\n",
       "[46639 rows x 85 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_similarity = calc_similarity(df_signatures, 'tag', calc_cols, methods=[similarity_method])\n",
    "\n",
    "df_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91941402",
   "metadata": {},
   "source": [
    "Import required scikit-learn functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f64aa99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, QuantileTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# algorithms \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.svm import SVC \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# evaluation \n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, mean_squared_error, balanced_accuracy_score "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9591ef9",
   "metadata": {},
   "source": [
    "Split a training & evaluation and test set. First investigate location of targets. Split datasets based on those distributions for equal division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b67686a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 gauges / buffers fall outside the buffer zone of 4:\n",
      "         d_X_cell    d_Y_cell\n",
      "count   23.000000   23.000000\n",
      "mean   -20.826087   -6.086957\n",
      "std     55.257786   54.266106\n",
      "min   -214.000000 -250.000000\n",
      "25%    -11.500000   -1.000000\n",
      "50%     -4.000000    6.000000\n",
      "75%      2.000000   10.500000\n",
      "max     13.000000   30.000000\n",
      "\n",
      "Analyse size of buffer or searching range with number of hits:\n",
      "In a search range of 0 cells, 244 matches are found\n",
      "In a search range of 1 cells, 284 matches are found\n",
      "In a search range of 2 cells, 32 matches are found\n",
      "In a search range of 3 cells, 9 matches are found\n",
      "In a search range of 4 cells, 3 matches are found\n",
      "\n",
      "Show result of split:\n",
      "Train val set:\n",
      "n_buffer\n",
      "0.0    207.0\n",
      "Name: target, dtype: float64\n",
      "\n",
      "Test set\n",
      "n_buffer\n",
      "0.0    37.0\n",
      "Name: target, dtype: float64\n",
      "\n",
      "503 samples in total train val set\n",
      "92 samples in total train val set\n"
     ]
    }
   ],
   "source": [
    "## first split data based on being inside or outside the buffer zone (=4)\n",
    "df_outside_buffer = df_similarity[ df_similarity['target'] == -1].copy() \n",
    "gauge_outside = df_outside_buffer['tag'].unique().astype(int)\n",
    "\n",
    "fn_gauge = Path(r\"C:\\Users\\mvand\\Documents\\Master EE\\Year 4\\Thesis\\data\\training_data_S1\")  / \"V1_grdc_efas_selection-cartesius-snapped-1.csv\" \n",
    "meta_gauge = pd.read_csv(fn_gauge, index_col = 0)\n",
    "\n",
    "## analyse large shifts \n",
    "df_meta_gauge = meta_gauge.loc[gauge_outside]\n",
    "print('{} gauges / buffers fall outside the buffer zone of 4:'.format(len(gauge_outside)))\n",
    "print(df_meta_gauge[['d_X_cell', 'd_Y_cell']].describe())\n",
    "# print(df_meta_gauge[['d_X_cell', 'd_Y_cell']])\n",
    "\n",
    "## copy data to new dataframe \n",
    "df_predict_buffer = df_similarity.copy() #.drop(index=df_outside_buffer.index) \n",
    "## set all outside buffer vals to 0 \n",
    "df_predict_buffer.loc[df_outside_buffer.index, 'target'] = 0. \n",
    "## add target column for in-buffer prediction \n",
    "df_predict_buffer['in_buffer'] = 1 \n",
    "df_predict_buffer.loc[ df_predict_buffer['tag'].isin(gauge_outside)  , 'in_buffer'] = 0. \n",
    "\n",
    "print('\\nAnalyse size of buffer or searching range with number of hits:')\n",
    "for buffer_size in np.sort(df_predict_buffer['n_buffer'].unique()):\n",
    "    n_hits =df_predict_buffer[ (df_predict_buffer['n_buffer'] == buffer_size) & (df_predict_buffer['target']==1.) ]['target'].sum()\n",
    "    print('In a search range of {:.0f} cells, {:.0f} matches are found'.format(buffer_size, n_hits))\n",
    "\n",
    "\n",
    "## get buffers based on location of match \n",
    "buffer_0 = df_predict_buffer[ (df_predict_buffer['n_buffer'] == 0) & (df_predict_buffer['target']==1) ]['tag'].unique()\n",
    "buffer_1 = df_predict_buffer[ (df_predict_buffer['n_buffer'] == 1) & (df_predict_buffer['target']==1) ]['tag'].unique()\n",
    "buffer_2 = df_predict_buffer[ (df_predict_buffer['n_buffer'] == 2) & (df_predict_buffer['target']==1) ]['tag'].unique()\n",
    "buffer_3 = df_predict_buffer[ (df_predict_buffer['n_buffer'] == 3) & (df_predict_buffer['target']==1) ]['tag'].unique()\n",
    "buffer_4 = df_predict_buffer[ (df_predict_buffer['n_buffer'] == 4) & (df_predict_buffer['target']==1) ]['tag'].unique()\n",
    "\n",
    "## split with same ratio \n",
    "train_val_0, test_0 = train_test_split(buffer_0, test_size=0.15, random_state=21)\n",
    "train_val_1, test_1 = train_test_split(buffer_1, test_size=0.15, random_state=21)\n",
    "train_val_2, test_2 = train_test_split(buffer_2, test_size=0.15, random_state=21)\n",
    "train_val_3, test_3 = train_test_split(buffer_3, test_size=0.15, random_state=21)\n",
    "train_val_4, test_4 = train_test_split(buffer_4, test_size=0.15, random_state=21)\n",
    "train_val_out, test_out = train_test_split(gauge_outside, test_size=0.15, random_state=21)\n",
    "\n",
    "## combine all sets \n",
    "train_val_sets = (train_val_0, train_val_1, train_val_2, train_val_3, train_val_4, train_val_out)\n",
    "test_sets = (test_0, test_1, test_2, test_3, test_4, test_out)\n",
    "\n",
    "id_train_val = np.concatenate( train_val_sets )\n",
    "id_test = np.concatenate( test_sets )\n",
    "\n",
    "## split data \n",
    "df_train_val = df_predict_buffer[ df_predict_buffer['tag'].isin(id_train_val)].copy()\n",
    "df_test = df_predict_buffer[ df_predict_buffer['tag'].isin(id_test) ].copy()\n",
    "\n",
    "## VARY SEARCHING AREA\n",
    "## drop all rows with n_buffer values > set_buffer_size\n",
    "## if match outside buffer, remaining set will be all zeros --> no buffer in current search area \n",
    "df_train_val = df_train_val[ df_train_val['n_buffer'] <= set_buffer_size ].copy()\n",
    "df_test = df_test[ df_test['n_buffer'] <= set_buffer_size ].copy()\n",
    "\n",
    "## update in_buffer values \n",
    "for gauge_id in id_train_val:\n",
    "    idx = df_train_val[ df_train_val['tag']== gauge_id].index \n",
    "    sum_target = df_train_val.loc[idx, 'target'].sum()\n",
    "    if sum_target < 1:\n",
    "        df_train_val.loc[idx, 'in_buffer'] = 0 \n",
    "\n",
    "for gauge_id in id_test:\n",
    "    idx = df_test[ df_test['tag']== gauge_id].index \n",
    "    sum_target = df_test.loc[idx, 'target'].sum()\n",
    "    if sum_target < 1:\n",
    "        df_test.loc[idx, 'in_buffer'] = 0 \n",
    "\n",
    "######         \n",
    "print('\\nShow result of split:')\n",
    "print('Train val set:')\n",
    "print( df_train_val[df_train_val['target']==1].groupby(by='n_buffer')['target'].sum() )\n",
    "print('\\nTest set')\n",
    "print( df_test[df_test['target']==1].groupby(by='n_buffer')['target'].sum() )\n",
    "print(f'\\n{len(id_train_val)} samples in total train val set')\n",
    "print(f'{len(id_test)} samples in total train val set')\n",
    "\n",
    "## list feature columns \n",
    "feature_columns = df_similarity.columns.values \n",
    "\n",
    "## remove non_feature cols \n",
    "if omit_coords:\n",
    "    for col in non_feature_cols+target_col+coord_cols:\n",
    "        col_ix = np.where(feature_columns==col)[0]\n",
    "        feature_columns = np.delete(feature_columns, col_ix)\n",
    "\n",
    "else:\n",
    "    for col in non_feature_cols+target_col:\n",
    "        col_ix = np.where(feature_columns==col)[0]\n",
    "#         feature_columns = np.delete(feature_columns, col_ix)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a79bd1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_ix = np.concatenate((id_train_val, id_test))\n",
    "\n",
    "# subset = meta_gauge.loc[all_ix, ['d_X_cell', 'd_Y_cell']]\n",
    "\n",
    "# subset['buffer_shift'] = 0 \n",
    "# x0 = 0 \n",
    "# y0 = 0 \n",
    "\n",
    "# for ix in subset.index:\n",
    "#     row = subset.loc[ix]\n",
    "    \n",
    "#     x_shift = ((row['d_X_cell'])**2)**0.5 \n",
    "#     y_shift = ((row['d_Y_cell'])**2)**0.5 \n",
    "#     max_shift = np.max([x_shift, y_shift])\n",
    "      \n",
    "#     subset.loc[ix,'buffer_shift'] = max_shift\n",
    "        \n",
    "# analyse_distances = subset['buffer_shift'].value_counts()\n",
    "\n",
    "# x_vals = analyse_distances.index\n",
    "# y_vals = analyse_distances.values \n",
    "\n",
    "\n",
    "# save_dir = Path(r\"C:\\Users\\mvand\\Documents\\Master EE\\Year 4\\Thesis\\meetings\\2021_06_15_EWC_sympo\\media\") \n",
    "# fn_1 = save_dir / 'shift_bins.png'\n",
    "# fn_2 = save_dir / 'shift_bins_zoom.png'\n",
    "\n",
    "# plt.figure(figsize=(12,4))\n",
    "# plt.bar(x_vals, y_vals)\n",
    "# plt.yscale('log')\n",
    "# plt.grid()\n",
    "# plt.xlabel('Shift nearest cell to matched cell', size=14);\n",
    "# plt.ylabel('n observations (log-scaled)', size=14);\n",
    "# plt.xticks(size=12);\n",
    "# plt.yticks(size=12);\n",
    "# plt.savefig(fn_1);\n",
    "\n",
    "# plt.figure(figsize=(8,4))\n",
    "# plt.bar(x_vals, y_vals)\n",
    "# plt.xlim(-1, 5)\n",
    "# plt.grid()\n",
    "# plt.xlabel('Shift nearest cell to matched cell', size=14);\n",
    "# plt.ylabel('n observations', size=14);\n",
    "# plt.xticks(size=12);\n",
    "# plt.yticks(size=12);\n",
    "# plt.savefig(fn_2);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a48f8b1",
   "metadata": {},
   "source": [
    "Load or calculate benchmark predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdecaf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmarks(df, id_col, target_col, obs_dir = None, sim_dir = None,\n",
    "               methods = ['CC', 'NSE', 'RMSE', 'KGE'],\n",
    "               fn_out = None): \n",
    "    \n",
    "    df_out = pd.DataFrame() \n",
    "    \n",
    "    df_out['ID'] = df.index.values \n",
    "    df_out[id_col] = df[id_col].values \n",
    "    df_out[target_col] = df[target_col].values \n",
    "    df_out = df_out.set_index('ID')\n",
    "    \n",
    "    buffer_idx = df_out[id_col].unique()\n",
    "    \n",
    "    for i in tqdm(range(len(buffer_idx))):\n",
    "        buffer_id = buffer_idx[i]\n",
    "        \n",
    "        _out = df_out[ df_out[id_col] == buffer_id] \n",
    "                \n",
    "        for method in methods:\n",
    "            \n",
    "            out_col = '{}_hat-{}'.format(target_col, method)\n",
    "            \n",
    "            if method in ['NSE', 'RMSE', 'KGE']:\n",
    "                \n",
    "                assert obs_dir.exists() , '[ERROR] obs_dir not specified or not found'\n",
    "                assert sim_dir.exists(), '[ERROR] sim_dir not specified or not found'\n",
    "                \n",
    "                fn_obs = obs_dir / '{}_Q_Day.Cmd.txt'.format(buffer_id)\n",
    "                fn_sim = sim_dir / 'buffer_{}_size-4.nc'.format(buffer_id) \n",
    "                \n",
    "                ## load observations\n",
    "                df_obs = pd.read_csv(fn_obs, skiprows=36, delimiter=';', encoding='cp850')\n",
    "                df_obs['Q_obs'] = df_obs[' Value'] \n",
    "                df_obs['date'] = pd.to_datetime(df_obs['YYYY-MM-DD'], yearfirst=True,\n",
    "                                               format='%Y-%m-%d')\n",
    "                df_obs = df_obs.drop(columns=[' Value', 'YYYY-MM-DD', 'hh:mm'])\n",
    "                df_obs = df_obs.set_index('date')\n",
    "                df_obs.loc[ df_obs['Q_obs'] == -999., 'Q_obs'] = np.nan \n",
    "                df_obs = df_obs.loc[df_obs.index >= '1991']\n",
    "                \n",
    "                ## load simulations \n",
    "                ds_sim = xr.open_dataset(fn_sim)\n",
    "                df_sim_list = ds_sim.to_dataframe().reset_index() \n",
    "                \n",
    "                df_sim = pd.DataFrame()\n",
    "                df_sim['date'] = pd.to_datetime( df_sim_list['time'].unique() )\n",
    "                df_sim  = df_sim.set_index('date')\n",
    "                \n",
    "                for i, x_cell in enumerate( df_sim_list['x'].unique() ):\n",
    "                    for j, y_cell in enumerate( df_sim_list['y'].unique() ):\n",
    "                        \n",
    "                        _df = df_sim_list[ (df_sim_list['x'] == x_cell) & (df_sim_list['y'] == y_cell) ]\n",
    "                        \n",
    "                        cell_id = '{}_{}{}'.format(buffer_id, int(i+1), int(j+1)) \n",
    "                        time = pd.to_datetime(_df['time'])\n",
    "                        df_sim.loc[time, cell_id] = _df['dis24'].values \n",
    "                \n",
    "                ## set simulations to observations extent \n",
    "                max_date = df_obs.tail(1).index.values[0]\n",
    "                min_date = df_obs.head(1).index.values[0]\n",
    "                df_sim = df_sim.loc[ (df_sim.index>=min_date) & (df_sim.index <= max_date)].copy()    \n",
    "                \n",
    "                ## mask and drop NaN values based on observations \n",
    "                gauge_mask = df_obs[df_obs['Q_obs'].isnull()>0].index \n",
    "                df_obs = df_obs.drop(index=gauge_mask)\n",
    "                df_sim = df_sim.drop(index=gauge_mask)   \n",
    "                \n",
    "                ## calculate selected metrics in the buffer \n",
    "                for cell in df_sim.columns:\n",
    "                    \n",
    "                    if method == 'NSE':\n",
    "                        try:\n",
    "                            res = he.evaluator(he.nse, df_sim[cell].values, df_obs['Q_obs'].values )[0]\n",
    "                        except:\n",
    "                            print(buffer_id)\n",
    "                            print(df_obs.head(2))\n",
    "                            print(df_obs.tail(2))\n",
    "                            print(df_sim.head(2).index)\n",
    "                            print(df_sim.tail(2).index)\n",
    "                            \n",
    "                        \n",
    "                    if method == 'RMSE':\n",
    "                        res = he.evaluator(he.rmse, df_sim[cell].values, df_obs['Q_obs'].values )[0]  \n",
    "                        \n",
    "                    if method == 'KGE':\n",
    "                        res, r, alpha, beta = he.evaluator(he.kge, df_sim[cell].values, df_obs['Q_obs'].values ) \n",
    "\n",
    "                    df_out.loc[cell, method] = res \n",
    "                \n",
    "                if method == 'NSE':\n",
    "                    y_hat_ix = df_out.loc[_out.index, method].idxmax()\n",
    "                if method == 'RMSE':\n",
    "                    y_hat_ix = df_out.loc[_out.index, method].idxmin()\n",
    "                if method == 'KGE':\n",
    "                    y_hat_ix = df_out.loc[_out.index, method].idxmax()\n",
    "                        \n",
    "                \n",
    "                df_out.loc[_out.index, out_col] = 0 \n",
    "                df_out.loc[y_hat_ix, out_col] = 1\n",
    "\n",
    "            if method == 'CC': \n",
    "                df_out.loc[ _out.index, out_col ] = 0 \n",
    "                center_ix = df[ (df['n_buffer'] == 0) & (df[id_col]==buffer_id)].index\n",
    "                df_out.loc[center_ix, out_col] = 1 \n",
    "            \n",
    "    df_out = df_out.dropna(axis=0)\n",
    "                \n",
    "    if fn_out == None:\n",
    "        return df_out\n",
    "    else:\n",
    "        df_out.to_csv(fn_out)\n",
    "        return fn_out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f5bd0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load or calculate benchmarks \n",
    "calc_bench = False\n",
    "fn_bench = signature_dir / \"S1_benchmarks_set1.csv\" \n",
    "\n",
    "if calc_bench:\n",
    "\n",
    "    base_dir = Path(r\"C:\\Users\\mvand\\Documents\\Master EE\\Year 4\\Thesis\\data\\training_data_S1\") \n",
    "\n",
    "    obs_dir = base_dir / \"V1\" \n",
    "    sim_dir = base_dir / \"efas_output_nc\"\n",
    "\n",
    "    df_benchmarks = benchmarks(df_similarity, 'tag', target_col[0], \n",
    "                               obs_dir, sim_dir) #, fn_out = fn_bench)\n",
    "\n",
    "else:\n",
    "    df_benchmarks = pd.read_csv(fn_bench, index_col=0)\n",
    "\n",
    "df_benchmarks\n",
    "\n",
    "df_benchmarks['range_target'] = df_similarity['range_target']\n",
    "df_benchmarks.loc[ df_benchmarks['range_target'].isnull() ] = 0 \n",
    "\n",
    "benchmarks_train_val = df_benchmarks[df_benchmarks['tag'].isin(id_train_val)]\n",
    "benchmarks_test = df_benchmarks[df_benchmarks['tag'].isin(id_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be95950",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ae87f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91986f17",
   "metadata": {},
   "source": [
    "If necessary, data can be transformed into a grid for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8634ff97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_grid(df, feature_cols, target_col, range_col=None, n_elements=9):\n",
    "    \n",
    "    ## create  multidimensional array with NaN value: -999. \n",
    "    feature_grid = np.ones((len(feature_cols), n_elements, n_elements)) * -999. \n",
    "    target_grid = np.zeros((n_elements, n_elements))\n",
    "    \n",
    "    range_grid = None \n",
    "    if range_col != None:\n",
    "        range_grid = np.zeros((n_elements, n_elements))\n",
    "        \n",
    "    ## fill array \n",
    "    for ix in df.index:\n",
    "        grid_x, grid_y = int(ix.split('_')[-1][0]), int(ix.split('_')[-1][1]) \n",
    "                \n",
    "        feature_grid[:, int(grid_y-1), int(grid_x-1) ] = df.loc[ix, feature_cols].values \n",
    "        target_grid[int(grid_y-1),int(grid_x-1)] = df.loc[ix, target_col] \n",
    "        \n",
    "        if range_col!= None:\n",
    "            range_grid[int(grid_y-1),int(grid_x-1)] = df.loc[ix, range_col]\n",
    "                     \n",
    "    return feature_grid, target_grid, range_grid\n",
    "\n",
    "def reshape_to_grid(df, feature_cols, target_col, range_col=None, id_col='tag', buffer_size=4):\n",
    "    \n",
    "    n_elements = int(1 + (2*buffer_size))\n",
    "    \n",
    "    ## create emtpy output grids \n",
    "    grid_features = np.zeros((df[id_col].nunique(), len(feature_cols), n_elements, n_elements )) \n",
    "    grid_targets = np.zeros((df[id_col].nunique(), n_elements, n_elements))\n",
    "    \n",
    "    grid_range = None\n",
    "    if range_col != None:\n",
    "        grid_range = np.zeros((df[id_col].nunique(), n_elements, n_elements))\n",
    "    \n",
    "    ## reshape each buffer \n",
    "    for i, idx in tqdm(enumerate(df[id_col].unique())):\n",
    "                \n",
    "        df_buffer = df[ df[id_col] == idx ] \n",
    "        \n",
    "        grid_buffer_features, grid_buffer_target, grid_buffer_range = feature_grid(df_buffer, feature_cols, \n",
    "                                                                                   target_col, range_col, n_elements) \n",
    "            \n",
    "        grid_features[i] = grid_buffer_features\n",
    "        grid_targets[i] = grid_buffer_target\n",
    "        \n",
    "        if range_col != None:\n",
    "            grid_range[i] = grid_buffer_range\n",
    "    \n",
    "    if range_col == None:\n",
    "        return grid_features, grid_targets \n",
    "    \n",
    "    return grid_features, grid_targets, grid_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e26f14f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "503it [00:00, 615.24it/s]\n",
      "92it [00:00, 564.30it/s]\n",
      "503it [00:00, 648.32it/s]\n",
      "503it [00:00, 619.45it/s]\n",
      "92it [00:00, 574.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(503, 1, 1) 38228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if grid_format:\n",
    "        \n",
    "    ## retransform datarame to grids \n",
    "#     n_elements = min( max(2, int(set_buffer_size+1)) , 4)\n",
    "    n_elements = max( int(set_buffer_size), 4 )\n",
    "    \n",
    "    ## CASE 1 \n",
    "    ## for determining target in buffer \n",
    "    X1_train_val, y1_train_val= reshape_to_grid(df_train_val, feature_columns,\n",
    "                                                target_buffer[0], buffer_size=n_elements) \n",
    "    X1_test, y1_test = reshape_to_grid(df_test, feature_columns, \n",
    "                                       target_buffer[0], buffer_size=n_elements) \n",
    "    \n",
    "    ## flatten X1 \n",
    "    n_samples, n_features, n_rows, n_cols = X1_train_val.shape \n",
    "    X1_train_val_flatten = np.reshape(X1_train_val, (n_samples, int(n_features*n_rows*n_cols) ) )\n",
    "    X1_test_flatten = np.reshape(X1_test, (len(X1_test), int(n_features*n_rows*n_cols))  )\n",
    "    \n",
    "    \n",
    "    ## for later plotting  \n",
    "    coords, dummy_target = reshape_to_grid(df_train_val, ['x', 'y',],\n",
    "                                      target_buffer[0], buffer_size = n_elements)\n",
    "    \n",
    "    ## simplify y1 values to single vector \n",
    "    y1_train_val_vector = np.zeros(n_samples)\n",
    "    for i in range(n_samples):\n",
    "        y1_train_val_vector[i] = np.max(y1_train_val[i])\n",
    "    \n",
    "    y1_test_vector = np.zeros(len(X1_test))\n",
    "    for i in range(len(y1_test_vector)):\n",
    "        y1_test_vector[i] = np.max(y1_test[i])\n",
    "    \n",
    "    \n",
    "    ## CASE 2 \n",
    "    ## for determining location of cell \n",
    "    ## for determining target in buffer \n",
    "    X2_train_val, y2_train_val, y2_train_val_range = reshape_to_grid(df_train_val, feature_columns,\n",
    "                                                                     target_col[0], range_col = range_col,\n",
    "                                                                     buffer_size=n_elements) \n",
    "    \n",
    "    X2_test, y2_test, y2_test_range = reshape_to_grid(df_test, feature_columns, \n",
    "                                                      target_col[0], range_col=range_col, \n",
    "                                                      buffer_size=n_elements) \n",
    "    \n",
    "    if case_max == False:\n",
    "        n_samples, m_features, i_rows, j_cols = X2_train_val.shape \n",
    "        \n",
    "        if i_rows > set_buffer_size or j_cols > set_buffer_size:\n",
    "            mid_row_ix = int( (i_rows-1) * 0.5)\n",
    "            mid_col_ix = int( (j_cols-1) * 0.5)\n",
    "            \n",
    "            min_row_ix, max_row_ix = mid_row_ix - set_buffer_size, mid_row_ix + set_buffer_size\n",
    "            min_col_ix, max_col_ix = mid_col_ix - set_buffer_size, mid_col_ix + set_buffer_size\n",
    "                                    \n",
    "            X2_train_val = X2_train_val[:,:, min_row_ix:max_row_ix+1, min_col_ix:max_col_ix+1]\n",
    "            y2_train_val = y2_train_val[:, min_row_ix:max_row_ix+1, min_col_ix:max_col_ix+1]\n",
    "            y2_train_val_range = y2_train_val_range[:, min_row_ix:max_row_ix+1, min_col_ix:max_col_ix+1]\n",
    "            \n",
    "            X2_test = X2_test[:,:, min_row_ix:max_row_ix+1, min_col_ix:max_col_ix+1]\n",
    "            y2_test = y2_test[:, min_row_ix:max_row_ix+1, min_col_ix:max_col_ix+1]\n",
    "            y2_test_range = y2_test_range[:, min_row_ix:max_row_ix+1, min_col_ix:max_col_ix+1]\n",
    "            \n",
    "    \n",
    "    ## flatten X2 \n",
    "    n_samples, n_features, n_rows, n_cols = X2_train_val.shape \n",
    "    \n",
    "    X2_train_val_flatten = np.reshape(X2_train_val, (n_samples, int(n_features*n_rows*n_cols) ) )\n",
    "    y2_train_val_flatten = np.reshape(y2_train_val, (n_samples, int(n_rows*n_cols) ) )\n",
    "    y2_train_val_range_flatten = np.reshape(y2_train_val_range, (n_samples, int(n_rows*n_cols) ) )\n",
    "        \n",
    "    n_test_samples = len(X2_test)\n",
    "    X2_test_flatten = np.reshape(X2_test, (n_test_samples, int(n_features*n_rows*n_cols))  )  \n",
    "    y2_test_flatten = np.reshape(y2_test, (n_test_samples, int(n_rows*n_cols) ) )\n",
    "    y2_test_range_flatten = np.reshape(y2_test_range, (n_test_samples, int(n_rows*n_cols) ) ) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c64caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e127e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train_val_flatten.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efe725f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276e9b20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fe804b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262596f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sum y_target vals to see distribution \n",
    "# x_ticks = np.arange(0,9) + 0.5 \n",
    "# labels = np.arange(-4, 5, 1)\n",
    "\n",
    "# train_sum = y2_train_val.sum(axis=0)\n",
    "# test_sum = y2_test.sum(axis=0)\n",
    "# total_sum = train_sum + test_sum\n",
    "\n",
    "# plt.figure(figsize=(8,6))\n",
    "# sns.heatmap( total_sum , cmap =  'Blues', annot=True, fmt='g', \n",
    "#            cbar = False, annot_kws={\"size\":16})\n",
    "\n",
    "# plt.xticks(x_ticks, ['{}'.format(v) for v in labels], size = 14);\n",
    "# plt.yticks(x_ticks, ['{}'.format(v) for v in labels][::-1], rotation=0, size = 14);\n",
    "# plt.xlabel('Shift from center coordinate', size = 16);\n",
    "# plt.ylabel('Shift from center coordinate', size = 16);\n",
    "\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # save_dir = Path(r\"C:\\Users\\mvand\\Documents\\Master EE\\Year 4\\Thesis\\meetings\\2021_06_28_GL\\new_media\") \n",
    "# # fn = save_dir / 'grid_shift_view.png'\n",
    "# # plt.savefig(fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab838881",
   "metadata": {},
   "source": [
    "Now, with k-fold cross validation find optimal algorithm settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf1c24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_performances(y, y_hat, k, model_name, grid_format = False,\n",
    "                          id_col='tag', target_col='range_target'):\n",
    "    \n",
    "    ## customize classifcation evaluation\n",
    "    TP = 0 \n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0 \n",
    "    n_targets = 0 \n",
    "    \n",
    "    if grid_format:\n",
    "        \n",
    "        n_samples, n_classes = y.shape \n",
    "        n_cells = n_samples * n_classes \n",
    "        n_samples, n_classes = y.shape       \n",
    "        \n",
    "        ## reshape to count TP, TN, FP, FN \n",
    "        for sample in range(len(y)):\n",
    "            \n",
    "            row_y = y[sample]\n",
    "            row_y_hat = y_hat[sample]\n",
    "            \n",
    "            ix_y = np.where(row_y > 0)[0]\n",
    "            ix_y_hat = np.where(row_y_hat > 0)[0]\n",
    "            \n",
    "            ## positve target label exists \n",
    "            if len(ix_y) > 0:\n",
    "                n_targets += 1 \n",
    "            \n",
    "                ## if a positive target label exists, and \n",
    "                ## prediction contains a positive label, \n",
    "                ## prediction is either true or false \n",
    "                if len(ix_y_hat) > 0:\n",
    "                    \n",
    "                    ## if prediction true, TP = 1 and TN are remaining cells \n",
    "                    if ix_y_hat[0] in ix_y:\n",
    "                        TP += 1 \n",
    "                        TN += (n_classes-1)\n",
    "                    \n",
    "                    ## if prediction is false, TP = 1 and TN are remaining cells \n",
    "                    if ix_y_hat[0] not in ix_y:\n",
    "                        FP += 1                    # or sum(row_y) ?, so 1-3 false positives)\n",
    "                        TN += (n_classes-2)        # or n_classes - sum(row_y)\n",
    "                        FN += 1 \n",
    "                \n",
    "                ## if a positive target label exists, but\n",
    "                ## prediction is all zeros, false negatives are counted \n",
    "                else: \n",
    "                    FN += 1 \n",
    "                    TN += (n_classes-1)\n",
    "            \n",
    "            ## no positive labels \n",
    "            if len(ix_y) == 0:\n",
    "                ## and prediction correctly all zero as well \n",
    "                if len(ix_y_hat) == 0:\n",
    "                    TN += n_classes \n",
    "                    \n",
    "                ## else one label is incorrectly labelled positive \n",
    "                ## rest is true negative \n",
    "                else:\n",
    "                    FP += 1\n",
    "                    TN += (n_classes-1)\n",
    "                    \n",
    "    else:\n",
    "        \n",
    "        ## reshape to count TP, TN, FP, FN \n",
    "        y['hat'] = y_hat\n",
    "        n_cells = len(y) \n",
    "        \n",
    "        for ix in y[id_col].unique():\n",
    "            _buffer = y[ y[id_col] == ix ]\n",
    "            \n",
    "            n_classes = len(_buffer)\n",
    "            \n",
    "            _y = _buffer[target_col]\n",
    "            _y_hat = _buffer['hat']\n",
    "            \n",
    "            ## if positive target\n",
    "            if _y.sum() > 0:\n",
    "                n_targets += 1 \n",
    "                _y_ix = _y[ _y>0].index.values \n",
    "\n",
    "                ## if positive prediction \n",
    "                if _y_hat.sum() > 0:\n",
    "                    _y_hat_ix = _y_hat.idxmax() \n",
    "                    \n",
    "                    ## if correct prediction \n",
    "                    if _y_hat_ix in _y_ix:\n",
    "                        TP += 1 \n",
    "                        TN += (n_classes-1)\n",
    "                    ## incorrect prediction\n",
    "                    else:\n",
    "                        FP += 1\n",
    "                        TN += (n_classes-2)\n",
    "                        FN += 1 \n",
    "                        \n",
    "                ## if no positive prediction\n",
    "                else:\n",
    "                    FN += 1 \n",
    "                    TN += (n_classes-1)\n",
    "                    \n",
    "            ## if no positive targets \n",
    "            else:\n",
    "                ## if no positive predictions \n",
    "                if _y_hat.sum() == 0:\n",
    "                    TN += n_classes\n",
    "                ## if positive predictions \n",
    "                else:\n",
    "                    FP += 1\n",
    "                    TN += (n_classes-1)\n",
    "    \n",
    "    ## calculate metrics \n",
    "    acc = (TP+TN) / (TP+TN+FP+FN)\n",
    "    b_acc = 0.5 * ( (TP/(TP+FN+1e-6)) + (TN/(TN+FP+1e-6))  )\n",
    "    prec = TP / (TP+FP+1e-6)\n",
    "    rec = TP / (TP+FN+1e-6) \n",
    "    f1 = (prec*rec) / (prec+rec+1e-6) \n",
    "    hit_rate = TP / n_targets \n",
    "\n",
    "    return_df = pd.DataFrame({\n",
    "                            'k': [k],\n",
    "                            'model': [model_name],\n",
    "                            'accuracy': [acc],\n",
    "                            'balanced_acc': [b_acc],\n",
    "                            'precision': [prec],\n",
    "                            'recall': [rec],\n",
    "                            'f1': [f1],\n",
    "                            'hit_rate': [hit_rate],\n",
    "                            'n': [n_targets],\n",
    "                            'N': [n_cells],\n",
    "                            'TP': [TP],\n",
    "                            'TN': [TN],\n",
    "                            'FP': [FP],\n",
    "                            'FN': [FN]\n",
    "                              })     \n",
    "    return return_df\n",
    "\n",
    "\n",
    "def buffer_classifier(df_val, y_prob, p0_col, p1_col,\n",
    "                     prediction_col, id_col, prob_threshold):\n",
    "        \n",
    "    df_val[p0_col] = y_prob[:,0]\n",
    "    df_val[p1_col] = y_prob[:,1] \n",
    "    df_val[prediction_col] = 0.\n",
    "    \n",
    "    for ix in df_val[id_col].unique():\n",
    "        max_ix = df_val[ (df_val[id_col]==ix) ][p1_col].idxmax()\n",
    "        \n",
    "        if df_val.loc[max_ix, p1_col] >= prob_threshold:\n",
    "            df_val.loc[max_ix, prediction_col] = 1.\n",
    "\n",
    "    return df_val\n",
    "\n",
    "def k_foldCV(X, y = [],  grid_format = False, K = 5, \n",
    "             id_col = None, feature_cols = [], target_col = None,\n",
    "             range_col = None, y_range=[],\n",
    "             n_subsample = 1, do_norm = True, do_scale = True, \n",
    "             do_PCA = True, n_pca = 0.9, whiten_pca = False,\n",
    "             methods = ['LR-2'], prob_threshold = 0.5, \n",
    "             return_classification = False, gauge_id_list = [],\n",
    "             take_subsample = True):\n",
    "             \n",
    "#              benchmarks = ['CC', 'NSE', 'RMSE', 'KGE'],\n",
    "#              df_benchmarks = None):\n",
    "\n",
    "    df_performance = pd.DataFrame()\n",
    "    \n",
    "    ## for returning classification \n",
    "    if grid_format:\n",
    "#         ds_collect_val = xr.Dataset() \n",
    "        \n",
    "        n_cells = y.shape[1] \n",
    "        ## assume square grid \n",
    "        n_buffer = int(n_cells**0.5)\n",
    "    else:\n",
    "        df_collect_val = pd.DataFrame()\n",
    "        \n",
    "    \n",
    "    if grid_format:\n",
    "        assert len(y) > 0, 'target column not specified'\n",
    "        sort_ix = list(range(len(X)))\n",
    "                \n",
    "        \n",
    "    if not grid_format:\n",
    "        assert id_col != None, 'buffer id col not specified'\n",
    "        assert target_col != None, 'buffer id col not specified'\n",
    "        assert len(feature_cols) > 0, 'feature columns not specified'\n",
    "        \n",
    "        sort_ix = X[id_col].unique()\n",
    "    \n",
    "    \n",
    "    ## shuffle and split ids \n",
    "    np.random.seed(26)\n",
    "    np.random.shuffle(sort_ix)\n",
    "    \n",
    "    ## split in K folds \n",
    "    k_split_idx = np.array_split(sort_ix, K)\n",
    "    \n",
    "    for k in tqdm(range(K)):\n",
    "        \n",
    "        ## split samples \n",
    "        id_val = k_split_idx[k]\n",
    "        id_train = np.setdiff1d(sort_ix, id_val)\n",
    "        \n",
    "        if grid_format:\n",
    "            X_train = X[id_train]\n",
    "            y_train = y[id_train]\n",
    "            \n",
    "            X_val = X[id_val]\n",
    "            y_val = y[id_val]\n",
    "            \n",
    "            if len(y_range) > 0:\n",
    "                y_val_range = y_range[id_val]\n",
    "        \n",
    "        if not grid_format:\n",
    "            \n",
    "            set_train = X[ X[id_col].isin(id_train) ]\n",
    "            set_val = X[ X[id_col].isin(id_val)]\n",
    "            \n",
    "            if take_subsample:\n",
    "                ## subsample training set only \n",
    "                set_train_1 = set_train[ set_train[target_col] == 1 ]\n",
    "\n",
    "                set_train_0_ix = []\n",
    "                for i, idx in enumerate(id_train):                    \n",
    "                    _df = set_train[ (set_train[target_col] != 1) & (set_train[id_col] == idx) ] \n",
    "                    if len(_df) > 0:     \n",
    "                        subsamples = _df.sample(n=n_subsample).index \n",
    "                        for sample in subsamples:\n",
    "                            set_train_0_ix.append(sample)\n",
    "\n",
    "                set_train_0 = set_train.loc[set_train_0_ix]\n",
    "\n",
    "                ## sample in_buffer = 0 samples \n",
    "                set_not_in_buffer = set_train[set_train['in_buffer']!=1]\n",
    "                n_sample = set_not_in_buffer[id_col].nunique()\n",
    "\n",
    "                ## add subsampled sets             \n",
    "                sample_train = set_train_1.append(set_train_0)\n",
    "                sample_train = sample_train.append( set_not_in_buffer.sample(n=n_sample) )\n",
    "            \n",
    "            ## or upsample!\n",
    "            else:                           \n",
    "                set_train_0 = set_train[set_train[target_col] ==0]\n",
    "                set_train_1 = set_train[set_train[target_col] ==1]\n",
    "                sample_train = set_train_0.append( set_train_1.sample(len(set_train_0), replace=True) )\n",
    "                             \n",
    "            ## split X and y \n",
    "            X_train = sample_train[feature_cols]\n",
    "            y_train = sample_train[target_col]\n",
    "\n",
    "            X_val = set_val[feature_cols]\n",
    "            y_val = set_val[target_col]\n",
    "            df_y_val = set_val.copy() #[[id_col, target_col]].copy()#.to_frame()\n",
    "\n",
    "            if range_col is not None:\n",
    "                y_val_range = set_val[[range_col, id_col]].copy()\n",
    "        \n",
    "        ## preprocess training data \n",
    "        if do_norm:\n",
    "            min_samples = len(X_train)\n",
    "            nm = QuantileTransformer(output_distribution='normal',\n",
    "                                    n_quantiles = int(min(1000, min_samples)))\n",
    "            X_train = nm.fit_transform(X_train)\n",
    "        else:\n",
    "            nm = None \n",
    "        \n",
    "        if do_scale:\n",
    "            sc = MinMaxScaler([0,1])\n",
    "            X_train = sc.fit_transform(X_train)\n",
    "        else:\n",
    "            sc = None\n",
    "        \n",
    "        if do_PCA:\n",
    "            pca = PCA(n_components = n_pca, whiten = whiten_pca)\n",
    "            pca.fit(X_train)\n",
    "            X_train = pca.transform(X_train)\n",
    "        else:\n",
    "            pca = None \n",
    "            \n",
    "        ## prepare trainin data \n",
    "        if do_norm:\n",
    "            X_val = nm.transform(X_val)\n",
    "        if do_scale:\n",
    "            X_val = sc.transform(X_val)\n",
    "        if do_PCA:\n",
    "            X_val = pca.transform(X_val)        \n",
    "        \n",
    "        ##  test models \n",
    "        for method in methods: \n",
    "            \n",
    "            prediction_col = '{}_{}'.format(method, target_col)\n",
    "            p0_col = '{}_p0'.format(method)\n",
    "            p1_col = '{}_p1'.format(method)\n",
    "                        \n",
    "            if 'LR' in method:\n",
    "                \n",
    "                ## train model \n",
    "                lr = LogisticRegression(max_iter=1000)\n",
    "                \n",
    "                if grid_format:\n",
    "                    lr = OneVsRestClassifier( lr )\n",
    "                \n",
    "                lr.fit(X_train, y_train)\n",
    "                \n",
    "                ## evaluate                                 \n",
    "                if 'LR-1' in method:\n",
    "                    y_val_hat = lr.predict(X_val)\n",
    "                        \n",
    "                if 'LR-2' in method:\n",
    "                    y_val_hat_prob = lr.predict_proba(X_val)\n",
    "                    \n",
    "                    if grid_format:\n",
    "                        y_val_hat = np.where( y_val_hat_prob >= prob_threshold, 1, 0)\n",
    "                    else:\n",
    "                        \n",
    "                        df_y_val = buffer_classifier(df_y_val, y_val_hat_prob,\n",
    "                                                    p0_col, p1_col, prediction_col,\n",
    "                                                    id_col, prob_threshold)\n",
    "                        \n",
    "                        y_val_hat = df_y_val[prediction_col]  \n",
    "\n",
    "                        \n",
    "                \n",
    "            if 'RF' in method:\n",
    "                \n",
    "                ## train model \n",
    "                rfc = RandomForestClassifier()                \n",
    "                rfc.fit(X_train, y_train)\n",
    "                \n",
    "                if 'RF-1' in method:\n",
    "                    y_val_hat = rfc.predict(X_val)\n",
    "                     \n",
    "                if 'RF-2' in method:\n",
    "                    _y_val_hat_prob = rfc.predict_proba(X_val)\n",
    "                    \n",
    "                    if grid_format:\n",
    "                        y_val_hat = np.zeros(y_val.shape)\n",
    "                        y_val_hat_prob = np.zeros(y_val.shape)\n",
    "\n",
    "                        for sample in range(len(_y_val_hat_prob)):\n",
    "                            if _y_val_hat_prob[sample].shape[1] >1:\n",
    "                                p1 = _y_val_hat_prob[sample][:,1]\n",
    "                                _y_val_hat = np.zeros(len(p1)) \n",
    "                                \n",
    "                                ix_max = p1.argmax() \n",
    "                                \n",
    "                                if p1[p1.argmax()] >= prob_threshold:\n",
    "                                    _y_val_hat[ix_max] = 1. \n",
    "\n",
    "                                y_val_hat[:,sample] = _y_val_hat \n",
    "                                y_val_hat_prob[:,sample] += p1 \n",
    "                    \n",
    "                    else:\n",
    "                        y_val_hat_prob = _y_val_hat_prob \n",
    "                        df_y_val = buffer_classifier(df_y_val, y_val_hat_prob,\n",
    "                                                    p0_col, p1_col, prediction_col,\n",
    "                                                    id_col, prob_threshold)\n",
    "                        \n",
    "                        y_val_hat = df_y_val[prediction_col]\n",
    "                    \n",
    "            if 'SVM' in method:\n",
    "                \n",
    "                svc = SVC(probability=True)\n",
    "                \n",
    "                if grid_format:\n",
    "                    svc = OneVsRestClassifier(svc) \n",
    "                \n",
    "                svc.fit(X_train, y_train)\n",
    "                \n",
    "                if 'SVM-1' in method:\n",
    "                    y_val_hat = svc.predict(X_val)\n",
    "                    \n",
    "                if 'SVM-2' in method:\n",
    "                    y_val_hat_prob = svc.predict_proba(X_val)\n",
    "                    \n",
    "                    if grid_format:\n",
    "                        y_val_hat = np.where( y_val_hat_prob >= prob_threshold, 1, 0)\n",
    "                        \n",
    "                    else:   \n",
    "                        \n",
    "                        df_y_val = buffer_classifier(df_y_val, y_val_hat_prob,\n",
    "                                                    p0_col, p1_col, prediction_col,\n",
    "                                                    id_col, prob_threshold)\n",
    "                        \n",
    "                        y_val_hat = df_y_val[prediction_col]                \n",
    "\n",
    "            if 'k-nn' in method:\n",
    "                \n",
    "                if not do_PCA:\n",
    "                    \n",
    "                    if not do_norm:\n",
    "                        min_samples = len(X_train)\n",
    "                        nm = QuantileTransformer(output_distribution='normal',\n",
    "                                                n_quantiles = int(min(1000, min_samples)))\n",
    "                        X_train = nm.fit_transform(X_train)\n",
    "                        X_val = nm.transform(X_train)\n",
    "                    \n",
    "                    if not do_scale:\n",
    "                        sc = MinMaxScaler([0,1])\n",
    "                        X_train = sc.fit_transorm(X_train)\n",
    "                        X_val = sc.transform(X_val)\n",
    "                    \n",
    "                    pca = PCA(n_components = n_pca)\n",
    "                    pca.fit(X_train)\n",
    "                    X_train = pca.transform(X_train)\n",
    "                    X_val = pca.transform(X_val)\n",
    "                \n",
    "                knn = KNeighborsClassifier() \n",
    "                                \n",
    "                knn.fit(X_train, y_train) \n",
    "                \n",
    "                if 'k-nn-1' in method:\n",
    "                    y_val_hat = knn.predict(X_val)\n",
    "                    \n",
    "                if 'k-nn-2' in method:\n",
    "                    _y_val_hat_prob = knn.predict_proba(X_val)\n",
    "                    \n",
    "                    if grid_format:\n",
    "                        y_val_hat = np.zeros(y_val.shape)\n",
    "                        y_val_hat_prob = np.zeros(y_val.shape)\n",
    "\n",
    "                        for sample in range(len(_y_val_hat_prob)):\n",
    "                            if _y_val_hat_prob[sample].shape[1] >1:\n",
    "                                p1 = _y_val_hat_prob[sample][:,1]\n",
    "                                _y_val_hat = np.zeros(len(p1)) \n",
    "                                \n",
    "                                ix_max = p1.argmax() \n",
    "                                \n",
    "                                if p1[p1.argmax()] >= prob_threshold:\n",
    "                                    _y_val_hat[ix_max] = 1. \n",
    "\n",
    "                                y_val_hat[:,sample] = _y_val_hat\n",
    "                                y_val_hat_prob[:,sample] += p1 \n",
    "                    \n",
    "                    else: \n",
    "                        y_val_hat_prob = _y_val_hat_prob \n",
    "                        df_y_val = buffer_classifier(df_y_val, y_val_hat_prob,\n",
    "                                                    p0_col, p1_col, prediction_col,\n",
    "                                                    id_col, prob_threshold)\n",
    "                        \n",
    "                        y_val_hat = df_y_val[prediction_col]\n",
    "                    \n",
    "            \n",
    "                               \n",
    "            ## calculate perforamnce \n",
    "            df_performance = df_performance.append( evaluate_performances(y_val_range, y_val_hat,\n",
    "                                                                          k, method, grid_format=grid_format) )\n",
    "            if not grid_format:\n",
    "                try:\n",
    "                    df_collect_val.loc[df_y_val.index, df_y_val.columns] = df_y_val \n",
    "                except:\n",
    "                    df_collect_val = df_collect_val.append(df_y_val)\n",
    "            \n",
    "            else:\n",
    "                \n",
    "               ## reshape data back to grid \n",
    "                n_samples = len(id_val)\n",
    "                y_val_gridded = y_val.reshape((n_samples, n_buffer, n_buffer)) \n",
    "                y_val_hat_gridded = y_val_hat.reshape((n_samples, n_buffer, n_buffer))\n",
    "                \n",
    "                if len(y_range) > 0:\n",
    "                    y_val_range_gridded = y_val_range.reshape((n_samples, n_buffer, n_buffer))\n",
    "                \n",
    "                try:\n",
    "                    y_hat_p1_gridded = y_val_hat_prob.reshape((n_samples, n_buffer, n_buffer))\n",
    "                except:\n",
    "                    y_hat_p1_gridded = [] \n",
    "                \n",
    "                if len(gauge_id_list) == 0:\n",
    "                    ix_y_val = [ f'{idv}_{method}_y' for idv in id_val]\n",
    "                    ix_y_val_hat = [f'{idv}_{method}_y_hat' for idv in id_val] \n",
    "                    ix_y_val_range = [f'{idv}_{method}_y_range' for idv in id_val] \n",
    "                    ix_y_hat_p1 = [f'{idv}_{method}_y_p1' for idv in id_val] \n",
    "                \n",
    "                if len(gauge_id_list) > 0:\n",
    "                    ix_y_val = [ f'{gauge_id_list[idg]}_{method}_y' for idg in id_val ]\n",
    "                    ix_y_val_hat = [f'{gauge_id_list[idg]}_{method}_y_hat' for idg in id_val] \n",
    "                    ix_y_val_range = [f'{gauge_id_list[idg]}_{method}_y_range' for idg in id_val] \n",
    "                    ix_y_hat_p1 = [f'{gauge_id_list[idg]}_{method}_y_p1' for idg in id_val] \n",
    "                \n",
    "                \n",
    "                \n",
    "                ds_y_val = xr.DataArray(data=y_val_gridded,\n",
    "                                        dims=['type', 'x', 'y'],\n",
    "                                        coords = dict( val=(['type'], ix_y_val) )\n",
    "                                       )\n",
    "                \n",
    "                ## initilize or concatenate                 \n",
    "                try:\n",
    "                    ds_collect_val = xr.concat([ds_collect_val, ds_y_val], dim='type')\n",
    "                except:\n",
    "                    ds_collect_val = ds_y_val \n",
    "                \n",
    "                ## concat remaining \n",
    "                ds_y_val_hat = xr.DataArray(data=y_val_hat_gridded,\n",
    "                                            dims=['type', 'x', 'y'],\n",
    "                                            coords = dict( val=(['type'], ix_y_val_hat) )\n",
    "                                           )\n",
    "                \n",
    "                ds_collect_val = xr.concat( [ds_collect_val, ds_y_val_hat], dim='type')\n",
    "                \n",
    "                if len(y_range) > 0:\n",
    "                    \n",
    "                    ds_y_val_range = xr.DataArray(data=y_val_range_gridded,\n",
    "                                                dims=['type', 'x', 'y'],\n",
    "                                                coords = dict( val=(['type'], ix_y_val_range) )\n",
    "                                           )\n",
    "                    ds_collect_val = xr.concat( [ds_collect_val, ds_y_val_range], dim='type' )\n",
    "                \n",
    "                if len(y_hat_p1_gridded) > 0:\n",
    "                    \n",
    "                    ds_y_val_range = xr.DataArray(data=y_hat_p1_gridded,\n",
    "                                                dims=['type', 'x', 'y'],\n",
    "                                                coords = dict( val=(['type'], ix_y_hat_p1) )\n",
    "                                           )\n",
    "                    \n",
    "                    ds_collect_val = xr.concat( [ds_collect_val, ds_y_val_range], dim='type' )                    \n",
    "                    \n",
    "    if return_classification:\n",
    "        if grid_format:\n",
    "            return df_performance, ds_collect_val    \n",
    "        else:\n",
    "            return df_performance, df_collect_val                 \n",
    "    else:\n",
    "        return df_performance \n",
    "\n",
    "    \n",
    "def k_foldCV_dev(X, y = [],  grid_format = False, K = 5, \n",
    "             id_col = None, feature_cols = [], target_col = None,\n",
    "             range_col = None, y_range=[],\n",
    "             n_subsample = 1, do_norm = True, do_scale = True, \n",
    "             do_PCA = True, n_pca = 0.9, whiten_pca = False,\n",
    "             methods = [], prob_threshold = 0.5, \n",
    "             return_classification = False, gauge_id_list = [],\n",
    "             take_subsample = True, take_upsample = False):\n",
    "    \n",
    "    df_performance = pd.DataFrame()\n",
    "    use_range =False \n",
    "    \n",
    "    ## for returning classification \n",
    "    if grid_format:\n",
    "#         ds_collect_val = xr.Dataset() \n",
    "        n_cells = y.shape[1] \n",
    "        ## assume square grid \n",
    "        n_buffer = int(n_cells**0.5)\n",
    "    else:\n",
    "        df_collect_val = pd.DataFrame()\n",
    "        \n",
    "    if grid_format:\n",
    "        assert len(y) > 0, 'target column not specified'\n",
    "        sort_ix = list(range(len(X)))\n",
    "                \n",
    "    if not grid_format:\n",
    "        assert id_col != None, 'buffer id col not specified'\n",
    "        assert target_col != None, 'buffer id col not specified'\n",
    "        assert len(feature_cols) > 0, 'feature columns not specified'\n",
    "        \n",
    "        sort_ix = X[id_col].unique()\n",
    "    \n",
    "    ## shuffle and split ids \n",
    "    np.random.seed(26)\n",
    "    np.random.shuffle(sort_ix)\n",
    "    \n",
    "    ## split in K folds \n",
    "    k_split_idx = np.array_split(sort_ix, K)\n",
    "    \n",
    "    for k in tqdm(range(K)):\n",
    "        \n",
    "        ## split samples \n",
    "        id_val = k_split_idx[k]\n",
    "        id_train = np.setdiff1d(sort_ix, id_val)\n",
    "        \n",
    "        if grid_format:\n",
    "            X_train = X[id_train]\n",
    "            y_train = y[id_train]\n",
    "            \n",
    "            X_val = X[id_val]\n",
    "            y_val = y[id_val]\n",
    "            \n",
    "            if len(y_range) > 0:\n",
    "                y_val_range = y_range[id_val] \n",
    "                use_range = True\n",
    "        \n",
    "        if not grid_format:\n",
    "            \n",
    "            set_train = X[ X[id_col].isin(id_train) ]\n",
    "            set_val = X[ X[id_col].isin(id_val)]\n",
    "          \n",
    "            if take_subsample:\n",
    "                take_upsample = False\n",
    "                ## subsample training set only \n",
    "                set_train_1 = set_train[ set_train[target_col] == 1 ]\n",
    "                set_train_0 = set_train[ set_train[target_col] == 0 ]\n",
    "\n",
    "                ## sample in_buffer = 0 samples \n",
    "                set_not_in_buffer = set_train[set_train['in_buffer']!=1]\n",
    "                n_sample = set_not_in_buffer[id_col].nunique()\n",
    "                n_sample = 0 \n",
    "\n",
    "                ## add subsampled sets             \n",
    "                sample_train = set_train_1.append( set_train_0.sample(n=len(set_train_1)) )\n",
    "#                 sample_train = sample_train.append( set_not_in_buffer.sample(n=n_sample) )\n",
    "            \n",
    "            ## or upsample!\n",
    "            if take_upsample:\n",
    "                set_train_0 = set_train[set_train[target_col] ==0]\n",
    "                set_train_1 = set_train[set_train[target_col]==1]\n",
    "                 \n",
    "                n_max = max( len(set_train_0), len(set_train_1) ) \n",
    "                \n",
    "                if len(set_train_0) < n_max:\n",
    "                    sample_train = set_train_1.append( set_train_0.sample(n_max, replace=True))\n",
    "                    \n",
    "                if len(set_train_1) < n_max:\n",
    "                    sample_train = set_train_0.append( set_train_1.sample(n_max, replace=True) )\n",
    "                     \n",
    "            if not take_subsample and not take_upsample:\n",
    "                sample_train = set_train \n",
    "\n",
    "            ## split X and y \n",
    "            X_train = sample_train[feature_cols]\n",
    "            y_train = sample_train[target_col]\n",
    "\n",
    "            X_val = set_val[feature_cols]\n",
    "            y_val = set_val[target_col]\n",
    "            df_y_val = set_val.copy() \n",
    "\n",
    "            if range_col is not None:\n",
    "                y_val_range = set_val[[range_col, id_col]].copy()\n",
    "                use_range = True\n",
    "        \n",
    "        ## preprocess training data \n",
    "        if do_norm:\n",
    "            min_samples = len(X_train)\n",
    "            nm = QuantileTransformer(output_distribution='normal',\n",
    "                                    n_quantiles = int(min(1000, min_samples)))\n",
    "            X_train = nm.fit_transform(X_train)\n",
    "        else:\n",
    "            nm = None \n",
    "        \n",
    "        if do_scale:\n",
    "            sc = MinMaxScaler([0,1])\n",
    "            X_train = sc.fit_transform(X_train)\n",
    "        else:\n",
    "            sc = None\n",
    "        \n",
    "        if do_PCA:\n",
    "            pca = PCA(n_components = n_pca, whiten = whiten_pca)\n",
    "            pca.fit(X_train)\n",
    "            X_train = pca.transform(X_train)\n",
    "        else:\n",
    "            pca = None \n",
    "            \n",
    "        ## prepare training data \n",
    "        if do_norm:\n",
    "            X_val = nm.transform(X_val)\n",
    "        if do_scale:\n",
    "            X_val = sc.transform(X_val)\n",
    "        if do_PCA:\n",
    "            X_val = pca.transform(X_val)        \n",
    "        \n",
    "        ##  test models \n",
    "        ## settings is list of tuples \n",
    "        for settings in methods:\n",
    "            \n",
    "            method, model = settings \n",
    "            \n",
    "            prediction_col = '{}_{}'.format(method, target_col)\n",
    "            p0_col = '{}_p0'.format(method)\n",
    "            p1_col = '{}_p1'.format(method) \n",
    "                        \n",
    "            ## train model \n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            if grid_format:\n",
    "                ## asses shape of _y_val_hat\n",
    "                _y_val_hat_prob = model.predict_proba(X_val)\n",
    "                \n",
    "                ## assess type to interpret proba \n",
    "                if isinstance(_y_val_hat_prob, np.ndarray):\n",
    "                    y_val_hat = np.where( _y_val_hat_prob >= prob_threshold, 1, 0)\n",
    "                \n",
    "                else:\n",
    "                    print(type(_y_val_hat_prob))\n",
    "                    y_val_hat = np.zeros(y_val.shape)\n",
    "                    y_val_hat_prob = np.zeros(y_val.shape)\n",
    "\n",
    "                    for sample in range(len(_y_val_hat_prob)):\n",
    "                        if _y_val_hat_prob[sample].shape[1] >1:\n",
    "                            p1 = _y_val_hat_prob[sample][:,1]\n",
    "                            _y_val_hat = np.zeros(len(p1)) \n",
    "\n",
    "                            ix_max = p1.argmax() \n",
    "\n",
    "                            if p1[p1.argmax()] >= prob_threshold:\n",
    "                                _y_val_hat[ix_max] = 1. \n",
    "\n",
    "                            y_val_hat[:,sample] = _y_val_hat \n",
    "                            y_val_hat_prob[:,sample] += p1 \n",
    "                \n",
    "            else:\n",
    "                y_val_hat_prob = model.predict_proba(X_val)\n",
    "                \n",
    "                df_y_val = buffer_classifier(df_y_val, y_val_hat_prob,\n",
    "                                                    p0_col, p1_col, prediction_col,\n",
    "                                                    id_col, prob_threshold)\n",
    "                        \n",
    "                y_val_hat = df_y_val[prediction_col]  \n",
    "            \n",
    "            \n",
    "            if use_range:\n",
    "                df_performance = df_performance.append( evaluate_performances(y_val_range, y_val_hat,\n",
    "                                                                           k, method, grid_format=grid_format) )                        \n",
    "            else:\n",
    "                df_performance = df_performance.append( evaluate_performances(set_val[[target_col, id_col]], y_val_hat,\n",
    "                                                                              k, method, grid_format=grid_format,\n",
    "                                                                             target_col = target_col) )   \n",
    "            \n",
    "    return df_performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a67ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## K-fold settings \n",
    "\n",
    "n_folds = 10 \n",
    "algorithms = ['LR-1', 'LR-2', 'RF-1', 'RF-2', 'SVM-1', 'SVM-2', 'k-nn-1', 'k-nn-2'] \n",
    "\n",
    "stat_cols = ['accuracy', 'balanced_acc', 'precision', 'recall', 'f1',  \n",
    "             'TP', 'TN', 'FP', 'FN', 'hit_rate', 'n', 'N']\n",
    "\n",
    "std_cols = ['accuracy', 'balanced_acc', 'precision', 'recall', 'f1',  \n",
    "             'TP', 'TN', 'FP', 'FN', 'hit_rate']\n",
    "\n",
    "test_dir = Path(r\"C:\\Users\\mvand\\Documents\\Master EE\\Year 4\\Thesis\\data\\test_output\\PCA_analysis_4\")\n",
    "\n",
    "if use_set_1:\n",
    "    name_set = 'set1'\n",
    "else:\n",
    "    name_set = 'set2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fe7da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ignore traning warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d96ab70",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST K-foldCV grid\n",
    "# performance, ds_val = k_foldCV(X2_train_val_flatten,  y2_train_val_flatten, grid_format=True,\n",
    "#                         y_range = y2_train_val_range_flatten, methods= algorithms,\n",
    "#                       gauge_id_list = id_train_val, return_classification=True)\n",
    "# performance.groupby(by='model')[stat_cols].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371380f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST K-foldCV single pixel \n",
    "# _df, df_class_noPCA = k_foldCV( df_train_val, id_col='tag', target_col = target_col[0],\n",
    "#                                    range_col = 'range_target',feature_cols = feature_columns, \n",
    "#                                    methods = algorithms, do_PCA = True, return_classification=True,\n",
    "#                               take_subsample = True)\n",
    "\n",
    "# _df.groupby(by='model')[stat_cols].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8329ad5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get benchmarks matching with database \n",
    "print('Benchmark performance:')\n",
    "\n",
    "df_benchmark_performances = pd.DataFrame()\n",
    "\n",
    "if case_max:\n",
    "    \n",
    "    y = benchmarks_train_val.copy()\n",
    "    y_hat_list = ['target_hat-CC', 'target_hat-NSE', 'target_hat-RMSE', 'target_hat-KGE']\n",
    "    \n",
    "    for y_hat_col in y_hat_list:\n",
    "        y_hat = benchmarks_train_val[y_hat_col]\n",
    "        model_name = y_hat_col.split('-')[-1]\n",
    "        performance = evaluate_performances(y, y_hat, 0, model_name, grid_format = False,\n",
    "                              id_col='tag', target_col='range_target')  #'range_target'\n",
    "        df_benchmark_performances = df_benchmark_performances.append(performance)\n",
    "else:\n",
    "    \n",
    "    ## center vals only \n",
    "    y = benchmarks_train_val[ benchmarks_train_val['target_hat-CC']==1 ].copy()\n",
    "        \n",
    "    y_hat_list = ['target_hat-CC', 'target_hat-NSE', 'target_hat-RMSE', 'target_hat-KGE']\n",
    "    \n",
    "    for y_hat_col in y_hat_list:\n",
    "       \n",
    "        y_hat = benchmarks_train_val[y_hat_col]\n",
    "        model_name = y_hat_col.split('-')[-1]\n",
    "\n",
    "        performance = evaluate_performances(y, y_hat, 0, model_name, grid_format = False,\n",
    "                              id_col='tag', target_col='range_target')  #'range_target'\n",
    "        \n",
    "        df_benchmark_performances = df_benchmark_performances.append(performance)\n",
    "        \n",
    "## update train_val for exact comparison \n",
    "# df_train_val_update = df_train_val.loc[y.index]\n",
    "# \n",
    "df_benchmark_performances.groupby(by='model')[stat_cols].sum()\n",
    "\n",
    "# save_file = test_dir / 'benchmark_performance_set1_2_trainval_range-updateFN.csv'\n",
    "# df_benchmark_performances.to_csv(save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac05fd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check division of NSE, RMSE, KGE True/false \n",
    "\n",
    "## https://seaborn.pydata.org/generated/seaborn.boxplot.html\n",
    "## default: whiskers / IQR = 1.5, above or below: outliers \n",
    "## Box is bordered by first Quartile (25percent) and third Quartile (75 percent)\n",
    "## The differnce between Q1 and Q3 is the InterQuartileRange (IQR)\n",
    "## The centerline in the box corresponds to to median value (Q50)\n",
    "## The whiskers on the plot extent to (Q1-1.5*IQR) or (Q3+1.5*IQR)\n",
    "## All values outside whiskers are considered outliers and shown individually \n",
    "\n",
    "# df_center = benchmarks_train_val[ benchmarks_train_val['target_hat-CC']==1 ].copy()\n",
    "\n",
    "# benchmarks_plot = [\n",
    "#     ('KGE', -3, 1.2),\n",
    "#     ('NSE', -3, 1.2),\n",
    "#     ('RMSE', -10, 300)\n",
    "# ]\n",
    "\n",
    "# plt.figure(figsize=(14,4))\n",
    "# sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# for i, settings in enumerate(benchmarks_plot):\n",
    "    \n",
    "#     param, plot_min, plot_max = settings\n",
    "    \n",
    "#     min_1_val = df_center[ df_center[f'target_hat-{param}']==1][param].min()\n",
    "    \n",
    "#     if 'RMSE' in param:\n",
    "#         min_1_val = df_center[ df_center[f'target_hat-{param}']==1][param].max()\n",
    "    \n",
    "#     print('{}: {:.3f}'.format(param, min_1_val))\n",
    "    \n",
    "#     plt.subplot(1, len(benchmarks_plot), int(i+1) ) \n",
    "\n",
    "#     plt.title(f'{param} values versus classification label')\n",
    "#     sns.boxplot( hue=f'target_hat-{param}', x='target', y=param, data = df_center, palette = \"crest\")\n",
    "#     plt.ylim( plot_min, plot_max);    \n",
    "    \n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08484563",
   "metadata": {},
   "source": [
    "The plots above show the true target values versus the benchmark predicted values, with raw benchmark values distributed. Could be used to set a limit value to say: values above are 1 with high certainty. However: could be model specific!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b023a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmarks_train_val['tag'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fce510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_val['tag'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3897001e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## from case max extract data for varying buffer size \n",
    "\n",
    "# alg_dev = [\n",
    "#         ('SVM-0', OneVsRestClassifier(SVC(probability=True)) ),\n",
    "#         ('SVM-C2', OneVsRestClassifier(SVC(C=2, probability=True))  ),\n",
    "#         ('SVM-C01', OneVsRestClassifier(SVC(C=0.1, probability=True))  )\n",
    "#     ]\n",
    "\n",
    "# performance = k_foldCV_dev(X2_train_val_flatten,  y2_train_val_flatten, grid_format=True,\n",
    "#                                 y_range = y2_train_val_range_flatten, methods= alg_dev,\n",
    "#                                 gauge_id_list = id_train_val, return_classification=True,\n",
    "#                                 do_PCA = do_pca, n_pca = set_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4699e777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance.groupby(by='model').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82decbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEVELOP \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# if case_max:\n",
    "#     print(f'Maximum buffer search: {select_alg}')\n",
    "    \n",
    "#     ## compare performance with: (NC), NSE, RMSE, KGE \n",
    "    \n",
    "# #     alg_dev = [\n",
    "# #         ('SVM-0', OneVsRestClassifier(SVC(probability=True)) ),\n",
    "# #         ('SVM-C2', OneVsRestClassifier(SVC(C=2, probability=True))  ),\n",
    "# #         ('SVM-C01', OneVsRestClassifier(SVC(C=0.1, probability=True))  )\n",
    "# #     ]\n",
    "        \n",
    "#     performance = k_foldCV_dev(X2_train_val_flatten,  y2_train_val_flatten, grid_format=True,\n",
    "#                                 y_range = y2_train_val_range_flatten, methods= alg_dev,\n",
    "#                                 gauge_id_list = id_train_val, return_classification=True,\n",
    "#                                 do_PCA = do_pca, n_pca = set_pca)\n",
    "        \n",
    "# else:\n",
    "# #     print(f'Minimum buffer search: {select_alg}')\n",
    "\n",
    "#     ## compare performance with: NC, NSE, RMSE, KGE \n",
    "\n",
    "#     alg_dev = [\n",
    "#         ('SVM-0', OneVsRestClassifier(SVC(probability=True)) ),\n",
    "#         ('SVM-C2', OneVsRestClassifier(SVC(C=2, probability=True))  ),\n",
    "#         ('SVM-C01', OneVsRestClassifier(SVC(C=0.1, probability=True))  )\n",
    "#     ]    \n",
    "    \n",
    "# #     alg_dev = [\n",
    "# #         ( 'RF-0', RandomForestClassifier() ),\n",
    "# #         ( 'RF-entropy', RandomForestClassifier(criterion=\"entropy\") ),\n",
    "# #         ( 'RF-est200', RandomForestClassifier(n_estimators=200) ),\n",
    "# #         ( 'RF-est500', RandomForestClassifier(n_estimators=500) ),\n",
    "# #         ( 'RF-est1000', RandomForestClassifier(n_estimators=1000) ),\n",
    "# #         ( 'RF-est200_ent', RandomForestClassifier(n_estimators=200, criterion=\"entropy\") ),\n",
    "# #         ( 'RF-est500_ent', RandomForestClassifier(n_estimators=500, criterion=\"entropy\") ),\n",
    "# #         ( 'RF-est1000_ent', RandomForestClassifier(n_estimators=1000, criterion=\"entropy\") )\n",
    "# #     ]\n",
    "    \n",
    "#     performance = k_foldCV_dev( df_train_val_update, id_col='tag', target_col = target_col[0],\n",
    "#                                 range_col = 'range_target', \n",
    "#                                 feature_cols = feature_columns, take_subsample=False, take_upsample=True,\n",
    "#                                 methods = alg_dev, do_PCA = do_pca, return_classification=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e329ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance.groupby(by='model')[stat_cols].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f563c660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_benchmark_performances.groupby(by='model')[stat_cols].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23043fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance.groupby(by='model')[['TP', 'TN', 'FP', 'FN', 'n', 'N']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f36b07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b607655e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc01fc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c670fee1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6892c230",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b325bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PCA sensitivity analysis - for raster pixel \n",
    "\n",
    "df_grid_no_pca = pd.DataFrame()\n",
    "df_grid_pca60 = pd.DataFrame()\n",
    "df_grid_pca70 = pd.DataFrame()\n",
    "df_grid_pca80 = pd.DataFrame()\n",
    "df_grid_pca90 = pd.DataFrame()\n",
    "df_grid_pca95 = pd.DataFrame()\n",
    "df_grid_pca99 = pd.DataFrame() \n",
    "\n",
    "\n",
    "# ## training no_pca takes a long time, only do K-fold CV once \n",
    "# # _df, ds_class_noPCA = k_foldCV( X2_train_val_flatten,  y2_train_val_flatten, grid_format=True,\n",
    "# #                                 y_range = y2_train_val_range_flatten, methods= algorithms,  \n",
    "# #                                 gauge_id_list = id_train_val, return_classification=True,\n",
    "# #                                 do_PCA = False)\n",
    "# # df_grid_no_pca = df_grid_no_pca.append(_df)\n",
    "\n",
    "\n",
    "for kn in range(n_folds):\n",
    "    print('Iteration {}'.format(int(kn+1)))\n",
    "#     # no pca \n",
    "    _df, ds_class_noPCA = k_foldCV( X2_train_val_flatten,  y2_train_val_flatten, grid_format=True,\n",
    "                                    y_range = y2_train_val_range_flatten, methods= algorithms,  \n",
    "                                    gauge_id_list = id_train_val, return_classification=True,\n",
    "                                    do_PCA = False)\n",
    "    df_grid_no_pca = df_grid_no_pca.append(_df)\n",
    "    \n",
    "    # PCA 60% \n",
    "    _df, ds_class_pca60 = k_foldCV( X2_train_val_flatten,  y2_train_val_flatten, grid_format=True,\n",
    "                                    y_range = y2_train_val_range_flatten, methods= algorithms,  \n",
    "                                    gauge_id_list = id_train_val, return_classification=True,\n",
    "                                    do_PCA = True, n_pca = 0.6)\n",
    "    df_grid_pca60 = df_grid_pca60.append(_df)    \n",
    "\n",
    "    ## PCA 70% \n",
    "    _df, ds_class_pca70 = k_foldCV( X2_train_val_flatten,  y2_train_val_flatten, grid_format=True,\n",
    "                                    y_range = y2_train_val_range_flatten, methods= algorithms,  \n",
    "                                    gauge_id_list = id_train_val, return_classification=True,\n",
    "                                    do_PCA = True, n_pca = 0.7)\n",
    "    df_grid_pca70 = df_grid_pca70.append(_df) \n",
    "\n",
    "    ## PCA 80% \n",
    "    _df, ds_class_pca80 = k_foldCV( X2_train_val_flatten,  y2_train_val_flatten, grid_format=True,\n",
    "                                    y_range = y2_train_val_range_flatten, methods= algorithms,  \n",
    "                                    gauge_id_list = id_train_val, return_classification=True,\n",
    "                                    do_PCA = True, n_pca = 0.8)\n",
    "    df_grid_pca80 = df_grid_pca80.append(_df) \n",
    "\n",
    "    ## PCA 90% \n",
    "    _df, ds_class_pca90 = k_foldCV( X2_train_val_flatten,  y2_train_val_flatten, grid_format=True,\n",
    "                                    y_range = y2_train_val_range_flatten, methods= algorithms,  \n",
    "                                    gauge_id_list = id_train_val, return_classification=True,\n",
    "                                    do_PCA = True, n_pca = 0.9)\n",
    "    df_grid_pca90 = df_grid_pca90.append(_df) \n",
    "\n",
    "    ## PCA 95% \n",
    "    _df, ds_class_pca95 = k_foldCV( X2_train_val_flatten,  y2_train_val_flatten, grid_format=True,\n",
    "                                    y_range = y2_train_val_range_flatten, methods= algorithms,  \n",
    "                                    gauge_id_list = id_train_val, return_classification=True,\n",
    "                                    do_PCA = True, n_pca = 0.95)\n",
    "    df_grid_pca95 = df_grid_pca95.append(_df) \n",
    "\n",
    "    ## PCA 99% \n",
    "    _df, ds_class_pca99 = k_foldCV( X2_train_val_flatten,  y2_train_val_flatten, grid_format=True,\n",
    "                                    y_range = y2_train_val_range_flatten, methods= algorithms,  \n",
    "                                    gauge_id_list = id_train_val, return_classification=True,\n",
    "                                    do_PCA = True, n_pca = 0.99)\n",
    "    df_grid_pca99 = df_grid_pca99.append(_df)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fdfe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_df_pf = [\n",
    "    df_grid_no_pca, \n",
    "    df_grid_pca60,\n",
    "    df_grid_pca70, \n",
    "    df_grid_pca80,\n",
    "    df_grid_pca90, \n",
    "    df_grid_pca95, \n",
    "    df_grid_pca99]\n",
    "\n",
    "list_ds_cl = [\n",
    "    ds_class_noPCA, \n",
    "    ds_class_pca60, \n",
    "    ds_class_pca70,\n",
    "    ds_class_pca80,\n",
    "    ds_class_pca90, \n",
    "    ds_class_pca95, \n",
    "    ds_class_pca99]\n",
    "\n",
    "\n",
    "\n",
    "fn_df_pf = [\n",
    "    f'alg_pf_grid_noPCA_{name_set}_{set_buffer_size}_{similarity_method}.csv', \n",
    "    f'alg_pf_grid_60PCA_{name_set}_{set_buffer_size}_{similarity_method}.csv',\n",
    "    f'alg_pf_grid_70PCA_{name_set}_{set_buffer_size}_{similarity_method}.csv',\n",
    "    f'alg_pf_grid_80PCA_{name_set}_{set_buffer_size}_{similarity_method}.csv',\n",
    "    f'alg_pf_grid_90PCA_{name_set}_{set_buffer_size}_{similarity_method}.csv',\n",
    "    f'alg_pf_grid_95PCA_{name_set}_{set_buffer_size}_{similarity_method}.csv',\n",
    "    f'alg_pf_grid_99PCA_{name_set}_{set_buffer_size}_{similarity_method}.csv']\n",
    "\n",
    "fn_ds_cl = [\n",
    "    f'alg_cl_grid_noPCA_{name_set}_{set_buffer_size}_{similarity_method}.nc', \n",
    "    f'alg_cl_grid_60PCA_{name_set}_{set_buffer_size}_{similarity_method}.nc',\n",
    "    f'alg_cl_grid_70PCA_{name_set}_{set_buffer_size}_{similarity_method}.nc',\n",
    "    f'alg_cl_grid_80PCA_{name_set}_{set_buffer_size}_{similarity_method}.nc',\n",
    "    f'alg_cl_grid_90PCA_{name_set}_{set_buffer_size}_{similarity_method}.nc',\n",
    "    f'alg_cl_grid_95PCA_{name_set}_{set_buffer_size}_{similarity_method}.nc',\n",
    "    f'alg_cl_grid_99PCA_{name_set}_{set_buffer_size}_{similarity_method}.nc']\n",
    "\n",
    "for i, out_df in enumerate(list_df_pf):\n",
    "    \n",
    "    fn_df = test_dir / fn_df_pf[i]\n",
    "    out_df.to_csv(fn_df)\n",
    "    print(fn_df)\n",
    "    \n",
    "    fn_ds = test_dir / fn_ds_cl[i]\n",
    "    list_ds_cl[i].to_netcdf(fn_ds)\n",
    "    print(fn_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de6bba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PCA sensitivity analysis - for single pixel \n",
    "\n",
    "df_single_no_pca = pd.DataFrame()\n",
    "df_single_pca60 = pd.DataFrame()\n",
    "df_single_pca70 = pd.DataFrame()\n",
    "df_single_pca80 = pd.DataFrame()\n",
    "df_single_pca90 = pd.DataFrame()\n",
    "df_single_pca95 = pd.DataFrame()\n",
    "df_single_pca99 = pd.DataFrame() \n",
    "\n",
    "for kn in range(n_folds):\n",
    "    print('Iteration {}'.format(int(kn+1)))\n",
    "    ## no pca \n",
    "    _df, df_class_noPCA = k_foldCV( df_train_val, id_col='tag', target_col = target_col[0],\n",
    "                                   range_col = 'range_target',feature_cols = feature_columns, \n",
    "                                   methods = algorithms, do_PCA = False, return_classification=True)\n",
    "    df_single_no_pca = df_single_no_pca.append(_df)\n",
    "\n",
    "    ## PCA 60% \n",
    "    _df, df_class_PCA60 = k_foldCV( df_train_val, id_col='tag', target_col = target_col[0], \n",
    "                                   range_col = 'range_target', feature_cols = feature_columns, \n",
    "                                   methods = algorithms, do_PCA = True, n_pca = 0.6, return_classification=True)\n",
    "    \n",
    "    df_single_pca60 = df_single_pca60.append(_df)    \n",
    "\n",
    "    ## PCA 70% \n",
    "    _df, df_class_PCA70 = k_foldCV( df_train_val, id_col='tag', target_col = target_col[0], \n",
    "                                   range_col = 'range_target', feature_cols = feature_columns, \n",
    "                                   methods = algorithms, do_PCA = True, n_pca = 0.7, return_classification=True)\n",
    "    df_single_pca70 = df_single_pca70.append(_df) \n",
    "\n",
    "    ## PCA 80% \n",
    "    _df, df_class_PCA80 = k_foldCV( df_train_val, id_col='tag', target_col = target_col[0],\n",
    "                                   range_col = 'range_target', feature_cols = feature_columns,\n",
    "                                   methods = algorithms, do_PCA = True, n_pca = 0.8, return_classification=True)\n",
    "    df_single_pca80 = df_single_pca80.append(_df) \n",
    "\n",
    "    ## PCA 90% \n",
    "    _df, df_class_PCA90 = k_foldCV( df_train_val, id_col='tag', target_col = target_col[0], \n",
    "                                   range_col = 'range_target', feature_cols = feature_columns, \n",
    "                                   methods = algorithms, do_PCA = True, n_pca = 0.9, return_classification=True )\n",
    "    df_single_pca90 = df_single_pca90.append(_df) \n",
    "\n",
    "    ## PCA 95% \n",
    "    _df, df_class_PCA95 = k_foldCV( df_train_val, id_col='tag', target_col = target_col[0], \n",
    "                                   range_col = 'range_target', feature_cols = feature_columns, \n",
    "                                   methods = algorithms, do_PCA = True, n_pca = 0.95, return_classification=True )\n",
    "    df_single_pca95 = df_single_pca95.append(_df) \n",
    "\n",
    "    ## PCA 99% \n",
    "    _df, df_class_PCA99 = k_foldCV( df_train_val, id_col='tag', target_col = target_col[0], \n",
    "                                   range_col = 'range_target', feature_cols = feature_columns,\n",
    "                                   methods = algorithms, do_PCA = True, n_pca = 0.99, return_classification=True)\n",
    "    df_single_pca99 = df_single_pca99.append(_df) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3be8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## SAVE RESULTS \n",
    "list_df_pf = [df_single_no_pca, df_single_pca60, df_single_pca70, df_single_pca80, \n",
    "              df_single_pca90, df_single_pca95, df_single_pca99 ]\n",
    "\n",
    "list_df_cl = [df_class_noPCA, df_class_PCA60, df_class_PCA70, df_class_PCA80, \n",
    "              df_class_PCA90, df_class_PCA95, df_class_PCA99]\n",
    "\n",
    "fn_df_pf = [f'alg_pf_single_noPCA_{name_set}_{set_buffer_size}_{similarity_method}.csv', \n",
    "            f'alg_pf_single_60PCA_{name_set}_{set_buffer_size}_{similarity_method}.csv',\n",
    "            f'alg_pf_single_70PCA_{name_set}_{set_buffer_size}_{similarity_method}.csv',\n",
    "            f'alg_pf_single_80PCA_{name_set}_{set_buffer_size}_{similarity_method}.csv',\n",
    "            f'alg_pf_single_90PCA_{name_set}_{set_buffer_size}_{similarity_method}.csv',\n",
    "            f'alg_pf_single_95PCA_{name_set}_{set_buffer_size}_{similarity_method}.csv',\n",
    "            f'alg_pf_single_99PCA_{name_set}_{set_buffer_size}_{similarity_method}.csv']\n",
    "\n",
    "fn_df_cl = [f'alg_cl_single_noPCA_{name_set}_{set_buffer_size}_{similarity_method}.csv', \n",
    "            f'alg_cl_single_60PCA_{name_set}_{set_buffer_size}_{similarity_method}.csv',\n",
    "            f'alg_cl_single_70PCA_{name_set}_{set_buffer_size}_{similarity_method}.csv',\n",
    "            f'alg_cl_single_80PCA_{name_set}_{set_buffer_size}_{similarity_method}.csv',\n",
    "            f'alg_cl_single_90PCA_{name_set}_{set_buffer_size}_{similarity_method}.csv',\n",
    "            f'alg_cl_single_95PCA_{name_set}_{set_buffer_size}_{similarity_method}.csv',\n",
    "            f'alg_cl_single_99PCA_{name_set}_{set_buffer_size}_{similarity_method}.csv']\n",
    "\n",
    "for i, out_df in enumerate(list_df_pf):\n",
    "    \n",
    "    fn_pf = test_dir / fn_df_pf[i]\n",
    "    out_df.to_csv(fn_pf)\n",
    "    print(fn_pf)\n",
    "    \n",
    "    fn_cl = test_dir / fn_df_cl[i]\n",
    "    list_df_cl[i].to_csv(fn_cl)\n",
    "    print(fn_cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd056b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "## reset warnings \n",
    "warnings.filterwarnings('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9730d008",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67789a20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16f5e06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb77c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba9db52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6ebcd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc5bc15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dab097",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d206fb93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf7d2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## prepare training & validation set - single grid \n",
    "\n",
    "# # subsample \n",
    "# df_train_1 = df_train_val[ df_train_val[target_col[0]] == 1]\n",
    "# _df_train_0 = df_train_val[ df_train_val[target_col[0]] == 0]\n",
    "# ix_df_train_0 = []\n",
    "\n",
    "# ## sample for each 0 sample \n",
    "# for i, idx in enumerate(_df_train_0['tag'].unique()):\n",
    "#     sub_df_train_0 = _df_train_0[_df_train_0['tag']==idx]\n",
    "#     ## get sample\n",
    "#     subsample = sub_df_train_0.sample(n=1).index.values[0]\n",
    "#     ix_df_train_0.append(subsample)\n",
    "# df_train_0 = df_train_val.loc[ix_df_train_0]\n",
    "# df_train = df_train_1.append(df_train_0)\n",
    "\n",
    "# # add a number of samples not in buffer as well \n",
    "# df_not_in_buffer = df_train_val[df_train_val['in_buffer']!=1]\n",
    "# n_sample = df_not_in_buffer['tag'].nunique()\n",
    "\n",
    "# df_train = df_train.append( df_not_in_buffer.sample(n=n_sample) ).copy()\n",
    "\n",
    "# print('Division of target values before subsampling: ')\n",
    "# print( df_train_val['target'].value_counts())\n",
    "# print('\\nand after subsampling: ')\n",
    "# print(df_train['target'].value_counts())\n",
    "\n",
    "# X_train = df_train[feature_columns]\n",
    "# y_train = df_train[target_col[0]]\n",
    "# y_train_range = df_train['range_target']\n",
    "\n",
    "# # normalize \n",
    "# min_samples = len(X_train) \n",
    "# nm = QuantileTransformer( output_distribution='normal',\n",
    "#                         n_quantiles = int(min(1000, min_samples)))\n",
    "# nm.fit(X_train)\n",
    "# X_train = nm.transform(X_train)\n",
    "\n",
    "# # scale \n",
    "# sc = MinMaxScaler([0,1])\n",
    "# sc.fit(X_train)\n",
    "# X_train = sc.transform(X_train)\n",
    "\n",
    "# # pca \n",
    "# print(f'\\nnumber of features before PCA = {X_train.shape[1]}')\n",
    "# pca = PCA(n_components=0.9)\n",
    "# pca.fit(X_train)\n",
    "# X_train = pca.transform(X_train)\n",
    "# print(f'number of features after PCA = {X_train.shape[1]} with 90% variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162bfd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## plot PC_i vs PC_j\n",
    "# pc_i = 0\n",
    "# pc_j = 1\n",
    "\n",
    "# ix_0 = np.where(y_train==0)[0]\n",
    "# ix_1 = np.where(y_train==1)[0]\n",
    "\n",
    "# plt.scatter( X_train[ix_0,pc_i], X_train[ix_0,pc_j], color = 'b', label='0' ) \n",
    "# plt.scatter( X_train[ix_1,pc_i], X_train[ix_1,pc_j], color = 'r', label='1'  ) \n",
    "\n",
    "# plt.xlabel('PC{}'.format(pc_i+1));\n",
    "# plt.ylabel('PC{}'.format(pc_j+1));\n",
    "# plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3205dfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## apply same transformations and splits on test set \n",
    "# X_test = df_test[feature_columns]\n",
    "# y_test = df_test[target_col[0]]\n",
    "# y_test_range = df_test['range_target'] \n",
    "\n",
    "# ## normalize \n",
    "# X_test = nm.transform(X_test)\n",
    "\n",
    "# ## scale \n",
    "# X_test = sc.transform(X_test)\n",
    "\n",
    "# ## pca \n",
    "# X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7235660",
   "metadata": {},
   "outputs": [],
   "source": [
    "## train single pixel classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a75e25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81e7044",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODEL 1 - target in buffer, yes or no?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae60a4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96617eb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2781abf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ef6c9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0035d231",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODEL 2 - if target in buffer, where? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb78ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733ed039",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6233d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f04d04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb384cc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2141ec7d",
   "metadata": {},
   "source": [
    "After finding best settings, use train_val set to train an algorithm, evaluate the final performance with the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cbfc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff71ee1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8c2af6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
